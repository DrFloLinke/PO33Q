[["index.html", "PO33Q: Determinants of Democracy - Analysing Emergence, Survival, and Fall Preface", " PO33Q: Determinants of Democracy - Analysing Emergence, Survival, and Fall Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 01 May, 2025 Preface !!! site still under construction !!! Never before has the world witnessed such a high proportion of democracies. But what makes them emerge? How do democracies sustain themselves? What makes some countries such persistent dictatorships? These questions have attracted a huge body of literature in Comparative Politics and make for fascinating empirical research projects. This module will provide you with the statistical tools necessary for such investigations, and thus allow you not only to interpret and critique existing studies in this field but also to research these questions yourself. With no prior knowledge in statistical analysis necessary, we will use real-world data sets and explore the aforementioned questions through the methods of linear and logistic regression, as well as survival analysis. This online companion has two purposes: (1) it introduces you to the quantitative methods element of the module, and (2) it provides you with some exercises that we will be working through in the seminars each week. We start with an introduction to R in Week 1, and then move closer and closer to the method you will be using in the final assessment of this module: a Markov Transition Model. As such, the module also follows the methodological evolution that the democratisation literature in general, and the modernisation literature in particular have traversed since the 1950s: from linear regression to a time-series probability model. PO33Q is my flagship module and has been lauded by the external examiner as the best of its kind she had seen. A lot of work and love has gone into this module, and I hope you will enjoy taking it as much as I enjoy teaching it. "],["website-features.html", "Website Features", " Website Features You will find embedded in the text five different types of boxes which serve different purposes: Some explanations that will hopefully make your work with this webpage or learning the material itself easier. These boxes contain important definitions relating to the methods we explore in this companion. You can also click on glossary entries in the text to take you to the relevant glossary section of the companion. A brief question which tests your understanding of the previous material. Here you need to be careful when working with R to avoid problems. To support you in learning the material we cover in this online companion, I have recorded videos for some central terms and topics. "],["accessibility.html", "Accessibility", " Accessibility For those of you who prefer a dark background, like me, you can select this option from the menu at the top of the page. Click the “A” symbol, and then you can choose between “white”, “sepia”, or “night”. The companion uses the font “Lexend”. Lexend fonts are intended to reduce visual stress and so improve reading performance. Initially they were designed with dyslexia and struggling readers in mind, but Bonnie Shaver-Troup, creator of the Lexend project, soon found out that these fonts are also great for everyone else. "],["introduction-to-r.html", "Introduction to R How Data are organised R &amp; RStudio – Installation R - Getting Started RScript First Steps in R The Working Directory R Packages Working with Your Data Set Data Manipulation The Real Data Set Book Recommendations", " Introduction to R How Data are organised I have put a little sample data set together for you which you can see here: Figure 1: Sample Data Set Data such as these are known as secondary data, as they have been collected by somebody else. In this case by the World Bank. The data presented here are also known as cross-sectional data, as they look at different units (here countries) at a single point in time. I will introduce you to time-series data in Week 7. Each column represents a variable, the first one country names, and the second per capita GDP in 2015. Each row represents an observation, in our case an individual country. The meeting point between the the variable and the observation is a particular value. So for example, in Figure 1 the GDP (second column) of the third country (row) is $ 5792 (cell). You see in the first row the names of the variables. Always make these short and sweet, but especially telling. Don’t go for something like “x4_st”, or “fubar”, as nobody (including yourself after a little while) will have any clue what this is. R &amp; RStudio – Installation Now we are ready to start working with R. The first step is to install the program. Please follow these instructions: Go to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program. Go to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program. Now open RStudio. Whilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio. R - Getting Started In this companion I am using two different fonts: Font for plain text A typewriter font for R functions, values, etc. Let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen: Figure 2: RStudio It has – for now – three components to it. On the left hand-side you see the so-called “Console” into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the “Workspace” which consists of an upper and a lower window. The upper window has three tabs in it. The tab “Environment” will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the “History” tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the “Connections” tab you can connect to online sources. We will not use this tab. In the lower window, you have five tabs. Under “Files” you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The “Plots” tab will display the graphs we will be producing. “Packages” form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a “Help” function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab “Viewer”. Introduction to R Studio RScript If you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs. One of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” (King, 1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check. The creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due. To create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this: Figure 3: The RScript Window You can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows. If you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section. Figure 4 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight. Figure 4: Example of an RScript More Themes If you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes1 to choose from: install.packages( &quot;rsthemes&quot;, repos = c(gadenbuie = &#39;https://gadenbuie.r-universe.dev&#39;, getOption(&quot;repos&quot;)) ) rsthemes::install_rsthemes() You can also download Flo’s Dark Theme2 and then “add” it at the bottom of the “Appearance” menu. Appearance RScript Structure Well, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it. First of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article. I stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course). Since you know how to write in an RScript now, avoid using the “Console” to write your code. Only write in the RScript. RScript Structure First Steps in R But enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type 5+3 and press “command” / “enter” (or “Ctrl” / “enter” if you are on Windows). In everything that is to follow, commands will be shown in boxes with the output underneath preceded by a hash tag. So, including result, the calculation would look like this: 5+3 [1] 8 where the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. Note that the result of this operation is displayed in the “Console”, even if you write this in the RScript above. You can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript. A fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call3 result &lt;- 5+3 If we now call the object, R will return its value, 8. result [1] 8 The Working Directory It is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take. In those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 5. Figure 5: Folder Structure You see that there is a sub-folder for each week of the module (I have only done a few for illustrative purposes), and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO33Q. R works with so-called “Working Directories”. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO33QQ_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Please set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it. Now we need to tell R to use this folder. If you know the file structure of your computer you can simply use the command, and enter the path. Here is an example from my computer: setwd(&quot;~/Warwick/Modules/PO33Q//Week 1&quot;) If you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. Working Directory R Packages It would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets: install.packages(&quot;readxl&quot;) We can then load this package into our library with the library() command. library(readxl) Once you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again. Working with Your Data Set Opening We are now ready to open the data set in R - where it is called a “data frame”. First, download the Example data set (also available in the Downloads Section) and place it into the current working directory. To load it into R, we create a new object example, and ask R to read “Sheet 1 of the Excel file”example.xlsx”. example &lt;- read_excel(&quot;Week 1/example.xlsx&quot;, sheet=&quot;Sheet1&quot;) Loading the Data Set Please do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need. We can now use our data in R! Viewing the Data In the present case, you know what the data look like, but very often when you use secondary data sets, you don’t. So it’s a good idea to view the data frame before doing anything with it. To view the data frame in a way you might be familiar with from Excel (even though you cannot edit this in the same way). apply the View() command. View(example) If you only want to see the first 6 observations of each variable, use the head() command: head(example) # A tibble: 6 × 2 country gdp &lt;chr&gt; &lt;dbl&gt; 1 China 13571 2 Germany 44187 3 India 5792 4 UK 38865 5 US 52704 6 Zambia 3602 If you simply want to know the variable names in the data frame, type: names(example) [1] &quot;country&quot; &quot;gdp&quot; The next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure: str(example) tibble [6 × 2] (S3: tbl_df/tbl/data.frame) $ country: chr [1:6] &quot;China&quot; &quot;Germany&quot; &quot;India&quot; &quot;UK&quot; ... $ gdp : num [1:6] 13571 44187 5792 38865 52704 ... Variable Types in R You have seen in the output of the str() command that R distinguishes between a number of different variable types. Here is a broad overview of the variable types, so that you know which descriptive statistics you can calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types: numeric – numbers, continuous character (also called string) – letters Within numeric we can distinguish between the following: factor - nominal variable, categorical ordered factor - ordinal variable, categorical integer - numeric, but only “whole” numbers (discrete) numeric - any number (interval or ratio scales) If you are unfamiliar with measurement scales, then please look these up before proceeding. Descriptive Statistics Quite a large number of descriptive statistics can be calculated. For example: Mean Median Mode Range Standard Deviation Variance Calculating a Mean YouTube link to follow Standard Deviation YouTube link to follow If any of the other terms don’t mean anything to you beyond having heard the term before, please look these up. They are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. If we want to refer to a particular column in R (which is equivalent to a variable), then we need to specify the data frame within which the variable is located, followed by a $ sign and then the variable name. Schematically, this would look be written as dataframe$variable. With this information to hand, we can calculate the mean of the variable gdp: mean(example$gdp) [1] 26453.5 Then the median: median(example$gdp) [1] 26218 Mean, median, and also mode (the most frequently occurring value) are all measures of centrality, but centrality alone does not adequately describe a distribution. You can think of two scenarios, in both we have two people in a group and we are trying to describe their age. In group one we have one person who is 50 years old, and one who is 52 years old. Average age = 51. In the second group we have a toddler aged 2, and a very old person aged 100. Same average, but a very different distribution of age. Schematically you can see this in Figure 6 where two distributions have the same mean, median and mode, but their spread is quite different. Figure 6: Distributions with different Standard Deviations Therefore, we also need to look at the variability of a variable to adequately describe it. Again, there are quite a few measures available. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you the range straight away: min(example$gdp) [1] 3602 max(example$gdp) [1] 52704 range(example$gdp) [1] 3602 52704 The standard deviation is rather long-winded to calculate by hand, but the R command is short and sweet: sd(example$gdp) [1] 21319.75 The variance is the squared standard deviation, but you can calculate it with its own command in R, too: var(example$gdp) [1] 454531709 You can get information on the quartiles (these are also measures of spread), the mean, as well as the minimum and maximum of a variable through one, simple command: summary(example$gdp) Min. 1st Qu. Median Mean 3rd Qu. Max. 3602 7737 26218 26454 42856 52704 Data Manipulation Recoding When conducting quantitative research, variables will rarely come in the format in which you require them to be. I have been kind and reshaped all data you will be using for this module already. Nonetheless, you might come into a position in which you need to recode a variable, and here is how to do it. The process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). Let’s go all out with the tidyverse: library(tidyverse) Now we can recode. Let’s say we want to create a new variable with two categories: low income and high income, where the cut-off sits at $ 20,000. The comnand to do this takes a little explaining. We start by stating the dataframe we wish to work with, example. The symbol which follows, %&gt;%, reads as “and then”, and is called (yes seriously) a pipe. So we take the data frame example “and then” carry out a function called mutate. This function in turn defines the new variable gdpcat by recoding the variable gdp. The command then specifies all categories of the “old” variable gdp and what their respective values in the “new” variable gdpcat are going to be. The categories in each are set in quotation marks, as they are factor / character categories. The last step is then to assign this newly created variable gdpcat to our data frame example. example %&gt;% mutate(gdpcat= ordered( cut(gdp, breaks=c(0, 20000, Inf), labels=c(&quot;low&quot;,&quot;high&quot;)))) -&gt; example Make a habit of adding a note underneath each of the more complex code chunks in your RScript (preceded with a #) in which you translate the code into plain English. Let us now check the structure of the new variable to make sure that we have done everything correctly. str(example$gdpcat) Ord.factor w/ 2 levels &quot;low&quot;&lt;&quot;high&quot;: 1 2 1 2 2 1 In my case all looks fine - make sure yours looks the same. Recoding a Factor Variable Saving Please save the Rscript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the worksheet or the data, as running the RScript on the raw data will bring you precisely to where you left off. The Real Data Set Download the world.csv data set from the Downloads Section and place it into the current working directory. world &lt;- read.csv(&quot;Week 1/world.csv&quot;) The data are taken from World Bank (2024), Boix et al. (2018), and Marshall &amp; Gurr (2020). Table 1 provides a full code book, but you can also download it in pdf format here. Table 1: WDI Codebook variable label countrycode Country Code country Country Name year Year polity5 Combined Polity V Score democracy 0 = Autocracy, 1 = Democracy (Boix et al., 2018) gdppc GDP per capita (current US Dollars) enrol_gross School enrollment, primary (% gross) primcomp Primary completion rate (% of relevant age group) primcom_male Primary completion rate, male (% of relevant age group) primcom_fem Primary completion rate, female (% of relevant age group) enrol_net School enrollment, primary (% net) agri Agriculture, forestry, and fishing, value added (% of GDP) slums Population living in slums (% of urban population) telephone Households with telephones (Landline &amp; Mobile, %) internet Individuals using the Internet (% of population) tax Tax revenue (% of GDP) pop Population, total pop_fem Population, female (% of total population) electricity Access to electricity (% of total population) service Services, value added (% of GDP) oil Oil rents (% of GDP) natural Total natural resources rents (% of GDP) literacy Literacy rate, adult total (% of people ages 15 and above) lit_male Literacy rate, adult male (% of males ages 15 and above) lit_fem Literacy rate, adult female (% of males ages 15 and above) infant Mortality rate, infant (per 1,000 live births) health_ex Current health expenditure (% of GDP) hospital Hospital beds (per 1,000 people) tuberculosis Incidence of tuberculosis (per 100,000 people) life Life expectancy at birth, total (years) life_male Life expectancy at birth, male (years) life_female Life expectancy at birth, female (years) unemploy Unemployed (%) urban Urban population, total (% of total population) attend Births attended by skilled health staff (% of total) prenatal Pregnant women receiving prenatal care of at least four visits military Military Expenditure (% of GDP) un_continent_name Continent un_region_name Region on continent Again, let’s explore the data set through the view() function: view(world) Now, don’t panic. This data set is big, but you know the basic structure: country by year in rows, and the variables in the column. The value for the variable in a given country in a given year is in the meeting point between row and column. The data are (yes, the word “data” is plural) organised by country name in the first instance. So the first country you see is Afghanistan. There are multiple rows for Afghanistan, because each row gives you information about the value in a particular year (second column). This is repeated for every country in the world, leading to a total of 9.456 observations, to save you scrolling all the way down. Schematically, the structure of this data set looks as follows: When you look at the data set in R, you will see that not every box contains a value - this means that the data are missing for this particular country-year for that particular variable. This is an issue we will be discussing a later stage in more detail, but I can already say now, that this issue is more pronounced in developing countries than in developed ones, and that it often severely limits the variables you can include in the analysis. Let us conclude today with some more descriptive statistics to get used to entering and executing commands. Say, we want to find out the average GDP per capita of the UK since 1960 (which is when this data set begins, it ends in 2015). To do this, we first need to create a data frame which only contains data for GB. We call this process “subsetting”. The filter function comes out the dplyr package, and takes a particular data frame and filters all those observations which meet the condition specified. We will use this a lot on this module, and so it makes sense to remember this one well: GB &lt;- filter(world, countrycode==&quot;GBR&quot;) We can now produce summary(GB$gdppc) Min. 1st Qu. Median Mean 3rd Qu. Max. 1398 3606 14553 18629 29091 50398 and all other descriptive statistics outlined before for “Great Britain”. If you want the number of observations, then you can display them by calling: length(GB$gdppc) [1] 56 In this particular case, it would tell us that we have data for 56 years. If this all seems a little much at the moment, don’t worry. As we go through the module, R will become much, much easier to handle! Book Recommendations If you want some more material to read up on R, then these are my recommendations: Fogarty (2023): The best applied R book on the market until my own book comes out. Stinerock (2022): Popular with students for good reasons! J. D. Long &amp; Teetor (2019): Some recipes to cook with R Please consult the List of References for full details. Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎ This is a variation of the Dracula Theme.↩︎ To “call” means to execute a command.↩︎ "],["homework.html", "Homework", " Homework "],["glossary.html", "Glossary", " Glossary Table 2: Glossary Week 1 Term Description Methodological analysis A detailed evaluation of data to discover their structure and relevant information to answer a research question attribute A component or characteristic of a concept background concept The broad constellation of meanings and understandings associated with the concept (Adcock &amp; Collier, 2001, p. 531) categorical Describing the qualitative categories of a characteristic, for example different religions concept Abstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150) conceptualization Formulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock &amp; Collier, 2001, p. 531) continuous Can assume any value within defined measurement boundaries cross-sectional data Look at different units (or cross-sections) \\(i\\) at a single point in time data Derives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis data set A collection of numerical values for individual observations, separated into distinctive variables dependent variable Is dependent through some statistical or stochastic process on the value of an independent variable descriptive statistics Summarise information about the centre and variability of a variable deviation The deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\) discrete The result of a counting process hypothesis In statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference. independent variable Influences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable” interpretation The explanation of results to answer the research question literature review An analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question mean Is equal to the sum of the observations divided by the number of observations measurement Refers to the selection of a measure or variable median Separates the lower half from the upper half of observations method A tool for systematic investigation mode Is the most frequently occurring value QM The process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question quartile Divides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below research question A specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle secondary data Secondary data are data which have been collected by somebody else Social Sciences Are concerned with the study of society and seek to scientifically describe and explain the behaviour of actors standard deviation The standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\] systematized concept A specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock &amp; Collier, 2001, p. 531) theory A formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.) validity The extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong” variable An element of a conceptual component which varies. We also call these “measures” variance Is equal to the squared standard deviation Substantive term term "],["flashcards.html", "Flashcards", " Flashcards In this section you will find some flashcards each week. These should help you learn the purpose of R functions on PO33Q. These are divided into: New Functions this week, containing only those functions that we encounter for the first time All Functions this week, containing all functions we have used in that particular week All Functions up to now, containing all functions used on the module up to that particular week. You can download a .csv file containing the functions and the descriptions for each of these underneath the flashcard window. Should you wish to create your own flashcards, for example, with more difficult-to-remember functions, I have written some instructions how to go about it. R Functions This Week The data are available as a .csv file. "],["principles-of-statistical-inference.html", "Principles of Statistical Inference Probability Distributions Tests of Statistical Significance", " Principles of Statistical Inference Starting to write this, I am tackling the interesting task of condensing four weeks’ worth of material of my introductory quantitative module into a few paragraphs. But a brief overview of statistical inference and significance tests is essential for you to understand the interpretation of the methods we are going to explore. So let’s start with probability distributions. Probability Distributions The term probability distribution sounds fancy, but really only implies that we are assessing the relative frequency with which a particular value occurs. So, for example, if I have six History students on the module, and four PAIS students, then the probability of being a PAIS student is 40%. For categorical data, such as this, we can assign a probability to each category. But for continuous variables, such as the Polity V score, or per capita GDP this no longer works. Instead, we assign probabilities to an interval of numbers. In the following graph, I am putting the values of such a continuous variable on the x-axis, and then draw a curve over this which indicates the probability distribution for these values. Figure 7: Probability Distribution for a Continuous Variable In this graph, the values on the x-axis between alpha and beta have a probability of occurring that is equal to the orange area under the curve. This curve is bell-shaped, and symmetrical around the mean of the variable x, and is known as the normal distribution. The beauty of this distribution is that the amount of probability that is contained in an interval around the mean depends solely on the number of standard deviations, denoted as \\(\\sigma\\)) of the variable x. So, for example, if I travel one standard deviation to the left and to the right from the mean, I will always have 68% of the area under the distribution in this interval. Figure 8: Probability within one Standard Deviation For two standard deviations, this is equivalent to 96% and for three standard deviations 99.9%. Schematically this looks as follows: Figure 9: Probabilities under the Normal Distribution The total area under the probability distribution is equal to 100%. So, if the orange area in Figure 8 is equal to 68%, then the white area must be 32%. As this area is split equally into a left and a right side, the area on the right must be 16%. This is what is known as a right-tail probability. Here it is the probability beyond 1 standard deviation, or \\(1 \\sigma\\). This means that if we know how many standard deviations away from the mean of the distribution a value falls, we can make a statement on how likely it would be to observe a value that is higher than this value. For example, if I know the distribution of marks on a module, this would allow me to state how likely it was to achieve a First, or a mark of higher than 70. Assume that the average of an assessment was 63, and the standard deviation 9. Then the difference between the mean and the value we are interested in is equal to 70-63=7. To express this distance in units of standard deviations, we now divide 7 by 9, and obtain something that is called a z-score: \\[\\begin{equation} z=\\frac{\\text{Observation} - \\text{Mean}}{\\text{Standard Deviation}} \\end{equation}\\] In our case, z would be equal to 7/9, or 0.7777778. You can look this value up in a z-table which lists the right-tail probability for all conceivable values of z. For z=0.78 it returns a probability of 21.77%. With a mean of 63 and standard deviation of 9,the probability of scoring a First would be 21.77%. This all works fine, so long as we are dealing with the population. Sadly, we rarely have access to the population in the social sciences, however. Surveys, for example, are usually conducted for a small representative sub-group of the population, a sample. So, how do we make statements about the population if we only have access to a sample? The answer is: with a sampling distribution. Sampling Distributions Whenever we draw a sample from a population, the values we draw will vary. Therefore, also the mean of these values will vary each time. Imagine I draw a sample of 5 students from a seminar group, time and time again, and each time calculate the average age of these 5 students. Sometimes the average will be high, sometimes it will be low, but – and here comes the magic – the vaue that will come up most often in these samples is the true mean of the population (all students in the seminar). We call this population value a parameter. If we arrange all of the sample means we have obtained from this process, these will form a distribution in their own right, the sampling distribution. This has the true population mean (let’s denote this as \\(\\mu\\)) in its centre. In Figure 10, for example, I have simulated a population with a mean of zero, and have drawn 9000 samples from this population, each with 30 observations. As you can see, the distribution of means of each of these 9000 samples forms a normal distribution which has zero, the population mean, at its centre. Figure 10: Sampling Distribution So, we know the mean of the sampling distribution, but what about its standard deviation? We could of course take samples repeatedly from the population and calculate it, but that is wasteful. Instead, we estimate it by dividing the standard deviation of the one sample we usually have by the square root of the sample size. The result is the so-called standard error, denoted as “se”, the standard deviation of the sampling distribution. \\[\\begin{equation} se=\\frac{s}{\\sqrt{n}} \\end{equation}\\] where s is the sample standard deviation and n is the sample size. Using the sample standard deviation comes with the cost of having to deal with uncertainty, as this value will vary with each sample we draw. This forces us to abandon the normal distribution, and use the t-distribution instead. The t-Distribution The t-distribution is also bell-shaped and symmetrical, centered around a fixed mean of zero. However, its shape is not static, but depends on the number of degrees of freedom. Degrees of freedom are constraints on the estimation process that reflect the number of independent pieces of information available. In our case, the constraint arises from estimating the population standard deviation \\(\\sigma\\) using the sample standard deviation, denoted as s. For a single sample, the degrees of freedom are defined as n-1, where n is the sample size. Depending on these degrees of freedom, the t-distribution becomes wider or narrower, expressing the uncertainty we are incurring from working with a sample. You can see in Figure 11 that with increasing sample size the t-distribution becomes narrower and narrower, until – for an infinite sample size – it is equivalent to the normal distribution again. Figure 11: The t-Distribution With the help of the sampling distribution, we can now construct an interval of values within which we believe the true population parameter to fall with a certain probability. This interval is known as a confidence interval. Confidence Intervals How do we construct confidence intervals? We start by setting the confidence level—the probability with which we believe the confidence interval contains the true population parameter. It is defined as \\((1 - \\alpha)\\), where \\(\\alpha\\) is the significance level. Because we are constructing the interval based on a sample, we use the sampling distribution. We place the sample mean at the center of this distribution and treat it as our point estimate of the unknown population mean. To establish an interval around this value that corresponds to the desired confidence level, we determine how many standard errors we must move to the left and right. This number is the t-score, which we can find using a statistical table. The appropriate t-score depends on the sample size (or the degrees of freedom, to be more precise). For smaller samples, the t-distribution is wider, so we need to travel further away from the mean to achieve the same level of confidence than we would with a larger sample and a narrower distribution. For example, to construct a 95% confidence interval, we would use a t-score of approximately 2.306 for a sample of 9, but only 2.045 for a sample of 30. As an example, reconsider our fictitious assessment with an average of 63, and the standard deviation 9. I used this for a probability statement in a population earlier, but now assume this is based on a sample of 15 students from a much larger module. Within which boundaries do we believe the parameter to fall with 95% confidence? As a first step, we calculate the standard error: \\[\\begin{equation} se=\\frac{s}{\\sqrt{n}}= \\frac{9}{\\sqrt{15}} = 2.32379 \\end{equation}\\] As we have 15 observations, we have 14 degrees of freedom (n-1) which corresponds to a t-score of 2.145. We can thus state that with 95% confidence the parameter, \\(\\mu\\), falls between 58.02 and 67.98. \\[\\begin{align} \\bar{y} - t \\cdot se \\le \\;\\; &amp;\\mu \\; \\le \\bar{y} - t \\cdot se \\\\\\\\ 63- 2.145 \\cdot 2.32379 \\le \\;\\; &amp;\\mu \\; \\le 63 + 2.145 \\cdot 2.32379 \\\\\\\\ 58.01547 \\le \\; &amp;\\mu \\;\\; \\le 67.98453 \\end{align}\\] where \\(\\bar{y}\\) is the average in the sample. We will come back to confidence intervals in the context of regression analysis. Let us now look at significance tests. Tests of Statistical Significance As you know from the lecture last week, in the social sciences we generally test whether we can explain behaviour or some kind of phenomenon through a particular theory. In practical terms this means that we distill the theory into testable statements, called hypotheses. Take this hypothesis, for example: The average level of democracy in the Middle East is different from that of Europe. In this case, we are no longer interested in establishing an interval of numbers within which we believe the average level of democracy in the Middle East to fall, but we are comparing it with a particular value, or point: the level of democracy in Europe. This kind of assessment is called a significance test. Every significance test has two hypotheses, an alternative, and a null-hypothesis. The alternative hypothesis is the one I already stated:   H\\(_\\text{A}\\): The average level of democracy in the Middle East is different from that of Europe.     The null-hypothesis always represents no effect. In our case the region would have no impact on the democracy level, and the Polity V score would be identical in Europe and the Middle East.   H\\(_0\\): The average level of democracy in the Middle East is the same as in Europe.     The average Polity V score in the Middle East was -2.8 in 2015 with a standard deviation of 7.07 across 15 countries. The average Polity V score for Europe in the same year was 8.7. How to obtain these values me &lt;- filter(world, countrycode==&quot;BHR&quot;| #Bahrain countrycode==&quot;BHR&quot; | #Bahrain countrycode==&quot;CYP&quot; | #Cyprus countrycode==&quot;EGY&quot; | #Egypt countrycode==&quot;IRN&quot; | #iran countrycode==&quot;IRQ&quot; | #iraq countrycode==&quot;ISR&quot; | #israel countrycode==&quot;JOR&quot; | #jordan countrycode==&quot;KWT&quot; | #kuwait countrycode==&quot;LBN&quot; | #lebanon countrycode==&quot;OMN&quot; | #oman countrycode==&quot;QAT&quot; | #qatar countrycode==&quot;SAU&quot; | #saudi arabia countrycode==&quot;ARE&quot; | #UAE countrycode==&quot;YEM&quot; #Yemen ) me_15 &lt;- filter(me, year==2015) mean(me_15$polity) sd(me_15$polity) world$un_region_name europe &lt;- filter(world, un_region_name==&quot;Southern Europe&quot;| un_region_name==&quot;Western Europe&quot; | un_region_name==&quot;Eastern Europe&quot; | un_region_name==&quot;Northern Europe&quot;) europe_15 &lt;- filter(europe, year==2015) mean(europe_15$polity, na.rm=TRUE) t.test(me_15$gdppc, mu=8.7, data=me_15) In order to ascertain whether the observed value of -2.8 is statistically different from the null-hypothesis value, 8.7, we need to make a probability statement. To be precise, we need to quantify the probability of observing a value that is more extreme than than one we have already observed. If this probability is small, then the value we have can be regarded as highly unusual and we can conclude that it is statistically different from the null-hypothesis value. This probability of observing a value more extreme than the one we have is the p-value. Usually, we want the p-value to be quite small, either 5% or 1%. R will obtain this for us, but so you know what happens behind the scenes, here is a quick summary. We measure the distance between the observed value and the null-hypothesis value in units of standard error. This is equivalent to the t-value, our test statistic. We calculate it as follows: \\[\\begin{equation} t = \\frac{\\bar{y} - \\mu_{0}}{se}, \\quad \\text{where} \\, se = \\frac{s}{\\sqrt{n}} \\end{equation}\\] In our example: \\[\\begin{equation} t = \\frac{-2.8 - 8.7}{1.826243}=-6.297081, \\quad \\text{where} \\, se = \\frac{7.07301}{\\sqrt{15}} \\end{equation}\\] This places us quite far out into the tails of the t-distribution (see Figure 12), and as such the probability remaining beyond this value is very, very small. The probability of observing a value more extreme is therefore low, we can regard our observed value of -2.8 as unusual and conclude the the average democracy level in the Middle East is statistically different from that of Europe. The precise p-value for this is 0.001283, well below our desired significance level of 5%. Figure 12: The p-Value I have displayed this here as a two-sided test. We did not specify whether we expect the level of democracy in the Middle East to be higher or lower than in Europe, just different. As both directions are therefore possible, the p-value is made up of the sum of the areas in the left and the right tails of the distribution. One-sided tests are possible, but generally in regression analysis the test is two-sided which is why I will leave it at that here. And with that, we are finally ready to turn our attention to the actual topic for today: linear regression. "],["linear-regression---theory.html", "Linear Regression - Theory Introduction What is it? Ordinary Least Squares (OLS)", " Linear Regression - Theory Introduction Regression is the power house of the social sciences. It is widely applied and takes many different forms. In this Chapter we are going to explore the linear variant, estimated through a method called Ordinary Least Squares (OLS). This type of regression is used if our dependent variable is continuous. In Week 3 we will have a look at regression with a binary dependent variable and the calculation of the probability to fall into either of those two categories. But let’s first turn to linear regression. What is it? Regression - What is it? YouTube link to follow Regression is not only able to identify the direction of a relationship between an independent and a dependent variable, it is also able to quantify the size of the effect. Let us choose Y as our dependent variable, and X as our independent variable. We have some data which we are displaying in a scatter plot: With a little goodwill we can already see that there is a positive relationship: as X increases, Y increases, as well. Now, imagine taking a ruler and trying to fit in a line that best describes the relationship depicted by these points. This will be our regression line. The position of a line in a coordinate system is usually described by two items: the intercept with the Y-axis, and the slope of the line. The slope is defined as rise over run, and indicates by how much Y increases (or decreases is the slope is negative) if we add an additional unit of X. In the notation which follows we will call the intercept \\(\\beta_{0}\\), and the slope \\(\\beta_{1}\\). It will be our task to estimate these values, also called coefficients You can see this depicted graphically here: In the context of the module, we would for example have per capita GDP on the x-axis (independent variable), and Polity V measuring the level of democracy on the y-axis (dependent variable). This would look like this in the year 2015: Figure 13: Democracy and Development in 2015 Population We will first assume here that we are dealing with the population and not a sample. The regression line we have just drawn would then be called the Population Regression Function (PRF) and is written as follows: \\[\\begin{equation} E(Y|X_{i}) = \\beta_{0} + \\beta_{1} X_{i} \\end{equation}\\] Because we are dealing with the population, the line is the geometric locus of all the expected values of the dependent variable Y, given the values of the independent variables X, also known as the conditional expectation function (which value of Y would we expect, on average, for a given Y?). This has to do with the approach to statistics that underpins this module: frequentist statistics (as opposed to Bayesian statistics). We are understanding all values to be “in the long run”, and if we sampled repeatedly from a population, then the expected value is the value we would, well, expect to see most often in the long run. The regression line is not intercepting with all observations. Only two are located on the line, and all others have a little distance between them and the PRF. These distances between \\(E(Y|X_{i})\\) and \\(Y_{i}\\) are called error terms and are denoted as \\(\\epsilon_{i}\\). To describe the observations \\(Y_{i}\\) we therefore need to add the error terms to the PRF: \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\epsilon_{i} \\end{equation}\\] Sample In reality we hardly ever have the population in the social sciences, and we generally have to contend with a sample. Nonetheless, we can construct a regression line on the basis of the sample, the Sample Regression Function (SRF). It is important to note that the nature of the regression line we derive from the sample will be different for every sample, as each sample will have other values in it. Rarely, the PRF is the same as the SRF - but we are always using the SRF to estimate the PRF. In order to flag this up in the notation we use to specify the SRF, we are using little hats over everything we estimate, like this: \\[\\begin{equation} \\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} \\end{equation}\\] Analogously, we would would describe the observations \\(Y_{i}\\) by adding the estimated error terms \\(\\hat{\\epsilon}_{i}\\) to the equation. \\[\\begin{equation} Y_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} + \\hat{\\epsilon}_{i} \\end{equation}\\] These estimated error terms, \\(\\hat{\\epsilon}_{i}\\), are called residuals. The following graph visualises the relationship between an observation, the PRF, the SRF, the error terms, and the residuals. Ordinary Least Squares (OLS) When you eye-balled the scatter plot at the start of this Chapter in order to fit a line through it, you have sub-consciously done so by minimising the distance between each of the observations and the line. Or put differently, you have tried to minimise the error term \\(\\hat{\\epsilon}_{i}\\). This is basically the intuition behind fitting the SRF mathematically, too. We try to minimise the sum of all error terms, so that all observations are as close to the regression line as possible. The only problem that we encounter when doing this is that these distances will always sum up to zero. But similar to calculating the standard deviation where the differences between the observations and the mean would sum up to zero (essentially we are doing the same thing here), we simply square those distances. So we are not minimising the sum of distances between observations and the regression line, but the sum of the squared distances between the observations and the regression line. Graphically, we would end up with little squares made out of each \\(\\hat{\\epsilon}_{i}\\) which gives the the method its name: Ordinary Least Squares (OLS). Ordinary Least Squares (OLS) YouTube link to follow We are now ready to apply this stuff to a PO33Q-related example! "],["linear-regression-application.html", "Linear Regression – Application The Basic Command Interpreting the Output Choosing Variables Limitations Literature Recommendations", " Linear Regression – Application The Basic Command The command to run a regression in R is beguilingly simple: regression &lt;- lm(devar ~ indepvar, data=dataframe) We specify an object into which we store the results of the regression, here regression, and assign a function to this object called lm which stands for “linear model”. It is then convention to state the dependent variable first in the command, which in our case will always be democracy. This is then followed by a tilde and the independent variable which you want to include. In the case of modernisation theory, you might want to test what influence per capita GDP has on democracy. This week we are using the Polity V scale to measure democracy. “The ‘Polity Score’ captures [the] regime authority spectrum on a 21-pont scale ranging from -10 (hereditary monarchy) to +10 (consolidated democracy). (…) The Polity scheme consists of six component measures that record key qualities of of executive recruitment, constraints on executive authority and political competition. It also records changes in the institutionalized qualities of governing authority.”4 We start by setting the working directory setwd(&quot;~/Warwick/Modules/PO33Q/Week 2&quot;) and then load the Europe.csv data set into the workspace. We subset this data frame to observations from the year 2000, only. europe &lt;- read.csv(&quot;Week 2/Europe.csv&quot;) library(tidyverse) europe_2000 &lt;- filter(europe, year == &quot;2000&quot;) Our dependent variable is called polity. The independent variable is called gpdpc. We are now ready to run our first regression: reg_pol &lt;- lm(polity ~ gdppc, data=europe_2000) We can then produce a summary of the results as follows: summary(reg_pol) Call: lm(formula = polity ~ gdppc, data = europe_2000) Residuals: Min 1Q Median 3Q Max -14.1330 -0.6741 0.4742 1.4750 2.6515 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.997e+00 7.859e-01 8.902 8.61e-10 *** gdppc 1.068e-04 4.055e-05 2.635 0.0134 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.973 on 29 degrees of freedom (6 observations deleted due to missingness) Multiple R-squared: 0.1932, Adjusted R-squared: 0.1654 F-statistic: 6.944 on 1 and 29 DF, p-value: 0.01336 We can extract the number of observations used for the estimation by calling: nobs(reg_pol) [1] 31 There is a lot of information in this, and I will take you through the output step by step now. Interpreting the Output The Number of Observations nobs(reg_pol) [1] 31 Letl’s deal with the last step first. The number of observations is equal to the number of countries in this case. We have subset the data to the year 2000, and so we have 33 countries in the analysis. This seems trivial for now, but it will become important later on. Once observations are missing, R drops them from the analysis – especially in developing countries where data are often missing in large quantities this can lead to a rapid decimation in the number of observations. This in turn is problematic for the strength inference we can draw from the analysis. The Intercept R always shows the value for the intercept in the intuitively labelled row “(Intercept)” and the column “Estimate”. In this case the value is 7.0. What does this coefficient mean, substantively? When you remember the graph depicting the regression line, this is the point where the line intercepts the y-axis. So, it is the value of \\(y\\), here democracy in the form of the Polity V score, when \\(x\\), here economic development in the form of GDP, is zero. So in other words, a country with a GDP per capita of zero would achieve a Polity V score of 7. The substantive interpretation sometimes makes sense (like here), but sometimes cannot be interpreted in this way. The Slope Coefficient The slope coefficient is shown in the row depicting the name of the independent variable, here “gdp”, and again the column “Estimate”. Our slope here, is \\(1.0681e-4\\). The \\(e-4\\) means that we have to move the decimal point 4 units to the left, so written fully, this means \\(0.0001068\\). We interpret it as follows: for every additional unit of per capita GDP, measured in US$, the Polity V score increases by \\(0.0001068\\), on average. This seems very small, but when you consider the size of GDP per capita in many countries, it seems logical that this value is as small as it is. If the coefficient was negative, then this would mean that for for every additional unit of per capita GDP, measured in US$, the Polity V score would decrease by \\(0.0001068\\), on average. The all-important p-value in Regression The p-Value YouTube link to follow First things first: how do we interpret the p-value for regression? We do a regression in order to ascertain whether there is a relationship between the independent and the dependent variable, or not. For testing whether there is one, we start from the assumption that there is none. This is what we called the null hypothesis; in our example here it would be that per capita GDP does not influence the level of democracy in a country. Now, remind yourself of the normal distribution from the video which has the mean age in its centre. In the video we were interested whether age influences height. Assuming, that there is no relationship between age and height, a regression line would look as follows: The line is perfectly flat (the slope coefficient is zero), intercepting the y-axis at the mean. It does so, because the regression line is the value of y we would expect on average for a given x. If this value does not vary, it has to be the average of y. If we now put a zero slope coefficient, such as the one for height, just in the more general form of \\(\\beta_{2}\\) in the centre of a normal distribution, it looks like this: What we want to test now, with regression, is whether the slope coefficient R has calculated for us (let me denote the estimated value of \\(\\beta_{2}\\) as \\(\\hat{\\beta_{2}}\\)), is far enough from this mean of zero, to say that we can be sure to say that there is a relationship. The statement that there is a relationship between the independent and the dependent variable, is called the alternative hypothesis. In our case the alternative hypothesis would read: “The level of per capita GDP influences the level of democracy in a country”. So how far away from the centre of zero do we have to go to say that there is indeed relationship, or put differently, that our alternative hypothesis is true? The standard in political science is that we need to have a 5% probability of finding a value more extreme than the one we have observed. Under the curve in the following graph, that is equal to the blue area on the right. And that is the p-value. If this area is 5% or less, then we have observed a value for the slope coefficient which is so far away from our assumed mean of zero, that we have sufficient evidence to reject the null hypothesis, and to accept the alternative hypothesis. But now, you might say, a slope coefficient can also be negative – here we are only looking at the right hand-side, and therefore at the scenario in which a slope coefficient is positive. And you are right. The scatter plot could give us a negative line. So, if we want to move away far enough from the assumed mean of zero in the centre, we must do so in both directions, to the left and to the right. Now, we need a value that is so far out, that to either side of the distribution, 2.5% of the area are left under the curve (2.5% on the left plus 2.5% on the right make the overall 5% we are interested in). We call this a two-sided test, whereas the scenario above is a one-sided test. The p-values reported by R for the slope coefficients are always two-sided tests (unless we tell R not to, but we are not doing that on this module). This value gives us the area under the normal distribution to the left and the right beyond our observed value, as shown in this figure: The value we are looking at in R to determine the p-value, is in the column \\(Pr(&gt;|t|)\\). In order to satisfy the requirement of the p-value being 5% or less, this value needs to be smaller than 0.05. Otherwise, more than 5% area are left, and we are not certain enough that our value is far enough away from the zero mean in the centre to say that it is “statistically different” from it. When we look at the value for the slope coefficient gdppc here, \\(0.0134\\), this means that the areas on the left and the right are jointly 1.34% – small enough for us to be sure to have found a value that is far enough away from zero to claim that there is a relationship. We therefore reject the null hypothesis, and accept the alternative hypothesis: we find evidence for a relationship between per capita GDP and Polity V in Europe in the year 2000. Again, if we want to visualise this, the actual p-value of \\(0.0134\\) would look like this: The Goodness of Fit (R-Squared) Goodness of Fit YouTube link to follow As you have seen in the video, the goodness of fit is a measure to indicate how much of the variation in the dependent variable (democracy) the independent variable (per capita GDP) is explaining. For this, we take the ratio of the explained sum of squares over the total sum of squares. The resulting percentage is R\\(^2\\), also known as the coefficient of determination. This number can also be found on the R output, and is in our case \\(0.1932\\), or 19.32%. This value is not too bad for a single variable! The maximum we can explain is of course 100% with an R-Squared value of 1.0, even though this is a dream never achieved empirically. But we are still quite some distance of this dream, and can probably do better. There surely must be factors other than per capita GDP that explain democracy in Europe in the year 2000. Choosing Variables Can I choose more than one independent variable? Yes, you can! And this is where the fun starts, because now we are getting a step closer to the real world. New modernisation posits that democracy is multi-causal, and does not rest on the influence of GDP alone. Instead, it puts forward a number of concepts that act as independent variables, one of which is health. We can measure health through Hospital beds (per 1,000 people), and include this in our model, on top of GDP. To do this we type: reg_pol1 &lt;- lm(polity ~ gdppc + hospital, data=europe_2000) You see that adding independent variables is easy, we just add them on with a plus sign. It does not matter whether the expected direction of influence is positive or negative, always add additional variables with a “+”. The command ought to lead to the following output: summary(reg_pol1) Call: lm(formula = polity ~ gdppc + hospital, data = europe_2000) Residuals: Min 1Q Median 3Q Max -11.0906 -0.4154 0.2112 1.4259 3.5517 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.134e+01 1.903e+00 5.959 2.05e-06 *** gdppc 7.668e-05 3.934e-05 1.949 0.0614 . hospital -5.828e-01 2.361e-01 -2.469 0.0199 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.742 on 28 degrees of freedom (6 observations deleted due to missingness) Multiple R-squared: 0.3374, Adjusted R-squared: 0.2901 F-statistic: 7.13 on 2 and 28 DF, p-value: 0.003142 nobs(reg_pol1) [1] 31 Ceteris Paribus Let us focus on the coefficient for hospital beds first. Its value is rounded \\(-0.5828\\), implying that for every additional hospital bed per 1,000 people, the Polity V score decreases by \\(0.5828\\) units on average. So far, so good, but as we have included other variables in the regression model, namely per capita GDP, we need to account for this fact in our interpretation. We do this by adding “all other things being equal” (Latin: ceteris paribus) to this interpretation. What does this mean? It means, that if we take into account the level of per capita GDP, and hold this level constant, then for every additional hospita bed, the Polity V score decreases by \\(0.5828\\) units on average. As such, we force the regression model to isolate the effect of hospital beds by including other possible explanatory factors, such as per capita GDP. You will sometimes read this in articles in the form of “controlling for”. As the number of hospital beds only explain the Polity V score partially now, the coefficients in a multiple regression model are also known as partial slope coefficients. To give you a different example: suppose we want to find out whether sex influences income. We could simply run a regression with income as the dependent variable, and sex as the independent variable. But we also know, that age influences income, as with increasing age people have more experience which is reflected in their salary. So even though we are not interested in the amount age influences income, we would include it in the regression model, so as to isolate the effect of the variable we are interested in: sex. Back to our modernisation example and life expectancy. The p-value for this coefficient is \\(0.0199\\), and therefore well below the required 5% threshold. We can conclude that the number of hospital beds influence the level of democracy in Europe in the year 2000. Per capita GDP is rendered insignificant, however. Parsimony But can you just add independent variables at your leisure? The short answer is no. The long answer is: parsimony. This means “as few as possible, as many as necessary”. The “necessary” component is guided by the theoretical underpinning of your investigation. For example, you subscribe to new modernisation theory, and believe that that it is not only economic development in the form of per capita GDP that determines the level of democracy, but that indicators of social change also play an important role. Now it is your job as a researcher to decide how we measure social change. Do we include education? And if so, how do we measure it, say by literacy levels? Should we choose a different measure for health that measures it more directly than hospital beds? Then we might decide on life expectancy. But are these two enough to measure social change, or do we need to look at other facets? We seek to include as few as possible to produce an empirical picture of social change, but so many that we are doing proper justice to the theory. We can then proceed to test different scenarios. For example, does economic development already explain democracy? What happens if we add social change? Or does social change explain democracy on its own, already? These questions lead to the topic of “model specification”, which we will discuss in greater detail in week 4. R-Squared Again As soon as we introduce more than one independent variable to the model, we cannot use “Multiple R-Squared” any more. The reason is that this measure cannot properly take into account added variables. It will either stay the same, or increase, it cannot decrease. This of course, makes no sense, for example if we add average shoe size in 2000 to the analysis, this would not help to explain democracy, but Multiple R-Squared would still likely go up. We therefore need to new measure, called As the number of hospital beds only explain the Polity V score partially now, the coefficients in a multiple regression model are also known as Adjusted R-Squared which not only penalises us for adding more indepenent variables, but will also decrease if a variable takes explanatory power away from a model. You find it here: Limitations OLS is a great method to model the relationship between one or multiple independent variables on a dependent variable. But this only works for cross-sectional data. OLS cannot deal with As the number of hospital beds only explain the Polity V score partially now, the coefficients in a multiple regression model are also known as time-series data a condition that is crucial when we want to model the emergence and survival of democracy. Why? There is a bunch of assumptions that need to be satisfied in order for OLS to deliver us with accurate results, collectively known as the Classical Linear Assumptions (CLM). One of these is that the error terms (our \\(\\epsilon_i\\)s) are not correlated with one another. This assumption is rarely met in time series data, as for example the Polity V score does not miraculously begin to dependent on the number of hospital beds in a given year. It will also depend to an extent on the number of hospital beds in the previous year, and indeed on the Polity V score of the previous year. This would cause autocorrelation in the error terms which is bad news for the CLM. As such, you will not be able to use linear regression in this format in the final assessment! Literature Recommendations Fogarty (2023): Chapter 11, read Chapter 12 if you want to be really good Stinerock (2022): Chapter 12 and Sections 13.1-13.5, 13.8 and 13.10. For more detail see .↩︎ "],["important-disclaimer.html", "Important Disclaimer", " Important Disclaimer I have discussed linear regression with you this week for two reasons: I am trying to sketch the methodological development through which the relationship between economic development and democracy has been analysed over time. Lipset (1959) started with a simple correlation analysis, and this quickly developed into linear regression analysis, as it is more powerful and more sophisticated than correlation analysis. It forms the foundation for the binary response models (probit in our case) which we will start with next week. To start with, we will only look at the probability to be democratic in a particular year across different countries. But in Week 7, we will extend this to not only investigate this relationship across countries, but also over time. This dynamic probit, or Markov Transition Model (MTM), is the model you need to apply in the assessment of the module. Please DO NOT stick to linear regression in the assessment, as OLS is not capable to deal with time-series data. The same applies for a cross-sectional probit. The only method permissible int he assessment is an MTM. "],["homework-1.html", "Homework", " Homework "],["glossary-1.html", "Glossary", " Glossary Table 3: Glossary Week 2 Term Description Methodological adjusted rsquared The coefficient of determination for multiple regression. autocorrelation The value of one error term does not allow us to predict the value of another error term. As such their covariances must be zero causal In order to establish a causal relationship, the following criteria must be met concurrently: &lt;br&gt; &lt;ol&gt;&lt;li&gt;Relevance of the variables within the broader theoretical and empirical context of the research&lt;ol type=“A”&gt;&lt;li&gt;Clear Theoretical Framework&lt;/li&gt;&lt;li&gt;Clear conceptualization&lt;/li&gt;&lt;li&gt;Exclusion of alternative explanations&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Asymmetry&lt;/li&gt;&lt;li&gt;Significant (and sufficiently strong) statistical association&lt;/li&gt;&lt;/ol&gt; ceteris paribus All other things being equal coefficient A coefficient is a numerical expression which is multiplied with the value of a variable coefficient of determination Indicates the proportion of the variation in the dependent variable which is explained through the independent variable. It is defined as \\(\\frac{\\text{Explained Sum of Squares}}{\\text{Total Sum of Squares}}\\) conditional expectation function see Population Regression Function (PRF) confidence interval A confidence interval constructs an interval of numbers which will contain the true parameter of the population (e.g. the mean) in \\((1-\\alpha)\\) times of cases. \\(\\alpha\\) is usually chosen to be small, so that our confidence interval has a probability of 95% or 99% confidence level The confidence level is the probability with which the confidence interval is believed to contain the true parameter of the population and is defined as \\((1-\\alpha)\\) degrees of freedom Degrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary error term The error term quantifies the distance between each observation and the corresponding point on the regression line. The terms are denoted as \\(\\epsilon_{i}\\) intercept The intercept is the point at which the regression line intersects the y-axis. In this book we denote it as \\(\\beta_{0}\\) normal distribution The normal distribution is a bell-shaped probability distribution that is symmetrical around the mean. Approximately 68% of values fall within 1 standard deviation of the mean, 96% within 2 standard deviations, and 99.7% within 3 standard deviations. This is known as the empirical rule. Ordinary Least Squares The method of fitting a regression line by means of minimizing the sum of the squared distances between the observations and the estimated values p-value The p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\). parameter A parameter is the value a statistic would assume in the long run. It is also called the Expected Value partial slope coefficient A partial slope coefficient measures the influence of a variable in multiple regression, holding all other independent variables in the model constant population Collection of all cases which possess certain pre-defined characteristics Population Regression Function The Population Regression Function (PRF) describes the expected distribution of \\(y\\), given the values of the independent variable(s) \\(x\\). It is also called the conditional expectation function (CEF) and can be denoted as \\(E(y_{i}|x_{i})\\) probability distribution Specifies the likelihood of all possible outcomes for a particular variable regression Regression analysis determines the direction and magnitude of influence of one or more independent variables on a dependent variable regression line The regression line describes how the dependent variable is functionally related to the values of the independent variable. It it defined by the intercept \\(\\beta_{0}\\) and the slope \\(\\beta_{1}\\) residual An estimation of the error term. The difference between an observation \\(y_{i}\\) and the estimated value \\(\\hat{y}_{i}\\). Denoted as \\(\\hat{\\epsilon}_{i}\\) sample A sub-group of the population Sample Regression Function A regression line based on a randomly drawn sample significance level The significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9. significance test A significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true slope A slope is defined as rise over run, and so it tells us how many units of y we need to climb (or descend if the slope is negative) for every additional unit of the independent variable \\(x\\) standard error The standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\] t-distribution The t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process. test statistic A test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors. time-series data Time-series data review a certain characteristic over time \\(t\\), where \\(t\\) runs from 1 to \\(T\\) z-score The z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean. Substantive test test "],["flashcards-1.html", "Flashcards", " Flashcards R Functions This Week The data are available as a .csv file. "],["probit---theory.html", "Probit - Theory Probit - What is it?", " Probit - Theory Last week, we encountered linear regression analysis which allowed us to quantify the amount and direction of one or more independent variables on a continuous dependent variable. I already mentioned there, that there is also a type of a regression which can deal with a binary dependent variable. This is usually a yes/no scenario, such as democracy / autocracy, war / peace, trade agreement / no trade agreement, … You get the picture. Many problems or questions in political science have binary outcomes, and so you are about to learn a very important and useful method to answer research questions. As in the previous Chapter, I will take you some through some theory first, and then we are applying the theory to an empirical example. This time concerning the survival of passengers on the Titanic. Probit - What is it? YouTube link to follow Probit - What is it? A question we can all relate to is whether to go out tonight, or not. The “propensity to go out” is not directly observable, and so we call this a latent variable. You can imagine this running from minus infinity to plus infinity, and at some point on this continuum you are making the decision to go out. Let’s call this point tau (\\(\\tau\\)). Graphically, this would look like this: Figure 14: Latent Variable Your inclination to go out, is likely to be influenced by the amount of money you have in your wallet / bank. If you are broke, you will be less inclined (if you are sensible), and if you are swimming in it, you will be more inclined. So, if the “propensity to go out” (which remember is running from minus to plus infinity) is influenced by your budget, then let’s construct a graph, in which we pop the propensity to go out on the y-axis, and the budget on the x-axis. If we assume that this relationship is linear, we can fit a regression line into this coordinate system, just as we have in the previous week: Figure 15: Effect of Budget on Propensity to Go Out Whilst this visualises the influence of the budget on the latent variable, what we are aiming for is to make a prediction about the probability of you going out, or not. Now imagine, your budget is \\(x_{1}\\). The regression line depicts the propensity that we would expect to see, on average, for somebody with a budget of \\(x_{1}\\). But the crucial point is that not everybody is average. Some might have an essay deadline approaching which makes them even less likely to go out. Others might just have recieved their essay mark, and want to celebrate that they scored a first. In other words, there is variability around the regression line. And we assume that whilst this variability is random, it still follows a particular distribution. In the case of a probit, this is the normal distribution5. I have added these distributions to Figure 16. As you can see, even at budget \\(x_{1}\\), some area of the distribution has slipped over the cut-off point, \\(\\tau\\). Figure 16: Towards the Cumulative Density Function, adapted from J. S. Long (1997, p. 46) The probability of going out is coloured in in grey. You can see that even at \\(x_{1}\\) there is a teeny bit of probability that you will go out. As the budget increases, more and more probability slides over the threshold \\(\\tau\\), until we reach the magical point of \\(x_{5}\\) where the probability is 50%. From there on, the amount of probability sliding over \\(\\tau\\) is steadily decreasing, because of the shape of the normal distribution. We can depict the amount of probability (or the size of the grey area) for each \\(x_{i}\\) in a separate graph which is called the Cumulative Probability Density Function, or short CDF: Figure 17: The Cumulative Distribution Function, adapted from J. S. Long (1997, p. 46) This s-shaped curve now gives us the probability (of going out) for each \\(x_{i}\\) (budget). It is important to note that the relationship is not linear, as in linear regression. Because we have an s-shaped curve the increase in probability when going from \\(x_{2}\\) to \\(x_{3}\\) is not the same as going from \\(x_{3}\\) to \\(x_{4}\\). You can see that visualised here: Figure 18: Marginal Effect under the Cumulative Density Function We will therefore not be able to interpret the coefficients in the same way as for OLS. We will be using predicted probabilities instead. But one step at a time. Let’s first get our hands dirty with some data. For logit, the logistic distribution is used. This would lead to very similar results.↩︎ "],["probit-application.html", "Probit – Application The Command The Coefficients Pedicted Probabilities How to Report Results Literature Recommendations", " Probit – Application The Command The command to run a probit is as follows: model &lt;- glm(depvar ~ indepvar data = dataframe, family = binomial(link = &quot;probit&quot;)) glm stands for “General Linear Model”. We then specify a model in the same way as last week, stating the dependent variable first, followed by a tilde and then the independent variable(s). We complete the command by naming the data frame we wish to use and selecting the model type, in our case it is a probit model which is binomial (our dependent variable only has two possible outcomes). Once again, let us do a specific example with the data set “Europe” in the year 2000. First set the working directory for today’s seminar: setwd(&quot;~/Warwick/Modules/PO33Q/Week 3&quot;) and then load the data set into a data frame called world which we subset to the year 2000. world &lt;- read.csv(&quot;files/Week 3/world.csv&quot;) library(tidyverse) world2000 &lt;- filter(world, year==2000) For a probit regression, we cannot use the Polity V scale any more, because it is continuous, and not binary. We therefore switch the democracy coding to index created by Boix, Miller &amp; Rosato in 2018. We will use life expectancy as our independent variable, and examine the situation in the year 2000. To do this, we call: probit &lt;- glm(democracy ~ life, data = world2000, family = binomial(link = &quot;probit&quot;)) This should lead to the following results: summary(probit) Call: glm(formula = democracy ~ life, family = binomial(link = &quot;probit&quot;), data = world2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -3.73066 0.72586 -5.140 2.75e-07 *** life 0.05793 0.01078 5.375 7.66e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 252.11 on 182 degrees of freedom Residual deviance: 220.10 on 181 degrees of freedom (8 observations deleted due to missingness) AIC: 224.1 Number of Fisher Scoring iterations: 4 The Coefficients The coefficients are displayed in the results as follows: But how do we interpret them? As you have seen in the Theory section above, the relationship between our independent variable and the probability of democracy is not linear: the curve was s-shaped, so that the increment in probability is not the same for, say moving from \\(x_{1}\\) to \\(x_{2}\\) and from \\(x_{2}\\) to \\(x_{3}\\). For illustration see the red bars in the following Figure: What we therefore need to do in the world of probit, is to evaluate the probability at individual values of \\(x_{i}\\), or put differently, assess how much of the bell-shaped curve has slid across our cut-off point \\(\\tau\\) (tau) at a particular point \\(x_{i}\\). And this brings us to predicted probabilities. Pedicted Probabilities Once we have estimated the model, we have determined the shape of the s-shaped curve at the start of the chapter. What we now need to do, is to evaluate the probability on the y-axis for different values on the x-axis. In our case this is the variable life. Setting the x-values Let us first get a basic overview of the variable life. We can do this by calling summary(world2000$life) Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s 44.52 59.46 69.58 66.49 73.84 81.08 8 So we know that the average life expectancy in the world in the year 2000 was 66.49 years,with a minimum of 44.52 years, and a maximum of 81 years. We also have eight missing observations (NA) which we need to exclude from the following by setting na.rm=TRUE6, as R is otherwise unable to calculate descriptive measures within the setx function, such as the mean. Let us set life to its mean now, by typing: setx = data.frame(life=66.49) If you want to change this value for calculations later on, you simply set different values in the setx dataframe or specify a new one). We now have the shape of the probability curve, and we have agreed on a point on the x-axis. We are finally ready to have a look at the probability to be a democracy at this point. Predicting the Probability The quantity of interest we are interested in is the probability of being a democracy. To calculate this quantity of interest, we use the function predict() and specify within this function for which model we want to calculate the quantity, and the data frame in which we specified the value of our x-variable. In our case this variable is life and we have assigned the mean value to an object called setx earlier. predict(probit, setx, type=&quot;response&quot;) 1 0.5481741 Remember that we have put the value of life expectancy at the mean. For this level of life expectancy R returns to us a probability of being a democracy at 54.82%. Contrarily, this means that probability to be an autocracy is 45.18% (the two probabilities always sum up to 1). Now we set life expectancy to its minimum setx = data.frame(life=min(world2000$life, na.rm = T)) and calculate the quantity of interest again. Our probabilities have changed very drastically (ensure you get the same results before proceeding): predict(probit, setx, type=&quot;response&quot;) 1 0.124708 If we set life expectancy to its maximum (how?), then we receive the following regime probabilities: predict(probit, setx, type=&quot;response&quot;) 1 0.8329802 Now we can make statements such as: In 2000, a country’s probability to be a democracy with average life expectancy was 54.82%. At the minimum life expectancy of 44.52 years, this probability drops by 42.35%age points to 12.47%. As such, a country in 2000 at minimum life expectancy would more likely be an autocracy than a democracy (why?). All Predicted Probabilities If you want to create a graph which depicts the predicted probability for all value sof your independent variable, then you can download the RScript for creating such a graph here. It will look like this: How to Report Results The question is now: how do you report all of this in an article, or closer to home, in your assessment? Let us start by calculating a model which assesses the impact of per capita GDP on the probability to be a democracy in 2000, worldwide. The R commands and output look like this: probit &lt;- glm(democracy ~ gdppc, data = world2000, family = binomial(link = &quot;probit&quot;)) summary(probit) Call: glm(formula = democracy ~ gdppc, family = binomial(link = &quot;probit&quot;), data = world2000) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.621e-01 1.161e-01 -1.396 0.163 gdppc 5.128e-05 1.309e-05 3.918 8.93e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 248.48 on 180 degrees of freedom Residual deviance: 225.62 on 179 degrees of freedom (10 observations deleted due to missingness) AIC: 229.62 Number of Fisher Scoring iterations: 5 Now, please, please never, ever copy this into an article or an assessment, as every time you do this, a little part of me dies. Make the effort of reporting the results in a nice and neat Table, that only contains all relevant information, communicates it in an accessible form, and is clearly labelled. The output above, processed properly, would look like this:     Table 4: Influence of per capita GDP on Democracy Dependent variable: Democracy per capita GDP 0.058*** (0.011) Constant -3.731*** (0.726) Observations 183 Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01     You see, this table manages to convey clearly the relationship we are assessing, the label of the independent variable (not in the form of cryptic variable names), the value of the intercept, the value of the slope coefficient, their respective p-value, and the number of observations. It also has an informative caption underneath and is properly labelled (so that it can be cross-referenced in the text). As such, the table itself could already communicate the main take-away message without somebody looking at the text. This needs to be the goal: the table, or figure needs to be able to communicate the message without having to look at the text. In turn, the text needs to be able to communicate the message on its own without looking at the table. But both need to say the same thing. After you have reported the results of the regression output in this way, you can then proceed to interpret (in the text) the transition probabilities as discussed above. Again, you might wish to sum up the main results in tabular format. Stargazer Rather than setting tables manually in Excel, or even worse in Word (it basically violates all standards of professional table formatting), you can let R do this work for you. All you need is a magical package called stargazer. I have provided some Sample Stargazer Code in the Downloads Section. Open it now, and try to understand it, I have annotated it. But here are some pointers: The stargazer function needs to contain, in order: the name of the R object where you stored the regression results, the option header=F to suppress the annoying immortalisation of the author, the option type=\"html\", and the option out=\"documentname.doc\" which places a word document with that file name in your working directory. Should you use MS Word to write your essay and your essay is saved in your working directory, do not save the table document under the same name as your essay, as R will overwrite it and it will be gone forever. Beyond these basic elements I have added some lines in the sample code to improve the table. For example, I replaced the variable names with the variable labels, and suppressed unwanted statistics. You could also add a name for the model, or give the table a title. There is some good documenttation available for this on this cheat sheet. It is worth investing some time into this. There is a learning curve at the beginning, but it will make your life so much easier further down the line. Multiple Independent Variables As in multiple linear regression, you can also have multiple independent variables in a probit model. Schematically this would look as follows: probit &lt;- glm(depvar ~ indepvar1 + indepvar2 + indepvar3, data = dataframe, family = binomial(link = &quot;probit&quot;), The fun starts when you begin to calculate probabilities for different values for all of these independent variables. Setting the x-values You can set the value for each variable individually, such as in this little example: setx = data.frame(gdppc=min(world2000$gdppc, na.rm = T), life=66.49) Predicting the Probability This is very much the same as above with one independent variable. It is crucial to remember which variables you have set at which value, so that you can interpret the probability values correctly. As in linear regression, the interpretation of individual variables is ceteris paribus. It therefore makes sense, if you wish to isolate the effect of one, single variable, only to vary the values of that variable, and to leave all the other ones the same. If you varied two variables at the same time, for example, you would no doubt see a change in probability, but you could not attribute it to a single independent variable, any more. Literature Recommendations Fogarty (2023): Chapter 13, pp. 268-295 Stinerock (2022): Section 13.12 J. S. Long (1997); More Technical and in-depth if you want some background This stands for “not available remove equals true”.↩︎ "],["exercises.html", "Exercises", " Exercises Use the global data set. Estimate a model assessing the impact of per capita GDP and “Adjusted net enrolment rate, primary (% of primary school age children)” in 2000. Design a Table to report results in MS Word. What is the probability to be a democracy evaluated at the mean for both independent variables? What is the difference in probability ceteris paribus for minimum and maximum per capita GDP? What happens if we replace net enrolment rate by life expectancy in the model? "],["homework-2.html", "Homework", " Homework "],["glossary-2.html", "Glossary", " Glossary Table 5: Glossary Week 3 Term Description Methodological dichotomous Can only assume two mutually exclusive, but internally homogeneous qualitative categories generalized linear model a family of models which allows us to estimate a linear relationship between dependent and independent variables even though the underlying relationship is not linear by means of a link function. It also allows us to vary the assumed distribution of our error terms latent variable Variables that can only be inferred indirectly through a mathematical model from other observable variables that can be directly observed or measured likelihood Likelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric model link function Converts the linear part of a generalized linear model into a probability normal distribution The normal distribution is a bell-shaped probability distribution that is symmetrical around the mean. Approximately 68% of values fall within 1 standard deviation of the mean, 96% within 2 standard deviations, and 99.7% within 3 standard deviations. This is known as the empirical rule. predicted probabilities The value of a predicted probability is equal to the cumulative density function, \\(F(X^\\prime\\beta)\\), evaluated at a chosen value of the independent variable probability Refers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring probability distribution Specifies the likelihood of all possible outcomes for a particular variable Substantive test test "],["flashcards-2.html", "Flashcards", " Flashcards R Functions This Week The data are available as a .csv file. "],["missing-data.html", "Missing Data Options in R Methodological Implications", " Missing Data Options in R Missing data is a real problem in doing empirical research. What compounds this problem is that developing countries are more affected than developed ones. Ironically, we are more interested in developing countries in this module, and so this is going to become a real-life struggle for you over the coming weeks. R has multiple ways in which to address missing data. We have already encountered the exclusion of missing values when using descriptive statistics within the setx function in Week 3, such as: mean(europe2000$lifeexp, na.rm=TRUE) When we run regression models, for example through the function lm, R let’s you specify what you wish to do with observations (rows) that have missing values in them with the function na.action. This option has four logical options7: na.fail: Stop if any missing values are encountered na.omit: Drop out any rows with missing values anywhere in them and forgets them forever. na.exclude: Drop out rows with missing values, but keeps track of where they were (so that when you make predictions, for example, you end up with a vector whose length is that of the original response.) na.pass: Take no action By default, R would apply na.omit which we call “listwise deletion”. This means, that as soon as one value within an observation is missing, R drops that entire observation. This can potentially decimate the number of observations for analysis quite drastically! This in turn has implications for inference. Let me illustrate this. Methodological Implications In an ideal world, we would have a data set in which each value is present (indicated by the presence of an x in each of the cells of this table): Unfortunately, in the real world, we are much more likely to have a Swiss cheese, such as this: Figure 19: Schematic of a real data set You will already have seen this when opening the data sets we are working with. In Figure 20, for example, the variable gdppc only has three missing values, but agri is completely missing. Figure 20: Excerpt from the world data set If you ran a regression model that uses the variable agri listwise deletion would delete all observations and you would end up with no model. Here, the amount of observations you would lose is obvious. But since different variables have missing values in different places, it is often not as obvious as that. Assume, for example that we wished to use all four variables in our Schematic in Figure 19. Even though the “missingness” problem looks much less pronounced than in the agri variable in the world data set (see Figure 20, listwise deletion would ensure that we would only be left with two observations out of ten in our model: So what do you do when you find yourself in such a situation? An obvious answer would be to simply leave out a variable that has a high degree of missingness. This solution is flawed, however. If your theory tells you that the variable plays a role in measuring one of the concepts then leaving it out would create so-called omitted variable bias – a problem you really don’t want to have. A much better solution, therefore, is to use a proxy variable. Take agri as an example. It measures the percentage of GDP generated by agriculture, and would probably enter a statistical model to measure the transition of a traditional society (agricultural) to a modern, industrialised society. So, rather then measuring the move away from agri, you would also measure the transformation into industry. If you can find a variable measuring the percentage of GDP generated by the industry sector, and this variable has fewer observations missing, you could you it instead of agri and still be measuring the same thing. Sometimes, it is not as straightforward as this, however, and you will have to resort to variables that have less measurement validity. Assume, for example, you want to measure the populations ability to read. Then literacy from our data set would be an obvious choice. But sadly, it has a high number of observations missing. An alternative is primary gross enrolment (enrol_gross), as pupils learn to read and write in primary school. This variable does not measure the characteristic we are interested in as directly as literacy, but it would allow us to include more observations in our model. It is an art to balance measurement validity against the number of observations, there is no hard and fast rule to help you make this decision. But as long as you discuss and explain your reasoning in the assessment, you will be fine for this module – after all, you are only starting out on this. https://faculty.nps.edu/sebuttre/home/R/missings.html↩︎ "],["model-building.html", "Model Building Control Variables Literature Recommendations", " Model Building Model Building YouTube link to follow When we are testing a theory, such as modernisation theory, then we usually have a plethora of independent variables to measure the concepts involved in that theory. Take economic development. In its origins, modernisation theory saw development almost purely as economic growth, a notion enshrined in what we refer to now as Classical Modernisation Theory. But as our understanding of what constitutes “development” has changed over time, so have the propositions of modernisation theory. Diamond (1992) made an important contribution to the literature by proposing that wealth, such as per capita GDP, really is only a means to facilitate social change, such as increasing education levels, allowing people to look after their health, etc. He posited that it is these social changes that facilitate democracy. Wealth is necessary, but it is only a first step. Modernisation theory, therefore, can be seen as having to parts, the classical, and the new one. When you look at journal articlaes, you will notice that these parts are usually tested separately in regression models, first just the classical component, then just the social component, then both of them together. This is an important strategy in regressiona nalysis, as it allows us to isoloate the explanatory power of not only each part of the theory, but also of individual variables. Let me illustrate what I mean by this. First, we need to decide how to actually measure “economic development” and “social change”. There are a lot of options: Economic Development GDP Agricultural Land Access to electricity Mobile Phone Subscriptions … Social Development Education: literacy, primary completion rate, school enrolment, etc. Health: life expectancy, tuberculosis incidents, health expenditure, etc. … Once we have selected our variables, we start by runnign bivariate models between each independent variable and democracy as the dependent variable. This give sus information if each variable is individually able to explain democracy, and if so, how well. Then we start to combine independent variables into multivariate models, to test different combinations. This will allow us to see if one independent variable might take explanatory power away from another, and as such explains democracy better. I have done this in the following table: Table 6: Impact of Socio-Economic Development on Democracy Dependent variable: Polity V (1) (2) (3) per capita GDP 0.0001*** 0.0001* (0.00003) (0.00003) Life Expectancy 0.160*** 0.090 (0.054) (0.066) Constant 2.584*** -7.373** -3.248 (0.605) (3.691) (4.339) Observations 150 152 150 R2 0.064 0.056 0.075 Adjusted R2 0.057 0.050 0.063 Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 As you can see, both per capita GDP and life expectancy can explain variation in the Polity V score in 2007 (Models 1 and 2, repsectively). But when we combine the two in Model 3, life expectancy loses its significance, whilst per capita retains (an admittedly lower level of) significance. In these three models, we have thus discovered evidence that Diamond was wrong: When we look at the role of per capita GDP and life expectancy simultaneously, per capita GDP is able to explain democracy whilst life expectancy is not. How would you make use of this in the assessment? It goes without saying that you will have to run a lot of models to find such a story. But it would make little sense to overburden the reader of your assessment with all of these models (quite apart from constraints on your word count). Instead, you would only include in your assessment those regression models in the results tables which allow you to tell this story. Note that in this case, I have also quite neatly tested both the classical and new approach to mdoernisation theory. Model 1 is classical modernisation, Models 2 and 3 test new modernisation theory. We have found evidence that classical modernisation theory is more applicable in this scenario than new modernisation. Control Variables The selection of independent variables MUST be guided by theory. After all, this is our purpose in running regression models: finding out whether a particular theory can explain an empirical phenomenon we are witnessing. If you select variables that are irrelevant to your theory, then the research design breaks down, as you are no longer focusing on testing your theory. I know it sounds trivial, but you would be surprised how many students mess this up in assessments. But there is one – and only one – exception to the rule of not including variables that are not motivated by the theory you are testing: so-called control variables. For example, we suspect that the amount of official development aid (ODA) a country has received might influence democracy. Modernisation theory is only concerned with processed WITHIN a country, and so a flow of money coming from outside of the country is not part of the theory. Nonetheless, such funds can either facilitate development, in the sense of helping to build infrastructure such as roads, etc. Or it can be used for democracy promotion directly (the World Bank does tie some of its ODA funds to this purpose), through funding relevant institutions, facilitating elections, etc. So, even though these external funds have no place in our theory, we have strong reason to believe that they affect the dependent variable. And it is for this reason, that we would still include them in our regression model to control for their influence. Why? The principle of ceteris paribus (see Week 3) only applies with respect to those variables included in the model. And if we wish to control for ODA, or to purify those coefficients which are motivated by theory from the influence of ODA, we need to include this variable in the model in order to do so. Even though I am presenting this here as the last step, you would realistically select these variables together with the ones motivated by the theory, as you will also have to perform conceptualisation and measurement, check data availability, etc. Literature Recommendations Fogarty (2023): Chapter 11: Section “Multiple Regression and Model Building” Stinerock (2022): Section 13.11 "],["homework-3.html", "Homework", " Homework "],["glossary-3.html", "Glossary", " Glossary Table 7: Glossary Week 4 Term Description Test Test "],["flashcards-3.html", "Flashcards", " Flashcards R Functions This Week The data are available as a .csv file. "],["consolidation.html", "Consolidation The World-Value Survey", " Consolidation There is no new methodological material to give you a bit of a rest. Instead, we will connect what we have learned so far in terms of methods with the theory of cultural modernisation. For our investigation we will be using the World Value Survey. The World-Value Survey The World Values Survey () is a global network of social scientists studying changing values and their impact on social and political life, led by an international team of scholars, with the WVS Association and WVSA Secretariat headquartered in Vienna, Austria. The survey, which started in 1981, seeks to use the most rigorous, high-quality research designs in each country. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world?s population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values ever executed, currently including interviews with almost 400,000 respondents. Moreover the WVS is the only academic study covering the full range of global variations, from very poor to very rich countries, in all of the world’s major cultural zones. The WVS has taken place across six waves, each collecting data from a different set of countries: "],["exercises-1.html", "Exercises Descriptives Linear Regression Probit", " Exercises Descriptives For all exercises, use the “WVS” data set which is available in the Downloads Section. wvs &lt;- read_dta(&quot;wvs.csv&quot;) What is the average value for survival/self-expression values? What does this mean? Filtering through Waves, how has this average changed? Repeat these two steps with traditional/rational values. Why is the comparison of these values over time difficult? Linear Regression For which waves do traditional–rational values in the population explain a country’s level of democracy? For which waves do survival–self-expression values in the population explain a country’s level of democracy? Identify reasons why earlier waves fail to explain the level of democracy. Assessed jointly, how much more or less democratic do survival/self-expression value and GDP growth make countries in wave 5 (2005-2009)? Probit For which waves do traditional–rational values in the population explain a country’s probability to be a democracy? For which waves do survival–self-expression values in the population explain a country’s probability to be a democracy? Identify reasons why earlier waves fail to explain regime type. Calculate a model assessing the impact of GDP growth on the probability of democracy for countries in wave 5. How much less likely is a country to be a democracy moving from minimum GDP growth to maximum GDP growth in wave 5? Calculate a model assessing the impact of traditional–rational values, and GDP growth on the probability of democracy for countries in wave 5. Interpret the results. Considering the main propositions of cultural modernisation, what are the implications of these results? Assess the results of each of the previous tasks in turn. ## Solutions {-} You can find the solutions to these exercises in the Downloads Section. "],["homework-4.html", "Homework", " Homework "],["glossary-4.html", "Glossary", " Glossary Table 8: Glossary Week 5 Term Description Test Test "],["flashcards-4.html", "Flashcards", " Flashcards R Functions This Week The data are available as a .csv file. "],["markov-transition-models.html", "Markov Transition Models Time Series, Cross-Sectional Data What are Markov Transition Models? Democratic Emergence Democratic Survival Interpretation", " Markov Transition Models Time Series, Cross-Sectional Data We are now entering the real world, as we will start to look at how characteristics vary, not only across different countries, but also across time. I have already presented the structure of time-series, cross-sectional (TSCS) data in Week 1, but to jog your memory, here it is again: As you can see, each country has multiple observations, one for each year in which data have been observed. We will be using this information this week to calculate probabilities of regime transitions. For example, what is the probability of a country to transition from autocracy to democracy? As we only observe the regime type once for each year, this question alludes to the difference in regime type between two years in the same country. We will do this with a so-called Markov Transition Model. What are Markov Transition Models? A Markov Transition Model (MTM) models the probability of being a democracy in a particular year, given its regime type in the previous year. So, if we model the probability of a country to be a democracy this year, given that it was an autocracy in the previous year, we are modelling the probability of a transition from autocracy to democracy. Similarly, if we model the probability of a country to be a democracy this year, given that it was also a democracy in the previous year, we are modelling the probability of democratic survival. We can express these probabilities as so-called conditional probabilities. Conditional Probabilities Conditional probabilities express what I have described before in a formal way. The conditional probability to model for democratic emergence is written as follows: \\[\\begin{equation} P(y_{i,t} = 1 | y_{i, t-1} = 0) \\end{equation}\\] This reads: The probability of a country i to be a democracy (y=1) in year t, given (this is what the vertical line | says) that country i was an autocracy (y=0) in the previous year (t-1). Analogously, the conditional probability for democratic survival is: \\[\\begin{equation} P(y_{i,t} = 1 | y_{i, t-1} = 1) \\end{equation}\\] This reads: The probability of a country i to be a democracy (y=1) in year t, given that country i was also a democracy (y=1) in the previous year (t-1). We will now apply this knowledge to create a model in R which calculates these conditional probabilities. Let’s start with democratic emergence. Democratic Emergence Democratic emergence is expressed as the probability of a country to be a democracy in year \\(t\\), given that it was a dictatorship in the previous year, \\(t-1\\) \\[\\begin{equation*} P(y_{i,t} = 1 | y_{i, t-1} = 0) \\end{equation*}\\] As a first step, we therefore need a variable that gives us the information which regime type each of our countries had in the previous year. We will use the world data set for this illustration. world &lt;- read.csv(&quot;files/Week 7/world.csv&quot;) library(tidyverse) We can now create the lagged democracy value. In order for R to know when a new country “starts”, we need to group observations by country first, and then lag the variable democracy. We do this with the intuitively called function lag(). We then ungroup the data again, as we have no need of the groups any more. world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.democracy = lag(democracy)) %&gt;% ungroup() This creates a new variable l.democracy which gives us the information we were after: the regime type of the country in the previous year. Note that as the first observation for each country cannot be lagged, it creates as many missing values as we have countries in the data set. Perhaps this makes more sense with a visualisation: Lagging the Independent Variables So far, we have lagged the dependent variable, to run an MTM. This was a methodological necessity. But from a substantive point of view it makes sense to also lag our independent variables. It is reasonable to assume, that the regime type in year \\(t\\) depends on the state of socio-economic development in the previous year, \\(t-1\\). It is rare that for example a recession hits, and the country immediately changes regime type. These things need time. And this is why we will now also lag the independent variables. This is the same procedure as before. For “per capita GDP” we call: world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.gdppc = lag(gdppc)) %&gt;% ungroup() Let’s do the same for life expectancy: world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.life = lag(life)) %&gt;% ungroup() and Primary gross enrolment rate: world &lt;- world %&gt;% group_by(countrycode) %&gt;% mutate(l.enrol_gross = lag(enrol_gross)) %&gt;% ungroup() Subsetting the Data for Conditional Probabilities Recall that for democratic emergence we need all of those observations in which a country has been an autocracy in the previous year. We can select these in R by filtering the data: world_democ0 &lt;- filter(world, l.democracy==0) Whilst we are at it, we might as well also do this for democratic survival. Here we need all observations in which l.democracy=1 world_democ1 &lt;- filter(world, l.democracy==1) And with that we are ready! Democratic Emergence Even though “Markov Transition Model” sounds very complicated, the code in R is actually no different from a “regular” probit model. The only difference is that we have selected a certain type of observations. For emergence we call: emergence &lt;- glm(democracy ~ l.gdppc, data = world_democ0, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(emergence) Call: glm(formula = democracy ~ l.gdppc, family = binomial(link = &quot;probit&quot;), data = world_democ0, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.967e+00 5.170e-02 -38.052 &lt;2e-16 *** l.gdppc -3.201e-05 1.558e-05 -2.054 0.04 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 794.78 on 3879 degrees of freedom Residual deviance: 787.79 on 3878 degrees of freedom (1099 observations deleted due to missingness) AIC: 791.79 Number of Fisher Scoring iterations: 8 Once again, we can include multiple independent variables by connecting these with + in the glm() function: emergence_full &lt;- glm(democracy ~ l.gdppc + l.life + l.enrol_gross, data = world_democ0, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(emergence_full) Call: glm(formula = democracy ~ l.gdppc + l.life + l.enrol_gross, family = binomial(link = &quot;probit&quot;), data = world_democ0, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -2.706e+00 3.683e-01 -7.346 2.05e-13 *** l.gdppc -7.424e-05 2.906e-05 -2.554 0.0106 * l.life 1.810e-02 7.848e-03 2.306 0.0211 * l.enrol_gross -2.358e-03 2.510e-03 -0.939 0.3476 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 664.33 on 2845 degrees of freedom Residual deviance: 650.69 on 2842 degrees of freedom (2133 observations deleted due to missingness) AIC: 658.69 Number of Fisher Scoring iterations: 9 Democratic Survival Modelling democratic survival is easy now, as we have already completed all necessary preparations. To model \\[\\begin{equation*} P(y_{i,t} = 1 | y_{i, t-1} = 1) \\end{equation*}\\] we call: survival &lt;- glm(democracy ~ l.gdppc, data = world_democ1, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(survival) Call: glm(formula = democracy ~ l.gdppc, family = binomial(link = &quot;probit&quot;), data = world_democ1, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.627e+00 8.555e-02 19.019 &lt; 2e-16 *** l.gdppc 2.268e-04 4.788e-05 4.738 2.16e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 583.28 on 3791 degrees of freedom Residual deviance: 503.78 on 3790 degrees of freedom (385 observations deleted due to missingness) AIC: 507.78 Number of Fisher Scoring iterations: 11 And again with all independent variables: survival_full &lt;- glm(democracy ~ l.gdppc + l.life + l.enrol_gross, data = world_democ1, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) summary(survival_full) Call: glm(formula = democracy ~ l.gdppc + l.life + l.enrol_gross, family = binomial(link = &quot;probit&quot;), data = world_democ1, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 8.398e-01 6.069e-01 1.384 0.16645 l.gdppc 1.373e-04 4.842e-05 2.836 0.00457 ** l.life 7.953e-03 1.182e-02 0.673 0.50122 l.enrol_gross 4.130e-03 3.675e-03 1.124 0.26105 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 414.00 on 2915 degrees of freedom Residual deviance: 357.24 on 2912 degrees of freedom (1261 observations deleted due to missingness) AIC: 365.24 Number of Fisher Scoring iterations: 11 Interpretation The interpretation is analogous to a cross-sectional probit model we explored in Week 3. Just bear in mind that the predicted probabilities are all indicating the probability for democracy to emerge, or for democracy to survive. "],["model-fit-in-binary-response-models.html", "Model Fit in Binary Response Models8 How to Measure Model Fit", " Model Fit in Binary Response Models8 How to Measure Model Fit There are quite a few model fit measures around for binary response models, such as pseudo R-Squared and the Akaike Information Criterion (AIC). What I am going to present here, however, is the so-called ROC curve, as I personally really don’t like the aforementioned measures. What is it? The ROC Curve YouTube link to follow The principle idea of ROC is to determine how well our model is able to separate cases into the two categories of our dependent variable. It approaches this question by comparing the actual observed values of the dependent variable with the values the model would predict, given the values of the independent variables. Consider Figure 21 a). On the x-axis you see eight observations, marked as coloured diamonds. Red diamonds represent countries which are dictatorships, and blue ones democracies. They are sorted by their respective level of per capita GDP. Now suppose that the displayed CDF is the results of a model we have estimated. With a cut off point \\(\\tau=0.5\\) we would correctly predict the group of four observations on the left to be dictatorships. They are True Negatives (TN). The group on the right would be correctly predicted as democracies, these are True Positives (TP). We have no incorrectly classified cases; our model has been able to separate cases perfectly. You can see this represented in the form of distributions in panel b). Figure 21: Perfect Separation As you well know by now, the real world is oddly deficient in achieving perfection such as this. We will observe both poor democracies, and rich dictatorships. This scenario is shown in Figure 22 a). Figure 22: Overlap According to the CDF we would predict the poor democracy as a dictatorship. It would be a False Negative (FN). Conversely, we would predict the rich dictatorship as a democracy and would obtain a False Positive (FP). The distribution of cases in Figure 22 b) is not as clearly separated any more as in Figure 21 b). Now they overlap, leading to incorrect classifications. These are marked accordingly in Figure 23. As we are no longer operating in a world in which we only have TNs and TPs, I think we can all agree that our model fit is no longer as good as in Figure 21. Figure 23: False Negatives and False Positives But there is another issue: whilst setting \\(\\tau\\) at 0.5 makes intuitive sense, there is nothing preventing us from shifting \\(\\tau\\) around. Indeed, the number of FPs and FNs very much depends on where we place our cut-off point. For example, if we don’t want any FNs, then we just have to shift \\(\\tau\\) sufficiently downwards. Or if we want to avoid FPs we only need to move it far enough upwards. I have illustrated this in Figure 24 a) and b), respectively. Figure 24: Shifting \\(\\tau\\) These are only three options of placing the threshold. But we have a total of eight observations which means that there are nine potential positions for the cut-off point with each leading to a different conclusion of how well our model fits the observed data. Let’s go through this more systematically and start by placing \\(\\tau\\) at the very bottom. Because the number of TNs, TPs, FNs, and FPs will only change at the “next” observation we only need to shift as many times as there are observations. For each shift we record the values of all four quantities in a crosstabulation which is displayed in the following table: These tables are useful in their own right, but since we will end up with as many of them as there are observations, this can quickly get very messy. There are only eight observations here, but imagine doing these for the world data set with 190 observationsWe therefore need to find a way to condense the information contained in each of the n confusion matrices into a single measure. If you look closely, you will see that TNs and FPs form a close relationship: if we shift up \\(\\tau\\) in Figure 24 a) we are reducing the number of FPs and obtain more TNs. As TPs relate in the same way to FNs, we can quantify their respective relationships in the following rates: \\[\\begin{equation} \\text{False Positive Rate}=\\text{Sensitivity}=\\frac{\\text{False Positives}}{\\text{False Positives}+\\text{True Negatives}} \\end{equation}\\] In our case you can think of the False Positive Rate (FPR) as the proportion of incorrectly specified dictatorships. \\[\\begin{align} \\text{True Positive Rate}&amp;=1-\\text{Specificity}=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}\\\\[10pt] \\text{Specificity} &amp;=1-\\text{True Positive Rate}=1-\\frac{\\text{TP}}{\\text{TP}+\\text{FN}} \\nonumber \\end{align}\\] For our example the True Positive Rate (TPR) represents the proportion of correctly specified democracies. If we calculate these rates for each of our n confusion matrices, we are already reducing four quantities into two. Let’s do it. To provide a visual aid in this rather laborious process, I have created Figure 25 which depicts all eight shifts. Figure 25: Towards the ROC Curve The following table displays the confusion matrix for each of the panels in Figure 25, as well as the respective TPR and FPR (you are welcome). As we only need the TPRs and FPRs going forward, it makes sense to collect these in their own little table: We are nearly there! The last step is to display all these values in the form of a curve with the TPR on the y-axis, and the FPR on the x-axis. You can see the result – the ROC curve – in Figure 26 a). Figure 26: ROC Curve Note that I have added a diagonal where TPR = FPR. This is sometimes described as a model without independent variables. I like to think of the line as the graphical point where our model would not be able to separate between the two categories, at all. The further the ROC curve is away from the diagonal, the better our model is at separating the two categories. But there are two sides to the diagonal. We want it to be above the diagonal, as here the model is predicting 0s as 0s and 1s as 1s. Underneath, the prediction is inverse and 0s are predicted as 1s, and 1s are predicted as 0s. To summarize the position of the curve into a numerical expression, the Area Under Curve (AUC) is used, as shown in Figure 26 b). If the area is 100% we are correctly predicting everything. At 50% the model is incapable of separation, and at 0% the model gets everything wrong. This is very useful to compare different models. R How do you do all of this in R? Let’s start with the simple emergence model. In order to calculate a ROC curve, we need a new package, called pROC. Install it and load it. library(pROC) To calculate the ROC curve, we need a few steps: prob_em &lt;- predict(emergence, type=&quot;response&quot;) world_democ0$prob_em &lt;- unlist(prob_em) roc &lt;- roc(world_democ0$democracy, world_democ0$prob_em) auc(roc) Area under the curve: 0.5048 What does each line do? predict the probability of democratic emergence for each observation (country year) and store them in a new vector called prob_em. add the vector prob_em to the data frame world_democ0. In order to do this, we need to unlist the values in the prob_em vector. we then call the roc function inw hich we compare the predicted probabilities (world_democ0$prob_em) with the actual, observed regime type (world_democ0$democracy) and store the result in an object called roc as a last step we calculate the area under the curve with the auc() function But we can also plot the ROC curve: plot(roc, print.auc=TRUE) Note that print.auc=TRUE prints the numerical value into this plot. You can suppress it by setting it to FALSE. Let’s follow this procedure for the other models. Emergence: Full Model prob_em_full &lt;- predict(emergence_full, type=&quot;response&quot;) world_democ0$prob_em_full &lt;- unlist(prob_em_full) roc &lt;- roc(world_democ0$democracy, world_democ0$prob_em_full) auc(roc) Area under the curve: 0.5932 plot(roc, print.auc=TRUE) Survival prob_sur &lt;- predict(survival, type=&quot;response&quot;) world_democ1$prob_sur &lt;- unlist(prob_sur) roc &lt;- roc(world_democ1$democracy, world_democ1$prob_sur) auc(roc) Area under the curve: 0.8317 plot(roc, print.auc=TRUE) Survival: Full Model prob_sur_full &lt;- predict(survival_full, type=&quot;response&quot;) world_democ1$prob_sur_full &lt;- unlist(prob_sur_full) roc &lt;- roc(world_democ1$democracy, world_democ1$prob_sur_full) auc(roc) Area under the curve: 0.8165 plot(roc, print.auc=TRUE) This is a verbatim reproduction from Reiche (forthcoming). The text is based on an explanatory video by StatQuest with Josh Starmer. All figures are copyrighted.↩︎ "],["joint-estimation-of-emergence-and-survival.html", "Joint Estimation of Emergence and Survival Rationale Example Application in R Literature Recommendations", " Joint Estimation of Emergence and Survival Rationale It is perfectly adequate to estimate the processes of democratic emergence and survival separately. There is a more elegant approach, however, in which emergence and survival are estimated together. This is often employed in the literature, for example in the article by Boix &amp; Stokes (2003). The rationale here is that the model will, overall, use more observations and therefore this might influence standard errors and thus statistical significance. As you will see, the joint estimation requires a lot of manual calculation and general faff. I am mainly showing you this here, so that you can understand the output in aforementioned articles. If you want to employ this yourself, then you could: estimate the joint model as outlined here to check statistical significance of variables then estimate emergence and survival separately and work with those separately estimated models to calculate predicted probabilities and ROC curves So, what’s the setup? Again, we are incorporating the lagged value \\(y_{t-1}\\), to encapsulate all the history prior to period \\(t\\). There is just one little trick I need to introduce before we can start on the model: I will replace \\(y_{t-1}\\) by an indicator variable \\(I_{D}\\) in Equation (1) which assumes the value 1 if a country was a democracy in the previous year, and zero otherwise (notation adapted from Epstein et al., 2006, p. 553). Following on from this, we can write the model as follows9: \\[\\begin{equation} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t} + \\epsilon_{i,t}) \\tag{1} \\end{equation}\\] where \\(P(D_{it})\\) is the probability that a country \\(i\\) was a democracy in year \\(t\\), \\(\\Phi\\) is the cumulative normal distribution (the s-shaped distribution which you know from the introduction of probit in week 3), \\(I_{D}\\) the aforementioned indicator variable, \\(X_{i,t}\\) is an independent variable for country \\(i\\) in year \\(t\\), and \\(\\epsilon_{i,t}\\) is a zero mean stochastic disturbance (I only include this for completeness’ sake here. As this is irrelevant for us here and now, I will drop this from the following discussion, not to confuse you unnecessarily). This model is a multiplicative interaction model which allow us to model the probability of a country to be a democracy, conditional on its regime type in the previous year. The indicator variable \\(I_{D}\\), equal to \\(y_{t-1}\\) captures this information. How does this work? Let us start by assuming that in year \\(t-1\\) country \\(i\\) was an autocracy. In this case, the indicator variable \\(I_{D}\\) would be equal to zero, and therefore Equation (1) can be re-written as \\[\\begin{equation} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t}) \\end{equation}\\] In this case, the coefficient \\(\\beta_{1}\\) would only represent the impact of variable \\(X_{i,t}\\) on the probability of an autocracy transitioning to democracy. Expressed more formally, we are dealing with conditional probabilities here, in the case of \\(\\beta_{1}\\), we would obtain the impact of variable \\(X_{i,t}\\) on a country to be a democracy in year \\(t\\), under the condition that it was an autocracy in year \\(t-1\\). It is conditional on this, because we set the indicator variable to zero before. We can construct a similar scenario for the condition, that a country was a democracy in the previous year. In this case, the indicator variable \\(I_{D}\\) would be equal to \\(1\\), and we would obtain equation (1) again: \\[\\begin{equation*} P(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t}) \\end{equation*}\\] With \\(I_{D}=1\\), this can be simplified to: \\[\\begin{equation} P(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) X_{i,t}) \\end{equation}\\] This equation illustrates very well, that the impact of variable \\(X_{i,t}\\) on the probability of democracy to remain a democracy is now made up out of the sum of the two coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\), whereas the constant is the sum of coefficients \\(\\beta_{0}\\) and \\(\\beta_{2}\\). Example To make this more tangible, let’s look at an example. Assume we want to look at the effect of per capita GDP on democratic emergence (i.e. the transition of an autocracy to democracy), and democratic survival (i.e. the transition from democracy to democracy). In this case we would specify the model as follows: \\[\\begin{equation} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t}) \\tag{2} \\end{equation}\\] The coefficient indicating the impact of per capita GDP on democratic emergence is \\(\\beta_{1}\\). As illustrated above, the indicator variable \\(I_{D}\\) is zero in this case, and the equation would be reduced to \\[\\begin{equation} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t}) \\end{equation}\\] For democratic survival, the coefficient indicating the impact of per capita GDP would be the sum of coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\) \\[\\begin{equation} P(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) \\text{per capita GDP}_{i,t}) \\end{equation}\\] Application in R How does all of this look in R, and how do you apply this to a real-world scenario? Let’s do this using the above example of per capita GDP in the global data set. You have already created the indicator variable \\(I_{D}\\) in the form of the variable l.democracy. What we need next is a variable that captures \\(I_{D} \\text{per capita GDP}_{i,t}\\), so that we can calculate \\(\\beta_{3}\\). To create this variable, type world$gdppc_l.democracy &lt;- world$l.democracy * world$gdppc Now, we are ready to estimate the model. Remember, formally, this is written as: \\[\\begin{equation*} P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t}) \\end{equation*}\\] We replace this in the R command with the equivalent variables: joint &lt;- glm(democracy ~ gdppc + l.democracy + gdppc_l.democracy, data = world, na.action = na.exclude, family = binomial(link = &quot;probit&quot;)) If you have done regression analysis before, and you worry about (multi-)collinearity in such a model, then please note that: Analysts should include all constitutive terms when specifying multiplicative interaction models except in very rare circumstances. By constitutive terms, we mean each of the elements that constitute the interaction term. Thus, X and Z are the constitutive terms in [this model: \\(y=\\beta_0 + \\beta_1 X + \\beta_2 Z + \\beta_4 XZ + \\epsilon\\)]. (Brambor et al., 2006, p. 66) You obtain the following output: summary(joint) Call: glm(formula = democracy ~ gdppc + l.democracy + gdppc_l.democracy, family = binomial(link = &quot;probit&quot;), data = world, na.action = na.exclude) Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.968e+00 5.139e-02 -38.290 &lt; 2e-16 *** gdppc -2.954e-05 1.452e-05 -2.034 0.042 * l.democracy 3.587e+00 1.001e-01 35.842 &lt; 2e-16 *** gdppc_l.democracy 2.513e-04 4.863e-05 5.169 2.36e-07 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 10718.5 on 7731 degrees of freedom Residual deviance: 1299.9 on 7728 degrees of freedom (1724 observations deleted due to missingness) AIC: 1307.9 Number of Fisher Scoring iterations: 11 For democratic emergence, we can report the Intercept (Intercept) and the slope coefficient gdppc straight away as -1.968 and -0.00002954, respectively. The slope coefficient is significant, as the p-value is \\(&lt;0.05\\). This is in line with the findings from estimating emergence separately. For democratic survival, we need to take the sum of (Intercept) and l.democracy to obtain the intercept, and gdppc and gdppc_l.democracy to obtain the slope coefficient. This calculation will yield the same coefficients as the two-stepped analysis. The last step is the assessment of statistical significance. For the emergence model, we can once again interpret the output straight away. For the survival scenario, however, we need to test the hypothesis that for example \\(\\beta_{0}\\) and \\(\\beta_{3}\\) are jointly different from zero. As the survival effect is a joint-venture between these two coefficients, we also need to assess their significance jointly, and not just concentrate on \\(\\beta_{3}\\). This is the mistake Przeworski et al. (2000) have made in their seminal book, and this is what is discussed on the first few pages of the article by Epstein et al. (2006). To do this, we are using a post-estimation command which is testing the aforementioned hypothesis that \\(\\beta_{0}\\) and\\(\\beta_{3}\\) are jointly different from zero, called a Wald-Test. For this we need to install and load a new package, called survey. This is a regression term test, where you first need to state the object within which the results are stored (here: joint), and the two terms you want to test, preceded by a tilde and connected by a plus. Lastly we specify that we want the Wald Test. library(survey) regTermTest(joint, ~gdppc+gdppc_l.democracy, method=&quot;Wald&quot;) Wald test for gdppc gdppc_l.democracy in glm(formula = democracy ~ gdppc + l.democracy + gdppc_l.democracy, family = binomial(link = &quot;probit&quot;), data = world, na.action = na.exclude) F = 13.48875 on 2 and 7728 df: p= 1.4194e-06 With \\(1.4194e-06\\) we can reject the null hypothesis, and conclude that jointly, the slope coefficients are different from zero, and as such that per capita GDP explains democratic survival. Literature Recommendations Brambor et al. (2006): This is for the Joint Estimation of Emergence and Survival This discussion draws on Brambor et al. (2006). Please note that I am deliberately NOT lagging the independent variables (IVs) on this worksheet to keep notation as simple as possible. If you decide to run the interaction model, make sure you lag the IVs as explained above.↩︎ "],["exercises-2.html", "Exercises The Data Set Basics Advanced", " Exercises The Data Set Use the data set called prz.dta which is available in the Downloads Section. This is the data set used in the book “Democracy and Development” (Przeworski et al., 2000). Deposit this in an appropriate working directory and import the data set into a data frame called prz. We will only be looking at a few variables – democ = 1 if democracy, 0 otherwise; gdpw - GDP per worker; g = growth rate; oil = 1 if oil producer, 0 otherwise.10 Basics Run a probit model where democ is the dependent variable and g, gdpw and oil are the independent variables. Put the results in column 1 of Table 1. What is (possibly) wrong with this approach? Interpret the coefficients on one or two of the variables. Run the same probit model as before but now include a lagged dependent variable. To create the lagged dependent variable, call: prz &lt;- prz %&gt;% group_by(country) %&gt;% mutate(l.democ = lag(democ)) %&gt;% ungroup() Put the results in column two of Table 1. What are we assuming by including a lagged dependent variable? Do you think that this is appropriate here? Now estimate a probit “transition to democracy” model i.e. how do growth, wealth and oil affect the probability that a country is a democracy this year given that it was a dictatorship last year. We are also lagging the independent variables by one year. Put the results in column 3 of Table 1. Interpret the sign of the coefficients on each independent variable. Now estimate a probit “survival of democracy” model i.e. how do (lagged) growth, wealth and oil affect the probability that a country is a democracy this year given that it was a democracy last year. Put the results in column 4 of Table 1. Interpret the sign of the coefficients on each independent variable. Advanced This section draws on the instructions for joint estimation. Now interact all the lagged independent variables with the lagged dependent variable. Estimate a fully interactive model and include all the constitutive terms. Put the results in column 5 of Table 1. What is the relationship between these coefficients and those in the previous two columns? Is there any extra information provided by this full interaction model that was not available from the previous two models? Now consider the straight probit model, the probit model with the lagged dependent variable, and the full interaction model. Produce the ROC curve for each of these models. Interpret a point on one of these curves. What do the ROC curves tell you about the fit of these three models? You can find the solutions to these exercises in the respective RScript in the Downloads Section. But you can also download the results table, and the stargazer code to produce this table. I have taken these exercises from some material written by Matt Golder from whom I learned all this many moons ago.↩︎ "],["homework-5.html", "Homework", " Homework "],["glossary-5.html", "Glossary", " Glossary Table 9: Glossary Week 7 Term Description dichotomous Can only assume two mutually exclusive, but internally homogeneous qualitative categories "],["flashcards-5.html", "Flashcards", " Flashcards R Functions This Week The data are available as a .csv file. "],["downloads.html", "Downloads Documents Data Sets (in alphabetical order) R Scripts", " Downloads Documents PO33Q Bibliography Codebook for all Data Sets Week 7 Stargazer Table Sample Essay Sample Essay, Annotated Data Sets (in alphabetical order) Africa Europe Eastern Europe Example Latin America Middle East Polity V Przeworski et al. (2000) South East Asia Sub-Saharan Africa World World Value Survey (WVS) R Scripts Week 1 Week 3 Week 3 Predicted Probability Graph Week 3 Sample Stargazer Code Week 5 Solutions Week 7 Solutions Week 7 Stargazer Table "],["glossary-6.html", "Glossary", " Glossary Unless otherwise noted, the definitions are taken from Reiche (forthcoming). Table 10: Glossary for PO33Q Term Description analysis A detailed evaluation of data to discover their structure and relevant information to answer a research question attribute A component or characteristic of a concept autocorrelation The value of one error term does not allow us to predict the value of another error term. As such their covariances must be zero background concept The broad constellation of meanings and understandings associated with the concept (Adcock &amp; Collier, 2001, p. 531) categorical Describing the qualitative categories of a characteristic, for example different religions causal In order to establish a causal relationship, the following criteria must be met concurrently: Relevance of the variables within the broader theoretical and empirical context of the research Clear Theoretical Framework Clear conceptualization Exclusion of alternative explanations Asymmetry Significant (and sufficiently strong) statistical association concept Abstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150) conceptualization Formulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock &amp; Collier, 2001, p. 531) confidence interval A confidence interval constructs an interval of numbers which will contain the true parameter of the population (e.g. the mean) in \\((1-\\alpha)\\) times of cases. \\(\\alpha\\) is usually chosen to be small, so that our confidence interval has a probability of 95% or 99%. confidence level The confidence level is the probability with which the confidence interval is believed to contain the true parameter of the population and is defined as \\((1-\\alpha)\\) constant A variable which does not vary continuous Can assume any value within defined measurement boundaries cross-sectional data Look at different units (or cross-sections) \\(i\\) at a single point in time data Derives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis data set A collection of numerical values for individual observations, separated into distinctive variables degrees of freedom Degrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary dependent variable Is dependent through some statistical or stochastic process on the value of an independent variable descriptive statistics Summarise information about the centre and variability of a variable deviation The deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\) dichotomous Can only assume two mutually exclusive, but internally homogeneous qualitative categories discrete The result of a counting process distribution Refers to the display of the values a variable can assume, together with their respective absolute or relative frequency generalisability The ability to apply the findings made on the basis of a representative sample to the population hypothesis In statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference. independent variable Influences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable” interpretation The explanation of results to answer the research question literature review An analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question mean Is equal to the sum of the observations divided by the number of observations measurement Refers to the selection of a measure or variable median Separates the lower half from the upper half of observations method A tool for systematic investigation mode Is the most frequently occurring value normal distribution The normal distribution is a bell-shaped probability distribution that is symmetrical around the mean. Approximately 68% of values fall within 1 standard deviation of the mean, 96% within 2 standard deviations, and 99.7% within 3 standard deviations. This is known as the empirical rule. p-value The p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\). parameter A parameter is the value a statistic would assume in the long run. It is also called the Expected Value population Collection of all cases which possess certain pre-defined characteristics population distribution The probability distribution of the population probability Refers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring QM The process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question quartile Divides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below reliability Refers to the extent to which repeated measurement produces the same results research question A specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle sample A sub-group of the population sample distribution The probability distribution of a sample secondary data Secondary data are data which have been collected by somebody else significance level The significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9. significance test A significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true Social Sciences Are concerned with the study of society and seek to scientifically describe and explain the behaviour of actors standard deviation The standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\] standard error The standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\] systematized concept A specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock &amp; Collier, 2001, p. 531) t-distribution The t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process. test statistic A test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors. theory A formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.) validity The extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong” variable An element of a conceptual component which varies. We also call these “measures” variance Is equal to the squared standard deviation adjusted rsquared The coefficient of determination for multiple regression. coefficient A coefficient is a numerical expression which is multiplied with the value of a variable coefficient of determination Indicates the proportion of the variation in the dependent variable which is explained through the independent variable. It is defined as \\(\\frac{\\text{Explained Sum of Squares}}{\\text{Total Sum of Squares}}\\) conditional expectation function see Population Regression Function (PRF) confidence interval A confidence interval constructs an interval of numbers which will contain the true parameter of the population (e.g. the mean) in \\((1-\\alpha)\\) times of cases. \\(\\alpha\\) is usually chosen to be small, so that our confidence interval has a probability of 95% or 99% confidence level The confidence level is the probability with which the confidence interval is believed to contain the true parameter of the population and is defined as \\((1-\\alpha)\\) degrees of freedom Degrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary dummy variable Dummy variables are dichotomous, categorical variables which indicate the presence or the absence of a characteristic. error term The error term quantifies the distance between each observation and the corresponding point on the regression line. The terms are denoted as \\(\\epsilon_{i}\\) intercept The intercept is the point at which the regression line intersects the y-axis. In this book we denote it as \\(\\beta_{0}\\) logarithm Logarithm is defined as the exponent or power to which a base must be raised to yield a given number. Expressed mathematically, \\(x\\) is the logarithm of \\(n\\) to the base \\(b\\) if \\(b^x = n\\), in which case \\(x = log_b n\\) (Murray, 2023) model building Running a number of regression models, each testing a different combination of variables. Ordinary Least Squares The method of fitting a regression line by means of minimizing the sum of the squared distances between the observations and the estimated values parsimony Refers to the principle . partial slope coefficient A partial slope coefficient measures the influence of a variable in multiple regression, holding all other independent variables in the model constant Population Regression Function The Population Regression Function (PRF) describes the expected distribution of \\(y\\), given the values of the independent variable(s) \\(x\\). It is also called the conditional expectation function (CEF) and can be denoted as \\(E(y_{i}|x_{i})\\) reference category The category of a dummy variable in respect to which the effect on the value of the dependent variable is displayed regression Regression analysis determines the direction and magnitude of influence of one or more independent variables on a dependent variable regression line The regression line describes how the dependent variable is functionally related to the values of the independent variable. It it defined by the intercept \\(\\beta_{0}\\) and the slope \\(\\beta_{1}\\) residual An estimation of the error term. The difference between an observation \\(y_{i}\\) and the estimated value \\(\\hat{y}_{i}\\). Denoted as \\(\\hat{\\epsilon}_{i}\\) Sample Regression Function A regression line based on a randomly drawn sample significance level Is denoted as \\(\\alpha\\) and defined as \\(1-\\)confidence level. slope A slope is defined as rise over run, and so it tells us how many units of y we need to climb (or descend if the slope is negative) for every additional unit of the independent variable \\(x\\) time-series data Time-series data review a certain characteristic over time \\(t\\), where \\(t\\) runs from 1 to \\(T\\) latent variable Variables that can only be inferred indirectly through a mathematical model from other observable variables that can be directly observed or measured likelihood Likelihood is a measure of the extent to which a sample provides support for particular values of a parameter in a parametric model link function Converts the linear part of a generalized linear model into a probability generalized linear model a family of models which allows us to estimate a linear relationship between dependent and independent variables even though the underlying relationship is not linear by means of a link function. It also allows us to vary the assumed distribution of our error terms predicted probabilities The value of a predicted probability is equal to the cumulative density function, \\(F(X^\\prime\\beta)\\), evaluated at a chosen value of the independent variable probability Refers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring probability distribution Specifies the likelihood of all possible outcomes for a particular variable "],["list-of-references.html", "List of References", " List of References Adcock, R., &amp; Collier, D. (2001). Measurement Validity: A Shared Standard for Qualitative and Quantitative Research. American Political Science Review, 95(3), 529–546. Boix, C., Miller, M., &amp; Rosato, S. (2018). Boix-Miller-Rosato Dichotomous Coding of Democracy, 1800-2015 (Version V3). Harvard Dataverse. https://doi.org/10.7910/DVN/FJLMKT Boix, C., &amp; Stokes, S. (2003). Endogenous Democratization. World Politics, 55, 517–549. Brambor, T., Clark, W. R., &amp; Golder, M. (2006). Understanding Interaction Models: Improving Empirical Analysis. Political Analysis, 14, 62–83. Clark, T., Foster, L., &amp; Bryman, A. (2021). Bryman’s Social Research Methods (Sixth). Oxford: Oxford University Press. Diamond, L. (1992). Economic Development and Democracy Reconsidered. American Behavioral Scientist, 35(4/5), 450–499. Epstein, D. L., Bates, R., Goldstone, J., Kristensen, I., &amp; O’Halloran, S. (2006). Democratic Transitions. American Journal of Political Science, 50(3), 551–569. Fogarty, B. J. (2023). Quantitative Social Science Data With R (Second). Thousand Oaks, CA: Sage. King, G. (1995). Replication, replication. PS: Political Science and Politics, 28(3), 541–559. Lipset, S. M. (1959). Some Social Requisites of Democracy: Economic Development and Political Legitimacy. The American Political Science Review, 53(1), 69–105. Long, J. D., &amp; Teetor, P. (2019). R cookbook: proven recipes for data analysis, statistics, and graphics. O’Reilly Media. Long, J. S. (1997). Regression Models for Categorial and Limited Dependent Variables. Thousand Oaks: Sage. Marshall, M. G., &amp; Gurr, T. R. (2020). Polity V Project: Political Regime Characteristics and Transitions, 1800-2018. available online at https://www.systemicpeace.org/inscr/p5manualv2018.pdf. Murray, F. J. (2023). logarithm. In Encyclopedia Britannica. available online at https://www.britannica.com/science/logarithm. Oxford Learner’s Dictionaries. (n.d.). available online at https://www.oxfordlearnersdictionaries.com/. Przeworski, A., Alvarez, M. E., Cheibub, J. A., &amp; Limongi, F. (2000). Democracy and Development - Political Institutions and Well-Being in the World, 1950-1990. Cambridge: Cambridge University Press. Reiche, F. (forthcoming). Introduction to Quantitative Methods in the Social Sciences. Oxford: Oxford University Press. Stinerock, R. (2022). Statistics with R – A Beginner’s Guide (Second). Thousand Oaks, CA: Sage. World Bank. (2024). World Development Indicators, 21.02.2024. available online at https://datacatalog.worldbank.org/dataset/world-development-indicators. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
