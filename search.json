[
  {
    "objectID": "01-PO33Q.html",
    "href": "01-PO33Q.html",
    "title": "Week 1",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#learning-outcomes",
    "href": "01-PO33Q.html#learning-outcomes",
    "title": "Week 1",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nExplain the role of comparative politics within political science\nApply the comparative method\nDescribe the stages of a research project\nPerform introductory data management and variable transformation in R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#how-data-are-organised",
    "href": "01-PO33Q.html#how-data-are-organised",
    "title": "Week 1",
    "section": "How Data are organised",
    "text": "How Data are organised\nI have put a little sample data set together for you which you can see here:\n\n\n\n\n\n\nFigure 1: Sample Data Set\n\n\n\nData such as these are known as secondary data, as they have been collected by somebody else. In this case by the World Bank. The data presented here are also known as cross-sectional data, as they look at different units (here countries) at a single point in time. I will introduce you to time-series data in Week 7.\nEach column represents a variable, the first one country names, and the second per capita GDP in 2015. Each row represents an observation, in our case an individual country. The meeting point between the variable and the observation is a particular value. So for example, in Figure 1 the GDP (second column) of the third country (row) is $ 5792 (cell). You see in the first row the names of the variables. Always make these short and sweet, but especially telling. Don’t go for something like “x4_st”, or “fubar”, as nobody (including yourself after a little while) will have any clue what this is.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#r-rstudio-installation",
    "href": "01-PO33Q.html#r-rstudio-installation",
    "title": "Week 1",
    "section": "R & RStudio – Installation",
    "text": "R & RStudio – Installation\nNow we are ready to start working with R. The first step is to install the program. Please follow these instructions:\n\nGo to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program.\nGo to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program.\nNow open RStudio.\n\n\nWhilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#r---getting-started",
    "href": "01-PO33Q.html#r---getting-started",
    "title": "Week 1",
    "section": "R - Getting Started",
    "text": "R - Getting Started\nIn this companion I am using two different fonts:\n\nFont for plain text\nA typewriter font for R functions, values, etc.\n\nLet’s have a look at RStudio itself. When you open the programme, you are presented with the following screen:\n\n\n\n\n\n\nFigure 2: RStudio\n\n\n\nIt has – for now – three components to it. On the left hand-side you see the so-called “Console” into which you can enter the commands, and in which also most of the results will be displayed. On the right hand side, you see the “Workspace” which consists of an upper and a lower window. The upper window has three tabs in it. The tab “Environment” will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the “History” tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the “Connections” tab you can connect to online sources. We will not use this tab.\nIn the lower window, you have five tabs. Under “Files” you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The “Plots” tab will display the graphs we will be producing. “Packages” form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a “Help” function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab “Viewer”.\n\nIntroduction to R Studio",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#rscript",
    "href": "01-PO33Q.html#rscript",
    "title": "Week 1",
    "section": "RScript",
    "text": "RScript\nIf you read the previous section carefully, you will have noticed that I wrote that you can enter the commands in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs.\nOne of the foundations of scientific research is “reproducibility” or “replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” (King, 1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up where they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check.\nThe creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of space. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due.\nTo create an RScript, click File → New File → RScript. A fourth window opens, and your screen will now look something like this:\n\n\n\n\n\n\nFigure 3: The RScript Window\n\n\n\nYou can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneously on a Mac and “Ctrl” / “Enter” on Windows.\nIf you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section.\nFigure 4 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools → Global Options → Appearance → Twilight.\n\n\n\n\n\n\nFigure 4: Example of an RScript\n\n\n\n\nMore Themes\nIf you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes1 to choose from:\n\ninstall.packages(\n  \"rsthemes\",\n  repos = c(gadenbuie = 'https://gadenbuie.r-universe.dev', getOption(\"repos\"))\n)\n\n\n\nRScript Structure\nWell, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it.\nFirst of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article.\nI stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course).\n\nSince you know how to write in an RScript now, avoid using the “Console” to write your code. Only write in the RScript.\n\n\nRScript Structure",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#first-steps-in-r",
    "href": "01-PO33Q.html#first-steps-in-r",
    "title": "Week 1",
    "section": "First Steps in R",
    "text": "First Steps in R\nBut enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type (in the RScript, not the Console):\n\n5+3\n\nand press “command” / “enter” (or “Ctrl” / “enter” if you are on Windows). In everything that is to follow, commands will be shown in their own individual boxes. These have a clipboard button on the right, so you can copy and paste the code into your own RScript. The output is presented in a separate box directly underneath. There is no clipboard button, as R will render this result in your own console when you run the code. So, including the result, the calculation would look like this:\n\n5+3\n\n[1] 8\n\n\nwhere the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. Note that the result of this operation is displayed in the “Console”, even if you write this in the RScript above.\n\nYou can copy the code from this page by clicking the clipboard in the top-right hand corner. You can then paste it into your RScript.\n\nA fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call2\n\nresult &lt;- 5+3\n\nIf we now call the object, R will return its value, 8.\n\nresult\n\n[1] 8",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#the-working-directory",
    "href": "01-PO33Q.html#the-working-directory",
    "title": "Week 1",
    "section": "The Working Directory",
    "text": "The Working Directory\nIt is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take.\nIn those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 5.\n\n\n\n\n\n\nFigure 5: Folder Structure\n\n\n\nYou see that there is a sub-folder for each week of the module, and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO33Q.\nR works with so-called “Working Directories”. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO33QQ_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless.\n\nPlease set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it.\n\nNow we need to tell R to use this folder. If you know the file structure of your computer you can simply use the setwd() command, and enter the path. Here is an example from my computer:\n\nsetwd(\"~/Warwick/Modules/PO33Q/Week 1\")\n\nIf you don’t know the file structure of your computer, then you can click Session → Set Working Directory → Choose Directory.\n\nWorking Directory",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#r-packages",
    "href": "01-PO33Q.html#r-packages",
    "title": "Week 1",
    "section": "R Packages",
    "text": "R Packages\nIt would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets:\n\ninstall.packages(\"readxl\")\n\nWe can then load this package into our library with the library() command.\n\nlibrary(readxl)\n\n\nOnce you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#working-with-your-data-set",
    "href": "01-PO33Q.html#working-with-your-data-set",
    "title": "Week 1",
    "section": "Working with Your Data Set",
    "text": "Working with Your Data Set\n\nOpening\nWe are now ready to open the data set in R - where it is called a “data frame”. First, download the Example data set (also available in the Downloads Section) and place it into the current working directory. To load it into R, we create a new object example, and ask R to read “Sheet 1 of the Excel file”example.xlsx”.\n\nexample &lt;- read_excel(\"Week 1/example.xlsx\", sheet=\"Sheet1\")\n\n\nLoading the Data Set\n\n\n\n\nPlease do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need.\n\nWe can now use our data in R!\n\n\nViewing the Data\nIn the present case, you know what the data look like, but very often when you use secondary data sets, you don’t. So it’s a good idea to view the data frame before doing anything with it. To view the data frame in a way you might be familiar with from Excel (even though you cannot edit this in the same way). apply the View() command.\n\nView(example)\n\nIf you only want to see the first 6 observations of each variable, use the head() command:\n\nhead(example)\n\n# A tibble: 6 × 2\n  country   gdp\n  &lt;chr&gt;   &lt;dbl&gt;\n1 China   13571\n2 Germany 44187\n3 India    5792\n4 UK      38865\n5 US      52704\n6 Zambia   3602\n\n\nIf you simply want to know the variable names in the data frame, type:\n\nnames(example)\n\n[1] \"country\" \"gdp\"    \n\n\nThe next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure:\n\nstr(example)\n\ntibble [6 × 2] (S3: tbl_df/tbl/data.frame)\n $ country: chr [1:6] \"China\" \"Germany\" \"India\" \"UK\" ...\n $ gdp    : num [1:6] 13571 44187 5792 38865 52704 ...\n\n\n\n\nVariable Types in R\nYou have seen in the output of the str() command that R distinguishes between a number of different variable types. Here is a broad overview of the variable types, so that you know which descriptive statistics you can calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types:\n\nnumeric – numbers, continuous\ncharacter (also called string) – letters\n\nWithin numeric we can distinguish between the following:\n\nfactor - nominal variable, categorical\nordered factor - ordinal variable, categorical\ninteger - numeric, but only “whole” numbers (discrete)\nnumeric - any number (interval or ratio scales)\n\n\nIf you are unfamiliar with measurement scales, then please look these up before proceeding.\n\n\n\nDescriptive Statistics\nQuite a large number of descriptive statistics can be calculated. For example:\n\nMean\nMedian\nMode\nRange\nStandard Deviation\nVariance\n\n\nCalculating a Mean\n\n\n\n\nStandard Deviation\n\n\n\n\nIf any of the other terms don’t mean anything to you beyond having heard the term before, please look these up.\n\nIf you want to see, how these two descriptive statistics affect the shape of a variable’s distribution, please use the app below (it is a web-based application and might take a moment to load).\n\n\n\n\n\n\n\nThey are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. If we want to refer to a particular column in R (which is equivalent to a variable), then we need to specify the data frame within which the variable is located, followed by a $ sign and then the variable name. Schematically, this would look be written as dataframe$variable.\nWith this information to hand, we can calculate the mean of the variable gdp:\n\nmean(example$gdp)\n\n[1] 26453.5\n\n\nThen the median:\n\nmedian(example$gdp)\n\n[1] 26218\n\n\nMean, median, and also mode (the most frequently occurring value) are all measures of centrality, but centrality alone does not adequately describe a distribution. You can think of two scenarios, in both we have two people in a group and we are trying to describe their age. In group one we have one person who is 50 years old, and one who is 52 years old. Average age = 51. In the second group we have a toddler aged 2, and a very old person aged 100. Same average, but a very different distribution of age. Schematically you can see this in Figure 6 where two distributions have the same mean, median and mode, but their spread is quite different.\n\n\n\n\n\n\nFigure 6: Distributions with different Standard Deviations\n\n\n\nTherefore, we also need to look at the variability of a variable to adequately describe it. Again, there are quite a few measures available. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you the range straight away:\n\nmin(example$gdp)\n\n[1] 3602\n\nmax(example$gdp)\n\n[1] 52704\n\nrange(example$gdp)\n\n[1]  3602 52704\n\n\nThe standard deviation is rather long-winded to calculate by hand, but the R command is short and sweet:\n\nsd(example$gdp)\n\n[1] 21319.75\n\n\nThe variance is the squared standard deviation, but you can calculate it with its own command in R, too:\n\nvar(example$gdp)\n\n[1] 454531709\n\n\nYou can get information on the quartiles (these are also measures of spread), the mean, as well as the minimum and maximum of a variable through one, simple command:\n\nsummary(example$gdp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3602    7737   26218   26454   42856   52704",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#data-manipulation",
    "href": "01-PO33Q.html#data-manipulation",
    "title": "Week 1",
    "section": "Data Manipulation",
    "text": "Data Manipulation\n\nRecoding\nWhen conducting quantitative research, variables will rarely come in the format in which you require them to be. I have been kind and reshaped all data you will be using for this module already. Nonetheless, you might come into a position in which you need to recode a variable, and here is how to do it.\nThe process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). Let’s go all out with the tidyverse:\n\nlibrary(tidyverse)\n\nNow we can recode. Let’s say we want to create a new variable with two categories: low income and high income, where the cut-off sits at $ 20,000. The command to do this takes a little explaining. We start by stating the dataframe we wish to work with, example. The symbol which follows, %&gt;%, reads as “and then”, and is called a “pipe”. So we take the data frame example “and then” carry out a function called mutate. This function in turn creates a new variable gdpcat by recoding the variable gdp. It does so by cutting up the original variable into two sections, the first reaching from 0 to 20,000, and the second for values higher than 20,0000. We will call the first section low and the second high. These are the labels of the new categorical variable gdpcat. As there is an order in these categories, I have aded the function order() to turn what would otherwise be a nominal into an ordinal variable. The last step is then to assign this newly created variable gdpcat to our data frame example.\n\nexample %&gt;% \n  mutate(gdpcat=\n           ordered(\n             cut(gdp, breaks=c(0, 20000, Inf), \n                 labels=c(\"low\",\"high\")))) -&gt; example\n\n\nMake a habit of adding a note underneath each of the more complex code chunks in your RScript (preceded with a #) in which you translate the code into plain English.\n\nLet us now check the structure of the new variable to make sure that we have done everything correctly.\n\nstr(example$gdpcat)\n\n Ord.factor w/ 2 levels \"low\"&lt;\"high\": 1 2 1 2 2 1\n\n\nIn my case all looks fine - make sure yours looks the same.\n\nRecoding a Factor Variable\n\n\n\n\n\nSaving\nPlease save the Rscript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the worksheet or the data, as running the RScript on the raw data will bring you precisely to where you left off.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#the-real-data-set",
    "href": "01-PO33Q.html#the-real-data-set",
    "title": "Week 1",
    "section": "The Real Data Set",
    "text": "The Real Data Set\nDownload the world.csv data set from the Downloads Section and place it into the current working directory.\n\nworld &lt;- read.csv(\"Week 1/world.csv\")\n\nThe data are taken from World Bank (n.d.), Miller et al. (2022), and Marshall & Gurr (2020). Table 1 provides a full code book, but you can also download it in pdf format here.\n\n\n\n\n\n\n\n\n\nvariable\nlabel\n\n\n\n\ncountrycode\nCountry Code\n\n\ncountry\nCountry Name\n\n\nyear\nYear\n\n\npolity5\nCombined Polity V Score\n\n\ndemocracy\n0 = Autocracy, 1 = Democracy (Boix et al., 2018)\n\n\ngdppc\nGDP per capita (current US Dollars)\n\n\nenrol_gross\nSchool enrollment, primary (% gross)\n\n\nprimcomp\nPrimary completion rate (% of relevant age group)\n\n\nprimcom_male\nPrimary completion rate, male (% of relevant age group)\n\n\nprimcom_fem\nPrimary completion rate, female (% of relevant age group)\n\n\nenrol_net\nSchool enrollment, primary (% net)\n\n\nagri\nAgriculture, forestry, and fishing, value added (% of GDP)\n\n\nslums\nPopulation living in slums (% of urban population)\n\n\ntelephone\nHouseholds with telephones (Landline & Mobile, %)\n\n\ninternet\nIndividuals using the Internet (% of population)\n\n\ntax\nTax revenue (% of GDP)\n\n\npop\nPopulation, total\n\n\npop_fem\nPopulation, female (% of total population)\n\n\nelectricity\nAccess to electricity (% of total population)\n\n\nservice\nServices, value added (% of GDP)\n\n\noil\nOil rents (% of GDP)\n\n\nnatural\nTotal natural resources rents (% of GDP)\n\n\nliteracy\nLiteracy rate, adult total (% of people ages 15 and above)\n\n\nlit_male\nLiteracy rate, adult male (% of males ages 15 and above)\n\n\nlit_fem\nLiteracy rate, adult female (% of males ages 15 and above)\n\n\ninfant\nMortality rate, infant (per 1,000 live births)\n\n\nhealth_ex\nCurrent health expenditure (% of GDP)\n\n\nhospital\nHospital beds (per 1,000 people)\n\n\ntuberculosis\nIncidence of tuberculosis (per 100,000 people)\n\n\nlife\nLife expectancy at birth, total (years)\n\n\nlife_male\nLife expectancy at birth, male (years)\n\n\nlife_female\nLife expectancy at birth, female (years)\n\n\nunemploy\nUnemployed (%)\n\n\nurban\nUrban population, total (% of total population)\n\n\nattend\nBirths attended by skilled health staff (% of total)\n\n\nprenatal\nPregnant women receiving prenatal care of at least four visits\n\n\nmilitary\nMilitary Expenditure (% of GDP)\n\n\nun_continent_name\nContinent\n\n\nun_region_name\nRegion on continent\n\n\n\n\n\n\nTable 1: WDI Codebook\n\n\n\n\n\nAgain, let’s explore the data set through the view() function:\nNow, don’t panic. This data set is big, but you know the basic structure: country by year in rows, and the variables in the column. The value for the variable in a given country in a given year is in the meeting point between row and column.\nThe data are (yes, the word “data” is plural) organised by country name in the first instance. So the first country you see is Afghanistan. There are multiple rows for Afghanistan, because each row gives you information about the value in a particular year (second column). This is repeated for every country in the world, leading to a total of 10,824 observations, to save you scrolling all the way down. Schematically, the structure of this data set looks as follows:\n\n\n\n\n\n\n\n\n\n\ncountry\nyear\nliteracy\n\n\n\n\nA\n2000\n80\n\n\nA\n2001\n81\n\n\nA\n2002\n85\n\n\nA\n2003\n90\n\n\nB\n2000\n92\n\n\nB\n2001\n93\n\n\nB\n2002\n95\n\n\nB\n2003\n99\n\n\n\n\n\n\nTable 2: Schematic: Time-Series, Cross-Sectional Data\n\n\n\n\n\n\nWhen you look at the data set in R, you will see that not every box contains a value - this means that the data are missing for this particular country-year for that particular variable. This is an issue we will be discussing a later stage in more detail, but I can already say now, that this issue is more pronounced in developing countries than in developed ones, and that it often severely limits the variables you can include in the analysis.\nLet us conclude today with some more descriptive statistics to get used to entering and executing commands. Say, we want to find out the average GDP per capita of the UK since 1960 (which is when this data set begins, it ends in 2020). To do this, we first need to create a data frame which only contains data for GB. We call this process “subsetting”. The filter function comes out the dplyr package, and takes a particular data frame and filters all those observations which meet the condition specified. We will use this a lot on this module, and so it makes sense to remember this one well:\n\nGB &lt;- filter(world, countrycode==\"GBR\")\n\nWe can now produce\n\nsummary(GB$gdppc)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1398    4138   18389   20514   39599   50398 \n\n\nand all other descriptive statistics outlined before for “Great Britain”. If you want the number of observations, then you can display them by calling:\n\nlength(GB$gdppc)\n\n[1] 61\n\n\nIn this particular case, it would tell us that we have data for 61 years.\nIf this all seems a little much at the moment, don’t worry. As we go through the module, R will become much, much easier to handle!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#book-recommendations",
    "href": "01-PO33Q.html#book-recommendations",
    "title": "Week 1",
    "section": "Book Recommendations",
    "text": "Book Recommendations\nIf you want some more material to read up on R, then these are my recommendations:\n\nFogarty (2023): The best applied R book on the market until my own book comes out.\nStinerock (2022): Popular with students for good reasons!\nLong & Teetor (2019): Some recipes to cook with R\n\nPlease consult the List of References for full details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO33Q.html#footnotes",
    "href": "01-PO33Q.html#footnotes",
    "title": "Week 1",
    "section": "",
    "text": "Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎\nTo “call” means to execute a command.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html",
    "href": "02-PO33Q.html",
    "title": "Week 2",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#learning-outcomes",
    "href": "02-PO33Q.html#learning-outcomes",
    "title": "Week 2",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nExplain the role of theories in empirical political science research\nDescribe and critique the propositions of modernisation theory\nDiscuss the principles of statistical inference\nUnderstand and apply linear regression (Ordinary Least Squares)\nAppraise model fit in linear regression models",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#probability-distributions",
    "href": "02-PO33Q.html#probability-distributions",
    "title": "Week 2",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nThe term probability distribution sounds fancy, but really only implies that we are assessing the relative frequency with which a particular value occurs. So, for example, if I have six History students on the module, and four PAIS students, then the probability of being a PAIS student is 40%. For categorical data, such as this, we can assign a probability to each category. But for continuous variables, such as the Polity V score, or per capita GDP this no longer works. Instead, we assign probabilities to an interval of numbers. In the following graph, I am putting the values of such a continuous variable on the x-axis, and then draw a curve over this which indicates the probability distribution for these values.\n\n\n\n\n\n\nFigure 1: Probability Distribution for a Continuous Variable\n\n\n\nIn this graph, the values on the x-axis between alpha and beta have a probability of occurring that is equal to the orange area under the curve. This curve is bell-shaped, and symmetrical around the mean of the variable x, and is known as the normal distribution. The beauty of this distribution is that the amount of probability that is contained in an interval around the mean depends solely on the number of standard deviations, denoted as \\(\\sigma\\), of the variable x. So, for example, if I travel one standard deviation to the left and to the right from the mean, I will always have 68% of the area under the distribution in this interval.\n\n\n\n\n\n\nFigure 2: Probability within one Standard Deviation\n\n\n\nFor two standard deviations, this is equivalent to 96% and for three standard deviations 99.9%. Schematically this looks as follows:\n\n\n\n\n\n\nFigure 3: Probabilities under the Normal Distribution\n\n\n\nThe total area under the probability distribution is equal to 100%. So, if the orange area in Figure 2 is equal to 68%, then the remaining area must be 32%. As this area is split equally into a left and a right side, the area on the right must be 16%. This is what is known as a right-tail probability. Here it is the probability beyond 1 standard deviation, or \\(1 \\sigma\\).\nThis means that if we know how many standard deviations away from the mean of the distribution a value falls, we can make a statement on how likely it would be to observe a value that is higher than this value. For example, if I know the distribution of marks on a module, this would allow me to state how likely it was to achieve a First, or a mark of higher than 70. Assume that the average of an assessment was 63, and the standard deviation 9. Then the difference between the mean and the value we are interested in is equal to 70-63=7. To express this distance in units of standard deviations, we now divide 7 by 9, and obtain something that is called a z-score:\n\\[\\begin{equation}\nz=\\frac{\\text{Observation} - \\text{Mean}}{\\text{Standard Deviation}}\n\\end{equation}\\]\nIn our case, z would be equal to 7/9, or 0.7777778. You can look this value up in a z-table which lists the right-tail probability for all conceivable values of z. For z=0.78 it returns a probability of 21.77%. With a mean of 63 and standard deviation of 9,the probability of scoring a First would be 21.77%.\nThis all works fine, so long as we are dealing with the population. Sadly, we rarely have access to the population in the social sciences, however. Surveys, for example, are usually conducted for a small representative sub-group of the population, a sample. So, how do we make statements about the population if we only have access to a sample? The answer is: with a sampling distribution.\n\nSampling Distributions\nWhenever we draw a sample from a population, the values we draw will vary. Therefore, also the mean of these values will vary each time. Imagine I draw a sample of 5 students from a seminar group, time and time again, and each time I calculate the average age of these 5 students. Sometimes the average will be high, sometimes it will be low, but – and here comes the magic – the value that will come up most often in these samples is the true mean of the population (all students in the seminar). We call this population value a parameter.\nIf we arrange all of the sample means we have obtained from sampling again and again, these means will form a distribution in their own right, the sampling distribution. This has the true population mean (let’s denote this as \\(\\mu\\)) in its centre. In Figure 4, for example, I have simulated a population with a mean of zero, and have drawn 9000 samples from this population, each with 30 observations. As you can see, the distribution of means of each of these 9000 samples forms a normal distribution which has zero, the population mean, at its centre.\n\n\n\n\n\n\nFigure 4: Sampling Distribution\n\n\n\nSo, we know the mean of the sampling distribution, but what about its standard deviation? We could of course take samples repeatedly from the population and calculate it, but that is wasteful. Instead, we estimate it by dividing the standard deviation of the one sample we usually have by the square root of the sample size. The result is the so-called standard error, denoted as “se”, the standard deviation of the sampling distribution.\n\\[\\begin{equation}\nse=\\frac{s}{\\sqrt{n}}\n\\end{equation}\\]\nwhere s is the sample standard deviation and n is the sample size.\nThe sample standard deviation varies with each sample we draw, however, and this introduces uncertainty into our analysis. The normal distribution cannot deal with this uncertainty, and so we have to abandon it in favour of the t-distribution.\n\n\nThe t-Distribution\nThe t-distribution is also bell-shaped and symmetrical, centered around a fixed mean of zero. But, crucially, its shape is not static, but depends on the number of degrees of freedom. Degrees of freedom are constraints on the estimation process that reflect the number of independent pieces of information available. In our case, the constraint arises from estimating the population standard deviation \\(\\sigma\\) using the sample standard deviation, \\(s\\). For a single sample, the degrees of freedom are defined as n-1, where n is the sample size. Depending on these degrees of freedom, the t-distribution becomes wider or narrower, expressing the uncertainty we are incurring from working with a sample of a particular size. You can see in Figure 5 that with increasing sample size the t-distribution becomes narrower and narrower, until – for an infinite sample size – it is equivalent to the normal distribution again. The smaller the sample size, the wider the t-distribution becomes. This means that as the sample size decreases we need to travel more standard deviations away from the mean in order to obtain a certain probability. More on this below under “Confidence Intervals”.\n\n\n\n\n\n\nFigure 5: Comparison of t-Distributions\n\n\n\nUsing the sampling distribution, we can create a range of values that we think the true population value (like the average) falls into. This range is called a confidence interval. For example, a 95% confidence interval means that if we took many random samples and made a new interval each time, about 95% of those intervals would contain the true value.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#confidence-intervals",
    "href": "02-PO33Q.html#confidence-intervals",
    "title": "Week 2",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nHow do we construct confidence intervals? We start by setting the confidence level — the proportion of confidence intervals, constructed from repeated random samples of the same population using the same method, that are expected to contain the true population parameter. It is denoted by (1 - ), where () is the significance level. For example, a 95% confidence level implies that 95% of such intervals would contain the true parameter in the long run.\nBecause the interval is constructed from a sample, we use the sampling distribution of the sample mean. We place the sample mean at the center of this distribution and treat it as our point estimate of the unknown population mean. To establish an interval around this estimate corresponding to the desired confidence level, we determine how many standard errors we must move to the left and right. Whereas under the normal distribution this was the z-score, under the t-score this number is called the t-score, which can be found using a statistical table.\nThe appropriate t-score depends on the sample size (or the degrees of freedom, to be more precise). As hinted above, for smaller samples the t-distribution is wider, so we need to travel further away from the mean to achieve the same level of confidence than we would with a larger sample and a narrower distribution. For example, to construct a 95% confidence interval, we would use a t-score of approximately 2.306 for a sample of 9, but only 2.045 for a sample of 30.\nAs an example, reconsider our fictitious assessment with an average of 63, and the standard deviation 9. I used this for a probability statement in a population earlier, but now assume this is based on a sample of 15 students from a much larger module. Within which boundaries do we believe the parameter to fall with 95% confidence?\nAs a first step, we calculate the standard error:\n\\[\\begin{equation}\nse=\\frac{s}{\\sqrt{n}}= \\frac{9}{\\sqrt{15}} = 2.32379\n\\end{equation}\\]\nAs we have 15 observations, we have 14 degrees of freedom (n-1) which corresponds to a t-score of 2.145. We can thus state that with 95% confidence the parameter, \\(\\mu\\), falls between 58.02 and 67.98.\n\\[\\begin{align}\n\\bar{y} - t \\cdot se \\le \\;\\; &\\mu \\; \\le \\bar{y} - t \\cdot se \\\\\\\\\n63- 2.145 \\cdot 2.32379 \\le \\;\\; &\\mu \\; \\le 63 + 2.145 \\cdot 2.32379 \\\\\\\\\n58.01547 \\le \\;\\; &\\mu \\; \\le 67.98453\n\\end{align}\\]\nwhere \\(\\bar{y}\\) is the average in the sample1.\nIf you want to explore the dynamics of a confidence interval interactively, feel free to use the app below.\n\n\n\n\n\nWe will come back to confidence intervals in the context of regression analysis. Let us now look at significance tests.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#tests-of-statistical-significance",
    "href": "02-PO33Q.html#tests-of-statistical-significance",
    "title": "Week 2",
    "section": "Tests of Statistical Significance",
    "text": "Tests of Statistical Significance\nAs you know from the lecture last week, in the social sciences we generally test whether we can explain behaviour or some kind of phenomenon through a particular theory. In practical terms this means that we distill the theory into testable statements, called hypotheses. Take this hypothesis, for example: The average level of democracy in the Middle East is statistically different from that of Europe. In this statement, we are no longer interested in establishing an interval of numbers within which we believe the average level of democracy in the Middle East to fall, but we are comparing it with a particular value, or point: the level of democracy in Europe. This kind of assessment is called a significance test.\nEvery significance test has two hypotheses, an alternative, and a null-hypothesis. The alternative hypothesis is the one I already stated:\n \n\nH\\(_\\text{A}\\): The average level of democracy in the Middle East is statistically different from that of Europe.\n\n   \nThe null-hypothesis always represents no effect. In our case the region would have no impact on the democracy level, and the Polity V score of Europe and the Middle East would be identical.\n \n\nH\\(_0\\): The average level of democracy in the Middle East is the same as in Europe.\n\n   \nThe average Polity V score in the Middle East was -2.8 in 2015 with a standard deviation of 7.07 across 15 countries. The average Polity V score for Europe in the same year was 8.7.\n\n\n\nHow to obtain these values\n\n\nme &lt;- filter(world, countrycode==\"BHR\" | #Bahrain\n                    countrycode==\"CYP\" | #Cyprus\n                    countrycode==\"EGY\" | #Egypt\n                    countrycode==\"IRN\" | #Iran\n                    countrycode==\"IRQ\" | #Iraq\n                    countrycode==\"ISR\" | #Israel\n                    countrycode==\"JOR\" | #Jordan\n                    countrycode==\"KWT\" | #Kuwait\n                    countrycode==\"LBN\" | #Lebanon\n                    countrycode==\"OMN\" | #Oman\n                    countrycode==\"QAT\" | #Qatar\n                    countrycode==\"SAU\" | #Saudi Arabia\n                    countrycode==\"ARE\" | #UAE\n                    countrycode==\"YEM\"   #Yemen\n             )\n\nme_15 &lt;- filter(me, year==2015)\nmean(me_15$polity5)\nsd(me_15$polity5)\n\nworld$un_region_name\n\neurope &lt;- filter(world, un_region_name==\"Southern Europe\"|  \n                   un_region_name==\"Western Europe\" | \n                   un_region_name==\"Eastern Europe\" |\n                   un_region_name==\"Northern Europe\")\n\neurope_15 &lt;- filter(europe, year==2015)          \nmean(europe_15$polity5, na.rm=TRUE)\n\n\nt.test(me_15$gdppc, mu=8.7,  \n       data=me_15)\n\n\n\nIn order to ascertain whether the observed value of -2.8 is statistically different from the null-hypothesis value, 8.7, we need to make a probability statement. To be precise, we need to quantify the probability of observing a value that is more extreme than than one we have already observed if the null hypothesis was true. If this probability is small, then the value we have can be regarded as highly unusual and we can conclude that it is statistically different from the null-hypothesis value. This probability of observing a value more extreme than the one we have observed, assuming the null hypothesis is true, is the p-value.\nUsually, we want the p-value to be quite small, either 5% or 1%. R will obtain this for us, but so you know what happens behind the scenes, here is a quick summary. We place the null hypothesis into the centre of a sampling distribution, effectively pretending that this is our population parameter. In other words, we assume that the null hypothesis is true. Now we look at where in relation to this null hypothesis value our observed value falls. We are then able to calculate the distance between the observed value and the null-hypothesis value in units of standard error. This is equivalent to the t-value, our test statistic. We calculate it as follows:\n\\[\\begin{equation}\nt = \\frac{\\bar{y} - \\mu_{0}}{se}, \\quad \\text{where} \\, se = \\frac{s}{\\sqrt{n}}\n\\end{equation}\\]\nIn our example:\n\\[\\begin{equation}\nt = \\frac{-2.8 - 8.7}{1.826243} = -6.297081, \\quad \\text{where} \\, se = \\frac{7.07301}{\\sqrt{15}}\n\\end{equation}\\]\nThis places us quite far out into the tails of the t-distribution (see Figure 6), and as such the probability of observing a value more extreme is very small. Hence, we can regard our observed value of -2.8 as unusual and conclude the the average democracy level in the Middle East is statistically different from that of Europe. The precise p-value for this is 0.001283, well below our desired significance level of 5%.\n\n\n\n\n\n\nFigure 6: The p-Value\n\n\n\nI have displayed this here as a two-sided test. We did not specify whether we expect the level of democracy in the Middle East to be higher or lower than in Europe, just different. As both directions are therefore possible, the p-value is made up of the sum of the areas in the left and the right tails of the distribution. One-sided tests are possible, but generally in regression analysis the test is two-sided which is why I will leave it at that here.\n\nInterpreting Significance Tests\nIn the assessment, and when reporting statistical results more generally, you’ll be required to interpret the outcome of a significance test. To do this well, it’s important to use language that reflects what null hypothesis significance testing (NHST) actually tells us, without overstating what the test can conclude.\nNHST works by starting with the null hypothesis - usually a claim like “there is no effect” or “there is no difference”. We use the data to test whether that assumption is plausible. If the p-value is small, it means that results as extreme as the one we observed — or more extreme — would be rare if the null hypothesis were true. So, we take that as evidence against the null. But if the p-value is large, then the data are consistent with the null. And that’s where your interpretation should stop (at least at the introductory level, to avoid stretching and overstating).\nThere are two possible outcomes of a significance test: (1) your p-value is below the required significance level, and (2) your p-value is above the required significance level. Let’s look at these in turn:\n\nIf your p-value is below the required significance level, you can say things like:\n\n“There is evidence of an effect.”\n“We reject the null hypothesis.”\n“The data provide evidence against the null hypothesis.”\n“There is statistically significant evidence of a difference (or relationship, or effect).”\n\n\nAvoid phrasing that implies certainty, such as:\n\n🚫 “We proved the alternative hypothesis.”\n🚫 “We proved that there is an effect.”\n🚫 “We accept the alternative hypothesis.”\n\n\n\n\nIf your p-value is above the required significance level to reject the null hypothesis, you can say things like:\n\n“There is insufficient evidence for an effect (of a difference, for a relationship).”\n“We cannot reject the null hypothesis.”\n“We fail to reject the null hypothesis.”\n“The results are consistent with the null hypothesis.”\n\n\nAvoid phrasing that overstates what the test can tell you, such as:\n\n🚫 “We proved the null hypothesis.”\n🚫 “We reject the alternative hypothesis.”\n🚫 “The null hypothesis is true.”\n\n\n\nHaving taught quantitative political analysis for many years, I know that students love to use the word “prove” when they interpret their results. But there is no proof in significance testing, because we deal with probability statements, not certainty.\n\nAnd with that, we are finally ready to turn our attention to the actual topic for today: linear regression.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#introduction",
    "href": "02-PO33Q.html#introduction",
    "title": "Week 2",
    "section": "Introduction",
    "text": "Introduction\nRegression is the power house of the social sciences. It is widely applied and takes many different forms. In this section we are going to explore the linear variant, estimated through a method called Ordinary Least Squares (OLS). This type of regression is used if our dependent variable is continuous. In Week 3 we will have a look at regression with a binary dependent variable and the calculation of the probability to fall into either of those two categories. But let’s first turn to linear regression.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#what-is-it",
    "href": "02-PO33Q.html#what-is-it",
    "title": "Week 2",
    "section": "What is it?",
    "text": "What is it?\nRegression is not only able to identify the direction of a relationship between an independent and a dependent variable, it is also able to quantify the size of the effect. Let us choose y as our dependent variable, and x as our independent variable. We have some data which we are displaying in a scatter plot:\n\n\n\n\n\n\nFigure 7: Scatter Plot\n\n\n\nWith a little goodwill we can already see that there is a positive relationship: as x increases, y increases, as well. Now, imagine taking a ruler and trying to fit in a line that best describes the relationship depicted by these points. This will be our regression line.\nThe position of a line in a coordinate system is usually described by two items: the intercept with the Y-axis, and the slope of the line. The slope is defined as rise over run, and indicates by how much \\(y\\) increases (or decreases is the slope is negative) if we add an additional unit of \\(x\\). In the notation which follows we will call the intercept \\(\\beta_{0}\\), and the slope \\(\\beta_{1}\\). It will be our task to estimate these values, also called coefficients. You can see this depicted graphically here:\n\n\n\n\n\n\nFigure 8: Regression Line\n\n\n\nIn the context of the module, we would for example have per capita GDP on the x-axis (independent variable), and Polity V measuring the level of democracy on the y-axis (dependent variable). This would look like this in the year 2015:\n\n\n\n\n\n\nFigure 9: Democracy and Development in 2015\n\n\n\n\nPopulation\nWe will first assume here that we are dealing with the population and not a sample. The regression line we have just drawn would then be called the Population Regression Function (PRF) and is written as follows:\n\\[\\begin{equation}\nE(y|x_{i}) = \\beta_{0} + \\beta_{1} x_{i}\n\\end{equation}\\]\n\\(E(Y|X_{i})\\) reads as the expected value of \\(y\\), given a particular observation of \\(x\\), and so the population regression function is also known as the conditional expectation function. It represents the average value of \\(y\\) we expect to observe, if \\(x\\) is equal to a particular value.\nAs you can see, the line is not intercepting with all observations – it only represents the conditional averages, after all. Only two observations are located on the line, and all others have a little distance between them and the PRF. These distances between \\(E(y|x_{i})\\) and \\(y_{i}\\) are called error terms and are denoted as \\(\\epsilon_{i}\\).\n\n\n\n\n\n\nFigure 10: Errors\n\n\n\nTo describe the observations \\(y_{i}\\) we therefore need to add the error terms to the PRF:\n\\[\\begin{equation}\ny_{i} = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon_{i}\n\\end{equation}\\]\n\n\nSample\nIn reality we hardly ever have the population in the social sciences, and we generally have to contend ourselves with a sample. Nonetheless, we can construct a regression line on the basis of the sample, the Sample Regression Function (SRF). It is important to note that the nature of the regression line we derive from the sample will be different for every sample, as each sample will have other values in it. Rarely, the PRF is the same as the SRF - but we are always using the SRF to estimate the PRF.\nIn order to flag this up in the notation we use to specify the SRF, we are using little hats over everything we estimate, like this:\n\\[\\begin{equation}\n\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\n\\end{equation}\\]\nAnalogously, we would would describe the observations \\(y_{i}\\) by adding the estimated error terms \\(\\hat{\\epsilon}_{i}\\) to the equation.\n\\[\\begin{equation}\ny_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i} + \\hat{\\epsilon}_{i}\n\\end{equation}\\]\nThese estimated error terms, \\(\\hat{\\epsilon}_{i}\\), are called residuals. The following graph visualises the relationship between an observation, the PRF, the SRF, the error terms, and the residuals.\n\n\n\n\n\n\nFigure 11: Population vs. Sample Regression Function",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#ordinary-least-squares-ols",
    "href": "02-PO33Q.html#ordinary-least-squares-ols",
    "title": "Week 2",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\nWhen you eye-balled the scatter plot at the start of this Chapter in order to fit a line through it, you have sub-consciously done so by minimising the distance between each of the observations and the line. Or put differently, you have tried to minimise the error terms \\(\\hat{\\epsilon}_{i}\\). This is basically the intuition behind fitting the SRF mathematically, too. We try to minimise the sum of all error terms, so that all observations are as close to the regression line as possible. The only problem that we encounter when doing this is that these distances will always sum up to zero.\nBut similar to calculating the standard deviation where the differences between the observations and the mean would sum up to zero (essentially we are doing the same thing here), we simply square those distances. So we are not minimising the sum of distances between observations and the regression line, but the sum of the squared distances between the observations and the regression line. Graphically, we would end up with little squares made out of each \\(\\hat{\\epsilon}_{i}\\) which gives the the method its name: Ordinary Least Squares (OLS).\n\n\n\n\n\n\nFigure 12: Ordinary Least Squares\n\n\n\n\nOrdinary Least Squares (OLS)\n\n\n\nWe are now ready to apply this stuff to a PO33Q-related example!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#the-basic-command",
    "href": "02-PO33Q.html#the-basic-command",
    "title": "Week 2",
    "section": "The Basic Command",
    "text": "The Basic Command\nThe command to run a regression in R is beguilingly simple:\n\nregression &lt;- lm(devar ~ indepvar, data=dataframe)\n\nWe specify an object into which we store the results of the regression, here regression, and assign a function to this object called lm which stands for “linear model”. It is then convention to state the dependent variable first in the command, which in our case will always be democracy. This is then followed by a tilde and the independent variable which you want to include.\nIn the case of modernisation theory, you might want to test what influence per capita GDP has on democracy. This week we are using the Polity V scale to measure democracy.\n\nThe ‘Polity Score’ captures [the] regime authority spectrum on a 21-pont scale ranging from -10 (hereditary monarchy) to +10 (consolidated democracy). (…) The Polity scheme consists of six component measures that record key qualities of of executive recruitment, constraints on executive authority and political competition. It also records changes in the institutionalized qualities of governing authority.”2\n\nWe start by setting the working directory\nand then load the Europe.csv data set into the workspace. We subset this data frame to observations from the year 1994, only. Yes, I know it’s a long time ago, but I need an example that works for everything I want to show you this week.\n\neurope &lt;- read.csv(\"Week 2/Europe.csv\")\nlibrary(tidyverse)\neurope_1994 &lt;- filter(europe, year == \"1994\")\n\nOur dependent variable is called polity5. The independent variable is called gpdpc. We are now ready to run our first regression:\n\nreg_pol &lt;- lm(polity5 ~ gdppc, data=europe_1994)\n\nWe can then produce a summary of the results as follows:\n\nsummary(reg_pol)\n\n\nCall:\nlm(formula = polity5 ~ gdppc, data = europe_1994)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5657 -0.3115  0.6054  1.5330  3.3036 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 6.120e+00  7.785e-01   7.861 2.45e-08 ***\ngdppc       1.377e-04  4.036e-05   3.411  0.00212 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.794 on 26 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.3092,    Adjusted R-squared:  0.2826 \nF-statistic: 11.64 on 1 and 26 DF,  p-value: 0.002123\n\n\nWe can extract the number of observations used for the estimation by calling:\n\nnobs(reg_pol)\n\n[1] 28\n\n\nThere is a lot of information in this, and I will take you through the output step by step now.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#interpreting-the-output",
    "href": "02-PO33Q.html#interpreting-the-output",
    "title": "Week 2",
    "section": "Interpreting the Output",
    "text": "Interpreting the Output\n\nThe Number of Observations\n\nnobs(reg_pol)\n\n[1] 28\n\n\nLet’s deal with the last step first. The number of observations is equal to the number of countries in this case. We have subset the data to the year 1994, and so we have 28 countries in the analysis. This seems trivial for now, but it will become important later on. Once observations are missing, R drops them from the analysis – especially in developing countries where data are often missing in large quantities this can lead to a rapid decimation in the number of observations. This in turn is problematic for the strength inference we can draw from the analysis.\n\n\nThe all-important p-value in Regression\n\nThe p-Value\n\n\n\nIt might seem odd to focus on the p-value before interpreting the coefficients substantively, but this is precisely what you have to do in practice, as well. If a regression coefficient is insignificant, then we have failed to reject the null hypothesis, and have no evidence for a relationship. This is it. This is all you can say. Should this happen to you in the assessment, please do not engage in speculation of “had the coefficient been significant, then it would suggest that…”. No. There is no relationship. State that, and move on. Only if there is a relationship you can move on to interpreting the coefficients substantively.\n\nIf you interpret insignificant coefficients substantively (other than that there is no relationship) in the assessment you will lose marks.\n\nBut how do you assess significance? We do a regression in order to ascertain whether there is a relationship between the independent and the dependent variable, or not. For testing whether there is one, we start from the assumption that there is none. This is what we call the null hypothesis; in our example here it would be that per capita GDP does not influence the level of democracy in a country.\nNow, remind yourself of the normal distribution from the video which has the mean age in its centre. In the video we were interested whether age influences height. Assuming, that there is no relationship between age and height, a regression line would look as follows:\n\n\n\n\n\n\nFigure 13: Flat Regression Line\n\n\n\nThe line is perfectly flat (the slope coefficient is zero), intercepting the y-axis at the mean. It does so, because the regression line is the value of y we would expect on average for a given x. If this value does not vary, it has to be the average of y. If we now put a zero slope coefficient, such as the one for height, just in the more general form of \\(\\beta_{2}\\) in the centre of a normal distribution, it looks like this:\n\n\n\n\n\n\nFigure 14: Null Hypothesis for \\(\\beta_{1}\\)\n\n\n\nWhat we want to test now, with regression, is whether the slope coefficient R has calculated for us (let me denote the estimated value of \\(\\beta_{1}\\) as \\(\\hat{\\beta_{1}}\\)), is far enough from this mean of zero, to say that we can be sure to say that there is a relationship.\nThe statement that there is a relationship between the independent and the dependent variable, is called the alternative hypothesis. In our case the alternative hypothesis would read: “The level of per capita GDP influences the level of democracy in a country”.\nSo how far away from the centre of zero do we have to go to say that there is indeed relationship, or put differently, that we have enough evidence to reject the null hypothesis? The standard in political science is that we need to have a 5% probability of finding a value more extreme than the one we have observed. Under the curve in the following graph, that is equal to the orange area on the right. And that is the p-value.\n\n\n\n\n\n\nFigure 15: p-value for \\(\\beta_{1}\\)\n\n\n\nIf this area is 5% or less, then we have observed a value for the slope coefficient which is so far away from our assumed mean of zero, that we have sufficient evidence to reject the null hypothesis.\nBut now, you might say, a slope coefficient can also be negative – here we are only looking at the right hand-side, and therefore at the scenario in which a slope coefficient is positive. And you are right. The scatter plot could give us a negative line. So, if we want to move away far enough from the assumed mean of zero in the centre, we must do so in both directions, to the left and to the right. Now, we need a value that is so far out, that to either side of the distribution, 2.5% of the area are left under the curve (2.5% on the left plus 2.5% on the right make the overall 5% we are interested in). We call this a two-sided test, whereas the scenario above is a one-sided test. The p-values reported by R for the slope coefficients are always two-sided tests (unless we tell R not to, but we are not doing that on this module). This value gives us the area under the normal distribution to the left and the right beyond our observed value, as shown in this figure:\n\n\n\n\n\n\nFigure 16: Two-Sided Test for \\(\\beta_{1}\\)\n\n\n\nThe value we are looking at in R to determine the p-value, is in the column \\(Pr(&gt;|t|)\\).\n\n\n\n\n\n\nFigure 17: p-value column\n\n\n\nIn order to satisfy the requirement of the p-value being 5% or less, this value needs to be smaller than 0.05. Otherwise, more than 5% area are left, and we are not certain enough that our value is far enough away from the zero mean in the centre to say that it is “statistically different” from it. When we look at the value for the slope coefficient gdppc here, 0.00212, this means that the areas on the left and the right are jointly 0.21% – small enough for us to be sure to have found a value that is far enough away from zero to claim that there is a relationship. We therefore reject the null hypothesis: we find evidence for a relationship between per capita GDP and Polity V in Europe in the year 1994. Again, if we want to visualise this, the actual p-value of \\(0.00212\\) would look like this:\n\n\n\n\n\n\nFigure 18: Visualisation of p-value\n\n\n\n\n\nThe Intercept\n\n\n\n\n\n\nFigure 19: Intercept output\n\n\n\nR always shows the value for the intercept in the intuitively labelled row “(Intercept)” and the column “Estimate”. In this case the value is 6.1. What does this coefficient mean, substantively? When you remember the graph depicting the regression line, this is the point where the line intercepts the y-axis. So, it is the value of \\(y\\), here democracy in the form of the Polity V score, when \\(x\\), here economic development in the form of GDP, is zero. So in other words, a country with a GDP per capita of zero would achieve a Polity V score of 6.1. The substantive interpretation sometimes makes sense (like here), but sometimes cannot be interpreted in this way.\n\n\nThe Slope Coefficient\n\n\n\n\n\n\nFigure 20: Slope output\n\n\n\nThe slope coefficient is shown in the row depicting the name of the independent variable, here “gdp”, and again the column “Estimate”. Our slope here, is \\(1.377e-04\\). The \\(e-4\\) means that we have to move the decimal point 4 units to the left, so written fully, this means \\(0.0001377\\). We interpret it as follows: for every additional unit of per capita GDP, measured in US$, the Polity V score increases by \\(0.0001377\\), on average. This seems very small, but when you consider the size of GDP per capita in many countries, it seems logical that this value is as small as it is. If the coefficient was negative, then this would mean that for for every additional unit of per capita GDP, measured in US$, the Polity V score would decrease by \\(0.0001377\\), on average.\n\nInterpreting a coefficient\nThe order in which to interpret a coefficient is as follows:\n\nIs it significant? If not, all you can say is that there is no influence. There is insufficient evidence to support the alternative hypothesis.\nIf it is significant, you can interpret its size and direction according to the statistical model (for example, slope coefficent vs. partial slope coefficient).\nWhat does the coefficient mean for the hypothesis? Look at the direction. Is the direction as predicted by the hypothesis? Then you have evidence to support the hypothesis. If the direction is inverse, then you have falsified the hypothesis, even though you have a significant coefficient.\n\n\n\n\nThe Goodness of Fit (R-Squared)\n\n\n\n\n\n\nFigure 21: R-squared output\n\n\n\n\nGoodness of Fit\n\n\n\nAs you have seen in the video, the goodness of fit is a measure to indicate how much of the variation in the dependent variable (democracy) the independent variable (per capita GDP) is explaining. For this, we take the ratio of the explained sum of squares over the total sum of squares. The resulting percentage is R\\(^2\\), also known as the coefficient of determination. This number can also be found on the R output, and is in our case \\(0.3092\\), or 30.92%. This value is not too bad for a single variable! The maximum we can explain is of course 100% with an R-Squared value of 1.0, even though this is a dream never achieved empirically. But we are still quite some distance of this dream, and can probably do better. There surely must be factors other than per capita GDP that explain democracy in Europe in the year 1994.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#choosing-variables",
    "href": "02-PO33Q.html#choosing-variables",
    "title": "Week 2",
    "section": "Choosing Variables",
    "text": "Choosing Variables\n\nCan I choose more than one independent variable?\nYes, you can! And this is where the fun starts, because now we are getting a step closer to the real world. New modernisation posits that democracy is multi-causal, and does not rest on the influence of GDP alone. Instead, it puts forward a number of concepts that act as independent variables, one of which is health. We can measure health through Hospital beds (per 1,000 people), and include this in our model, on top of GDP. To do this we type:\n\nreg_pol1 &lt;- lm(polity5 ~ gdppc + enrol_gross, data=europe_1994)\n\nYou see that adding independent variables is easy, we just add them on with a plus sign. It does not matter whether the expected direction of influence is positive or negative, always add additional variables with a “+”. The command ought to lead to the following output:\n\nsummary(reg_pol1)\n\n\nCall:\nlm(formula = polity5 ~ gdppc + enrol_gross, data = europe_1994)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.6561 -0.3999  0.2543  1.2566  3.0044 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -8.327e+00  6.458e+00  -1.290  0.21061   \ngdppc        1.129e-04  3.671e-05   3.076  0.00553 **\nenrol_gross  1.476e-01  6.352e-02   2.323  0.02979 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.421 on 22 degrees of freedom\n  (13 observations deleted due to missingness)\nMultiple R-squared:  0.4184,    Adjusted R-squared:  0.3655 \nF-statistic: 7.913 on 2 and 22 DF,  p-value: 0.002577\n\nnobs(reg_pol1)\n\n[1] 25\n\n\n\n\nCeteris Paribus\nLet us focus on the coefficient for gross primary school enrolment first. Its value is rounded \\(1.476e-01\\), or \\(0.1476\\), implying that for every additional percent of gross primary school enrolment, the Polity V score increases by \\(0.1476\\) units on average. So far, so good, but as we have included other variables in the regression model, namely per capita GDP, we need to account for this fact in our interpretation. We do this by adding “all other things being equal” (Latin: ceteris paribus) to this interpretation. What does this mean? It means, that if we take into account the level of per capita GDP, and hold this level constant, then for every additional percent of gross primary school enrolment the Polity V score increases by \\(0.1476\\) units on average. As such, we force the regression model to isolate the effect of gross primary school enrolment, by including other possible explanatory factors, such as per capita GDP. You will sometimes read this in articles in the form of “controlling for”. As the percentage of gross primary school enrolment only explains the Polity V score partially now, the coefficients in a multiple regression model are also known as partial slope coefficients.\nTo give you a different example: suppose we want to find out whether sex influences income. We could simply run a regression with income as the dependent variable, and sex as the independent variable. But we also know, that age influences income, as with increasing age people have more experience which is reflected in their salary. So even though we are not interested in the amount age influences income, we would include it in the regression model, so as to isolate the effect of the variable we are interested in: sex.\nBack to our modernisation example and school enrolment. The p-value for this coefficient is \\(0.02979\\), and therefore well below the required 5% threshold. We can conclude that the percentage of gross primary school enrolment influences the level of democracy in Europe in the year 1994. Per capita GDP remains significant.\n\n\nParsimony\nBut can you just add independent variables at your leisure? The short answer is no. The long answer is: parsimony. This means “as few as possible, as many as necessary”. The “necessary” component is guided by the theoretical underpinning of your investigation. For example, you subscribe to new modernisation theory, and believe that that it is not only economic development in the form of per capita GDP that determines the level of democracy, but that indicators of social change also play an important role. Now it is your job as a researcher to decide how we measure social change. Do we include health? And if so, how do we measure it, say by number of hospital beds per person? Should we choose a different measure for education that measures educational outcomes more directly than enrolment? Then we might decide on literacy. But are these two enough to measure social change, or do we need to look at other facets? We seek to include as few as possible to produce an empirical picture of social change, but so many that we are doing proper justice to the theory.\nWe can then proceed to test different scenarios. For example, does economic development already explain democracy? What happens if we add social change? Or does social change explain democracy on its own, already? These questions lead to the topic of “model specification”, which we will discuss in greater detail in week 4.\n\n\nR-Squared Again\nAs soon as we introduce more than one independent variable to the model, we cannot use “Multiple R-Squared” any more. The reason is that this measure cannot properly take into account added variables. It will either stay the same, or increase, it cannot decrease. This of course, makes no sense, for example if we add average shoe size in 1994 to the analysis, this would not help to explain democracy, but Multiple R-Squared would still likely go up. We therefore need a new measure, called Adjusted R-Squared which not only penalises us for adding more independent variables, but will also decrease if a variable takes explanatory power away from a model. You find it here:\n\n\n\n\n\n\nFigure 22: Adjusted R-squared location",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#limitations",
    "href": "02-PO33Q.html#limitations",
    "title": "Week 2",
    "section": "Limitations",
    "text": "Limitations\nOLS is a great method to model the relationship between one or multiple independent variables on a dependent variable. But this only works for cross-sectional data. OLS cannot deal with time-series data a condition that is crucial when we want to model the emergence and survival of democracy.\nWhy? There is a bunch of assumptions that need to be satisfied in order for OLS to deliver us with accurate results, collectively known as the Classical Linear Assumptions (CLM). One of these is that the error terms (our \\(\\epsilon_i\\)s) are not correlated with one another. This assumption is rarely met in time series data, as for example the Polity V score does not miraculously begin to dependend on the percentage of gross primary enrolment in a given year. It will also depend to an extent on the percentage of gross primary enrolment in the previous year, and indeed on the Polity V score of the previous year. This would cause autocorrelation in the error terms which is bad news for the CLM.\nAs such, you will not be able to use linear regression in this format in the final assessment!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#literature-recommendations",
    "href": "02-PO33Q.html#literature-recommendations",
    "title": "Week 2",
    "section": "Literature Recommendations",
    "text": "Literature Recommendations\n\nFogarty (2023) — Chapter 11, read Chapter 12 if you want to be really good. Online\nStinerock (2022) — Chapter 12 and Sections 13.1-13.5, 13.8 and 13.10. Online",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#important-disclaimer",
    "href": "02-PO33Q.html#important-disclaimer",
    "title": "Week 2",
    "section": "Important Disclaimer",
    "text": "Important Disclaimer\nI have discussed linear regression with you this week for two reasons:\n\nI am trying to sketch the methodological development through which the relationship between economic development and democracy has been analysed over time. Lipset (1959) started with a simple correlation analysis, and this quickly developed into linear regression analysis, as it is more powerful and more sophisticated than correlation analysis.\nIt forms the foundation for the binary response models (probit in our case) which we will start with next week. At first, we will only look at the probability to be democratic in a particular year across different countries. But in Week 7, we will extend this to not only investigate this relationship across countries, but also over time. This dynamic probit, or Markov Transition Model (MTM), is the model you need to apply in the assessment of the module. Please DO NOT stick to linear regression in the assessment, as OLS is not capable to deal with time-series data. The same applies for a cross-sectional probit. The only method permissible in the assessment is an MTM.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO33Q.html#footnotes",
    "href": "02-PO33Q.html#footnotes",
    "title": "Week 2",
    "section": "",
    "text": "For the pedants: I am aware this should read \\(\\bar{y} - t_{\\alpha/2} \\cdot se \\le \\mu \\le \\bar{y} - t_{\\alpha/2} \\cdot se\\), but I have left the subscripts out to make it less confusing here.↩︎\nFor more detail see http://www.systemicpeace.org/polityproject.html.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html",
    "href": "03-PO33Q.html",
    "title": "Week 3",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#learning-outcomes",
    "href": "03-PO33Q.html#learning-outcomes",
    "title": "Week 3",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nApply the tasks of conceptualisation and measurement\nEvaluate different conceptualisations and measurements of democracy\nUnderstand and apply binary response models (probit) in the context of cross-sectional data\nInterpret the results of binary response models\nDesign results tables with the R package modelsummary",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#probit---what-is-it",
    "href": "03-PO33Q.html#probit---what-is-it",
    "title": "Week 3",
    "section": "Probit - What is it?",
    "text": "Probit - What is it?\nA question we can all relate to is whether to go out tonight, or not. The “propensity to go out” is not directly observable, and so we call this a latent variable. You can imagine this running from minus infinity to plus infinity, and at some point on this continuum you are making the decision to go out. Let’s call this point tau (\\(\\tau\\)). Graphically, this would look like this:\n   \n\n\n\n\n\n\nFigure 1: Latent Variable\n\n\n\n   \nYour inclination to go out, is likely to be influenced by the amount of money you have in your wallet / bank. If you are broke, you will be less inclined (if you are sensible), and if you are swimming in it, you will be more inclined. So, if the “propensity to go out” (which remember is running from minus to plus infinity) is influenced by your budget, then let’s construct a graph, in which we pop the propensity to go out on the y-axis, and the budget on the x-axis. If we assume that this relationship is linear, we can fit a regression line into this coordinate system, just as we have in the previous week:\n   \n Whilst this visualises the influence of the budget on the latent variable, what we are aiming for is to make a prediction about the probability of you going out, or not.\nNow imagine, your budget is \\(x_{1}\\). The regression line depicts the propensity that we would expect to see, on average, for somebody with a budget of \\(x_{1}\\). But the crucial point is that not everybody is average. Some might have an essay deadline approaching which makes them even less likely to go out. Others might just have received their essay mark, and want to celebrate that they scored a first. In other words, there is variability around the regression line. And we assume that whilst this variability is random, it still follows a particular distribution. In the case of a probit, this is the normal distribution1. I have added these distributions to Figure 2.\n   \n\n\n\n\n\n\nFigure 2: Towards the Cumulative Density Function, adapted from Long (1997, p. 46)\n\n\n\n   \nThe probability of going out is coloured in in grey. As you can see, even at budget \\(x_{1}\\), some area of the distribution has slipped over the cut-off point, \\(\\tau\\). As the budget increases, more and more probability slides over the threshold \\(\\tau\\), until we reach the magical point of \\(x_{5}\\) where the probability is 50%. From there on, the amount of probability sliding over \\(\\tau\\) is steadily decreasing, because of the shape of the normal distribution.\nWe can depict the amount of probability (or the size of the grey area) for each \\(x_{i}\\) in a separate graph which is called the Cumulative Probability Density Function, or short CDF:\n   \n\n\n\n\n\n\nFigure 3: The Cumulative Distribution Function, adapted from Long (1997, p. 46)\n\n\n\n   \nThis s-shaped curve now gives us the probability (of going out) for each \\(x_{i}\\) (budget). It is important to note that the relationship is not linear, as in linear regression. Because we have an s-shaped curve the increase in probability when going from \\(x_{2}\\) to \\(x_{3}\\) is not the same as going from \\(x_{3}\\) to \\(x_{4}\\). You can see that visualised here:\n   \n\n\n\n\n\n\nFigure 4: Marginal Effect under the Cumulative Density Function\n\n\n\n   \nWe will therefore not be able to interpret the coefficients in the same way as for OLS. We will be using predicted probabilities instead. But one step at a time. Let’s first get our hands dirty with some data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#the-command",
    "href": "03-PO33Q.html#the-command",
    "title": "Week 3",
    "section": "The Command",
    "text": "The Command\nThe command to run a probit is as follows:\n\nmodel &lt;- glm(depvar ~ indepvar\n                     data = dataframe, \n                     family = binomial(link = \"probit\"))\n\nglm stands for “Generalised Linear Model”. We then specify a model in the same way as last week, stating the dependent variable first, followed by a tilde and then the independent variable(s). We complete the command by naming the data frame we wish to use and selecting the model type, in our case it is a probit model which is binomial (our dependent variable only has two possible outcomes).\nOnce again, let us do a specific example with the data set “Europe” in the year 2000. First set the working directory for today’s seminar:\n\nsetwd(\"~/Warwick/Modules/PO33Q/Week 3\")\n\nand then load the data set into a data frame called world which we subset to the year 2000.\n\nworld &lt;- read.csv(\"files/Week 3/world.csv\")\n\nlibrary(tidyverse)\n\nworld2000 &lt;- filter(world, year==2000)\n\nFor a probit regression, we cannot use the Polity V scale any more, because it is continuous, and not binary. We therefore switch the democracy coding to index created by Boix, Miller & Rosato in 2018. We will use life expectancy as our independent variable, and examine the situation in the year 2000. To do this, we call:\n\nprobit &lt;- glm(democracy ~ life, \n              data = world2000, \n              family = binomial(link = \"probit\"))\n\nThis should lead to the following results:\n\nsummary(probit)\n\n\nCall:\nglm(formula = democracy ~ life, family = binomial(link = \"probit\"), \n    data = world2000)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.83873    0.71998  -5.332 9.73e-08 ***\nlife         0.05970    0.01068   5.588 2.30e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 258.49  on 187  degrees of freedom\nResidual deviance: 224.06  on 186  degrees of freedom\n  (3 observations deleted due to missingness)\nAIC: 228.06\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#the-coefficients",
    "href": "03-PO33Q.html#the-coefficients",
    "title": "Week 3",
    "section": "The Coefficients",
    "text": "The Coefficients\nThe coefficients are displayed in the results as follows:\n\n\n\nCoefficients\n\n\nThe values of these coefficients determine the shape of the s-shaped curve we derived above. To see exactly how different values of the intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)) coefficients affect the curve, please use this app (it may take a moment to load, as it is a web-based application).\nBut how do we interpret the coefficients substantively? As you have seen in the Theory section above, the relationship between our independent variable and the probability of democracy is not linear: the curve was s-shaped, so that the increment in probability is not the same for, say moving from \\(x_{1}\\) to \\(x_{2}\\) and from \\(x_{2}\\) to \\(x_{3}\\). For illustration see the orange bars in the following Figure:\n   \n\n\n\n\n\n\nFigure 5: Marginal Effects for Life Expectancy on the Probability of Democracy\n\n\n\n   \nWhat we therefore need to do in the world of probit, is to evaluate the probability at individual values of \\(x_{i}\\), or put differently, assess how much of the bell-shaped curve has slid across our cut-off point \\(\\tau\\) (tau) at a particular point \\(x_{i}\\). And this brings us to predicted probabilities.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#predicted-probabilities",
    "href": "03-PO33Q.html#predicted-probabilities",
    "title": "Week 3",
    "section": "Predicted Probabilities",
    "text": "Predicted Probabilities\nOnce we have estimated the model, we have determined the shape of the s-shaped curve in Figure 3. What we now need to do, is to evaluate the probability on the y-axis for different values on the x-axis. In our case this is the variable life.\n\nSetting the x-values\nLet us first get a basic overview of the variable life. We can do this by calling\n\nsummary(world2000$life)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  44.83   60.16   69.42   66.70   73.94   82.12       3 \n\n\nSo we know that the average life expectancy in the world in the year 2000 was 66.70 years, with a minimum of 44.83 years, and a maximum of 82.12 years. We also have 3 missing observations (NA) which we need to exclude from the following by setting na.rm=TRUE2, as R is otherwise unable to calculate descriptive measures within the setx function, such as the mean. Let us set life to its mean now, by typing:\n\nsetx = data.frame(life=66.70)\n\nIf you want to change this value for calculations later on, you simply set different values in the setx dataframe or specify a new one).\nWe now have the shape of the probability curve, and we have agreed on a point on the x-axis. We are finally ready to have a look at the probability to be a democracy at this point.\n\n\nPredicting the Probability\nThe quantity of interest we are interested in is the probability of being a democracy. To calculate this quantity of interest, we use the function predict() and specify within this function for which model we want to calculate the quantity, and the data frame in which we specified the value of our x-variable. In our case this variable is life and we have assigned the mean value to an object called setx earlier.\n\npredict(probit, setx, type=\"response\")\n\n        1 \n0.5568585 \n\n\nRemember that we have put the value of life expectancy at the mean. For this level of life expectancy R returns to us a probability of being a democracy at 55.68%. Contrarily, this means that probability to be an autocracy is 44.32% (the two probabilities always sum up to 1).\nNow we set life expectancy to its minimum\n\nsetx = data.frame(life=min(world2000$life, na.rm = T))\n\nand calculate the quantity of interest again. Our probabilities have changed very drastically (ensure you get the same results before proceeding):\n\npredict(probit, setx, type=\"response\")\n\n        1 \n0.1225553 \n\n\nIf we set life expectancy to its maximum (how?), then we receive the following regime probabilities:\n\npredict(probit, setx, type=\"response\")\n\n        1 \n0.8562414 \n\n\nNow we can make statements such as:\n\nIn 2000, a country’s probability to be a democracy with average life expectancy was 55.68%.\nAt the minimum life expectancy of 44.42 years, this probability drops by 43.49%age points to 12.26%.\nAs such, a country in 2000 at minimum life expectancy would more likely be an autocracy than a democracy (why?).\n\n\n\nAll Predicted Probabilities\nIt is also possible to depict the predicted probabilities for all values of your independent graphically. That would look like this:\n   \n\n\n\n\n\n\nFigure 6: Predicted Probabilities\n\n\n\n   \n\n\n\nCode for the Predicted Probabilities Graph\n\n\nprobit &lt;- glm(democracy ~ life, \n               family = binomial(link = \"probit\"), \n               data = world2000)\n\nxlife &lt;- seq(0, 81.08, 1)\n\nylife &lt;- predict(probit, list(life = xlife),type=\"response\")\n\npredictions &lt;- data.frame(xlife,ylife) \n\nggplot(data=predictions, aes(x=xlife, y=ylife)) +\n  geom_line() +\n  labs(x= \"Life Expectancy at Birth\", y=\"Pr(Democracy)\") +\n  theme_classic()+\n  theme(axis.text=element_text(size=14),\n        axis.title=element_text(size=16))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#how-to-report-results",
    "href": "03-PO33Q.html#how-to-report-results",
    "title": "Week 3",
    "section": "How to Report Results",
    "text": "How to Report Results\nThe question is now: how do you report all of this in an article, or closer to home, in your assessment? Let us start by calculating a model which assesses the impact of per capita GDP on the probability to be a democracy in 2000, worldwide. The R commands and output look like this:\n\nprobit1 &lt;- glm(democracy ~ gdppc,\n                data = world2000, \n                family = binomial(link = \"probit\"))\nsummary(probit1)\n\n\nCall:\nglm(formula = democracy ~ gdppc, family = binomial(link = \"probit\"), \n    data = world2000)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.691e-01  1.138e-01  -1.485    0.137    \ngdppc        5.201e-05  1.283e-05   4.054 5.02e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 256.87  on 186  degrees of freedom\nResidual deviance: 232.63  on 185  degrees of freedom\n  (4 observations deleted due to missingness)\nAIC: 236.63\n\nNumber of Fisher Scoring iterations: 5\n\n\nNow, please, please never, ever copy this into an article or an assessment, as every time you do this, a little part of me dies. Make the effort of reporting the results in a nice and neat Table, that only contains all relevant information, communicates it in an accessible form, and is clearly labelled. The output above, processed properly, would look like this:\n\n   \n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable: Democracy\n\n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  per capita GDP\n                  0.000***\n                \n                \n                  \n                  (0.000)\n                \n                \n                  Intercept\n                  -0.169\n                \n                \n                  \n                  (0.114)\n                \n        \n      \n    \n\n\n\n\nTable 1: Influence of per capita GDP on Democracy\n\n\n\n\n   \n\nYou see, this table manages to convey clearly the relationship we are assessing, the label of the independent variable (not in the form of cryptic variable names), the value of the intercept, the value of the slope coefficient, their respective p-value, and the number of observations. It also has an informative caption underneath and is properly labelled (so that it can be cross-referenced in the text). As such, the table itself could already communicate the main take-away message without somebody looking at the text. This needs to be the goal: the table, or figure needs to be able to communicate the message without having to look at the text. In turn, the text needs to be able to communicate the message on its own without looking at the table. But both need to say the same thing.\nAfter you have reported the results of the regression output in this way, you can then proceed to interpret (in the text) the transition probabilities as discussed above. Again, you might wish to sum up the main results in tabular format.\n\nmodelsummary\nRather than setting tables manually in Excel, or even worse in Word (it basically violates all standards of professional table formatting), you can let R do this work for you. All you need for this is the modelsummary package (Arel-Bundock, 2022).\nJust like writing a shopping list, we start by creating a list of models for which we want modelsummary to produce a table. You can merely list the names of the objects in which the models are stored, such as probit1, or you can give them specific names which will appear as column titles in the table. This addition is by no means a must, but sometimes you might wish to give models a particular name, for example if you have used different methods of estimation, different components of a theory, emergence vs. survival, etc. By default, modelsummary just numbers models in ascending order.\n\nmodels &lt;- list(\n  \"(1)\"    = probit1\n)\n\nThis simple step is already sufficient to produce – an admittedly somewhat crude – results table. All we have to do is to load the modelsummary package and to use the previously defined list of models as the argument of the modelsummary() function:\n\nlibrary(modelsummary)\nmodelsummary(models)\n\n\nExecute these code chunks as we go through this Section, so you can see the alterations we make to the table in real-time.\n\nThis table is a vast improvement on the raw R output you would receive when calling summary(model1). But it is far from finished. One characteristic which is conspicuously absent is an assessment of statistical significance. In model summaries of this kind this information is usually provided in the form of asterisks or other symbols next to the respective coefficient. We can add these simply by adding the option stars=TRUE to the code.\n\nmodelsummary(models,\n             stars = TRUE)\n\nNext up is the modification of the stub. The stub is the leftmost column in which you name the indicators for which coefficients will be presented. In the stub, “[a]bbreviate nothing. And never ever ever use computer variable names to stand for concepts. These are personal code words that convey no meaning to readers.” (Stimson, n.d., p. 10) Following this advice, let us add proper labels to the independent variables.\nTo let modelsummary know how to replace each variable name, we create a so-called coefficient map. This is really just a character vector which follows a “before-after” logic in each of its rows. For example 'gddpc'='per capita GDP' replaces the variable name gdppc with the label per capita GDP. We need to do this for all the variables we used in our models. I am storing this in a vector called cm which stands for coefficient map.\n\ncm &lt;- c('gdppc'    = 'per capita GDP',\n        '(Intercept)' = 'Intercept')\n\nThe next step is easy, as we only need to feed this coefficient map into the modelsummary code with the option coef_map=cm:\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm)\n\nThis is a personal thing, but I also like to show the label of the dependent variable in results tables. They just feel incomplete to me without this information. There is no default to achieve this in modelsummary and so we need to use a little trick.\nWhat I want to add in the first row, but only in the second columm, is the text “Dependent Variable: Democracy”. This requires us to alter the style of the table slightly. modelsummary uses another package called tinytable to style the output. We load the package with library(tinytable) and instruct R to place the text where we want it.\n\nlibrary(tinytable)\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm)|&gt;\n  group_tt(j = list(\"Dependent Variable: Democracy\" = 2))\n\nLastly, let us tackle the model fit statistics. By default, modelsummary prints a whole festival of these into the bottom section of the table, but I want to concentrate only on the number of observations for now. We will be adding a model fit measure in Week 7.\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'DF|Deviance|Log.Lik|F|AIC|BIC|RMSE')|&gt;\n  group_tt(j = list(\"Dependent Variable: Democracy\" = 2))\n\nThis should produce this table:\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:Democracy\n\n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  per capita GDP\n                  0.000***\n                \n                \n                  \n                  (0.000)\n                \n                \n                  Intercept\n                  -0.169\n                \n                \n                  \n                  (0.114)\n                \n        \n      \n    \n\n\n\nYou can export your tables for use in MS Word, or Apple Pages. But you will not be able to export the full table, only up to the point at which you are specifying the options for tinytable with |&gt;. This is probably as good a reason as any to start working with Quarto or LaTeX… But if you are happy with this caveat, the following code will render the table in an MS Word document (output=\"table.docx\") that is placed in your working directory.\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'DF|Deviance|Log.Lik|F|AIC|BIC|RMSE',\n             output = \"table.docx\")\n\n\nShould you use MS Word to write your essay and your essay is saved in your working directory, do not save the table document under the same name as your essay, as R will overwrite it and it will be gone forever.\n\nIt is worth investing some time into this. There is a learning curve at the beginning, but it will make your life so much easier further down the line.\n\n\nMultiple Independent Variables\nAs in multiple linear regression, you can also have multiple independent variables in a probit model. Schematically this would look as follows:\n\nprobit &lt;- glm(depvar ~ indepvar1 + indepvar2 + indepvar3, \n              data = dataframe, \n              family = binomial(link = \"probit\"))\n\nThe fun starts when you begin to calculate probabilities for different values for all of these independent variables.\n\nSetting the x-values\nYou can set the value for each variable individually, such as in this little example:\n\nsetx = data.frame(gdppc=min(world2000$gdppc, na.rm = T),\n                  life=66.49)\n\n\n\n\nPredicting the Probability\nThis is very much the same as above with one independent variable. It is crucial to remember which variables you have set at which value, so that you can interpret the probability values correctly. As in linear regression, the interpretation of individual variables is ceteris paribus. It therefore makes sense, if you wish to isolate the effect of one, single variable, only to vary the values of that variable, and to leave all the other ones the same. If you varied two variables at the same time, for example, you would no doubt see a change in probability, but you could not attribute it to a single independent variable, any more.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#literature-recommendations",
    "href": "03-PO33Q.html#literature-recommendations",
    "title": "Week 3",
    "section": "Literature Recommendations",
    "text": "Literature Recommendations\n\nFogarty (2023): Chapter 13, pp. 268-295\nStinerock (2022): Section 13.12\nLong (1997); More Technical and in-depth if you want some background",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO33Q.html#footnotes",
    "href": "03-PO33Q.html#footnotes",
    "title": "Week 3",
    "section": "",
    "text": "For logit, the logistic distribution is used. This would lead to very similar results.↩︎\nThis stands for “not available remove equals true”.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html",
    "href": "04-PO33Q.html",
    "title": "Week 4",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html#learning-outcomes",
    "href": "04-PO33Q.html#learning-outcomes",
    "title": "Week 4",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nCritically assess the conceptualisation and measurement of socio-economic development in the context of modernisation theory\nUnderstand the implications of missing data\nApply strategies to deal with missing data\nApply different strategies in statistical modelling",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html#options-in-r",
    "href": "04-PO33Q.html#options-in-r",
    "title": "Week 4",
    "section": "Options in R",
    "text": "Options in R\nMissing data is a real problem in doing empirical research. What compounds this problem is that developing countries are more affected than developed ones. Ironically, we are more interested in developing countries in this module, and so this is going to become a real-life struggle for you over the coming weeks.\nR has multiple ways in which to address missing data. We have already encountered the exclusion of missing values when using descriptive statistics within the setx function in Week 3, such as:\nWhen we run regression models, for example through the function lm, R let’s you specify what you wish to do with observations (rows) that have missing values in them with the function na.action. This option has four logical options1:\n\nna.fail: Stop if any missing values are encountered\nna.omit: Drop out any rows with missing values anywhere in them and forgets them forever.\nna.exclude: Drop out rows with missing values, but keeps track of where they were (so that when you make predictions, for example, you end up with a vector whose length is that of the original response.)\nna.pass: Take no action\n\nBy default, R would apply na.omit which we call “listwise deletion”. This means, that as soon as one value within an observation is missing, R drops that entire observation. This can potentially decimate the number of observations for analysis quite drastically! This in turn has implications for inference. Let me illustrate this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html#methodological-implications",
    "href": "04-PO33Q.html#methodological-implications",
    "title": "Week 4",
    "section": "Methodological Implications",
    "text": "Methodological Implications\nIn an ideal world, we would have a data set in which each value is present (indicated by the presence of an x in each of the cells of this table):\n\n\n\n\n\n\n\n\n\n\n\nvar1\n\n\nvar2\n\n\nvar3\n\n\nvar4\n\n\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\nTable 1: An Ideal World\n\n\n\n\n\n\nUnfortunately, in the real world, we are much more likely to have a Swiss cheese, such as this:\n\n\n\n\n\n\n\n\n\n\n\nvar1\n\n\nvar2\n\n\nvar3\n\n\nvar4\n\n\n\n\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\nx\n\n\nx\n\n\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\n\n\nTable 2: The Real World\n\n\n\n\n\n\nYou will already have seen this when opening the data sets we are working with. In Figure 1, for example, the variable gdppc only has three missing values, but agri is completely missing.\n\n\n\n\n\n\n\n\n\nFigure 1: Excerpt from the world data set\n\n\n\nIf you ran a regression model that uses the variable agri, listwise deletion would delete all observations and you would end up with no model. Here, the amount of observations you would lose is obvious. But since different variables have missing values in different places, it is often not as obvious as that. Assume, for example that we wished to use all four variables in our Schematic in Table 3. Even though the “missingness” problem looks much less pronounced than in the agri variable in the world data set (see Figure 1), listwise deletion would ensure that we would only be left with two observations out of ten in our model:\n\n\n\n\n\n\n\n\n\n\n\nvar1\n\n\nvar2\n\n\nvar3\n\n\nvar4\n\n\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\nx\n\n\nx\n\n\n\n\n\n\n\nTable 3: What is left…\n\n\n\n\n\n\nSo what do you do when you find yourself in such a situation? An obvious answer would be to simply leave out a variable that has a high degree of missingness. This solution is flawed, however. If your theory tells you that the variable plays a role in measuring one of the concepts then leaving it out would create so-called omitted variable bias – a problem you really don’t want to have. A much better solution, therefore, is to use a proxy variable. Take agri as an example. It measures the percentage of GDP generated by agriculture, and would probably enter a statistical model to measure the transition of a traditional society (agricultural) to a modern, industrialised society. So, rather then measuring the move away from agri, you would also measure the transformation into industry. If you can find a variable measuring the percentage of GDP generated by the industry sector, and this variable has fewer observations missing, you could use it instead of agri and still be measuring the same thing.\nSometimes, it is not as straightforward as this, however, and you will have to resort to variables that have less measurement validity. Assume, for example, you want to measure the population’s ability to read. Then literacy from our data set would be an obvious choice. But sadly, it has a high number of observations missing. An alternative is primary gross enrolment (enrol_gross), as pupils learn to read and write in primary school. This variable does not measure the characteristic we are interested in as directly as literacy, but it would allow us to include more observations in our model. It is an art to balance measurement validity against the number of observations, there is no hard and fast rule to help you make this decision. But as long as you discuss and explain your reasoning in the assessment, you will be fine for this module – after all, you are only starting out on this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html#control-variables",
    "href": "04-PO33Q.html#control-variables",
    "title": "Week 4",
    "section": "Control Variables",
    "text": "Control Variables\nThe selection of independent variables MUST be guided by theory. After all, this is our purpose in running regression models: finding out whether a particular theory can explain an empirical phenomenon we are witnessing. If you select variables that are irrelevant to your theory, then the research design breaks down, as you are no longer focusing on testing your theory. I know it sounds trivial, but you would be surprised how many students mess this up in assessments.\nBut there is one – and only one – exception to the rule of not including variables that are not motivated by the theory you are testing: so-called control variables.\nFor example, we suspect that the amount of official development aid (ODA) a country has received might influence democracy. Modernisation theory is only concerned with processes WITHIN a country, and so a flow of money coming from outside of the country is not part of the theory. Nonetheless, such funds can either facilitate development, in the sense of helping to build infrastructure such as roads, etc. Or it can be used for democracy promotion directly (the World Bank does tie some of its ODA funds to this purpose), through funding relevant institutions, facilitating elections, etc. So, even though these external funds have no place in our theory, we have strong reason to believe that they affect the dependent variable. And it is for this reason, that we would still include them in our regression model to control for their influence. Why? The principle of ceteris paribus (see Week 3) only applies with respect to those variables included in the model. And if we wish to control for ODA, or to purify those coefficients which are motivated by theory from the influence of ODA, we need to include this variable in the model in order to do so.\nEven though I am presenting this here as the last step, you would realistically select these variables together with the ones motivated by the theory, as you will also have to perform conceptualisation and measurement, check data availability, etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html#literature-recommendations",
    "href": "04-PO33Q.html#literature-recommendations",
    "title": "Week 4",
    "section": "Literature Recommendations",
    "text": "Literature Recommendations\n\nFogarty (2023): Chapter 11: Section “Multiple Regression and Model Building”\nStinerock (2022): Section 13.11",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO33Q.html#footnotes",
    "href": "04-PO33Q.html#footnotes",
    "title": "Week 4",
    "section": "",
    "text": "https://faculty.nps.edu/sebuttre/home/R/missings.html↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html",
    "href": "05-PO33Q.html",
    "title": "Week 5",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html#learning-outcomes",
    "href": "05-PO33Q.html#learning-outcomes",
    "title": "Week 5",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nExplain and critically assess the role of political culture in democratisation\nUnderstand the issues in researching the role of culture in democratisation",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html#the-world-value-survey",
    "href": "05-PO33Q.html#the-world-value-survey",
    "title": "Week 5",
    "section": "The World-Value Survey",
    "text": "The World-Value Survey\n\nThe World Values Survey () is a global network of social scientists studying changing values and their impact on social and political life, led by an international team of scholars, with the WVS Association and WVSA Secretariat headquartered in Vienna, Austria.\nThe survey, which started in 1981, seeks to use the most rigorous, high-quality research designs in each country. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world’s population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values ever executed, currently including interviews with almost 400,000 respondents. Moreover the WVS is the only academic study covering the full range of global variations, from very poor to very rich countries, in all of the world’s major cultural zones.\n\nThe WVS has taken place across six waves, each collecting data from a different set of countries:\n \n\n\n\n\n\nWave\n\n\nYear\n\n\n\n\n\n\nWave 1\n\n\n1981-1984\n\n\n\n\nWave 2\n\n\n1990-1994\n\n\n\n\nWave 3\n\n\n1995-1998\n\n\n\n\nWave 4\n\n\n1999-2004\n\n\n\n\nWave 5\n\n\n2005-2009\n\n\n\n\nWave 6\n\n\n2010-2014",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html#descriptives",
    "href": "05-PO33Q.html#descriptives",
    "title": "Week 5",
    "section": "Descriptives",
    "text": "Descriptives\nFor all exercises, use the “WVS” data set which is available in the Downloads Section.\n\nWhat is the average value for survival/self-expression values? What does this mean?\n\nFiltering through Waves, how has this average changed?\n\nRepeat these two steps with traditional/rational values.\n\nWhy is the comparison of these values over time difficult?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html#linear-regression",
    "href": "05-PO33Q.html#linear-regression",
    "title": "Week 5",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nFor which waves do traditional–rational values in the population explain a country’s level of democracy?\n\nFor which waves do survival–self-expression values in the population explain a country’s level of democracy?\n\nIdentify reasons why earlier waves fail to explain the level of democracy.\n\nAssessed jointly, how much more or less democratic do survival/self-expression value and GDP growth make countries in wave 5 (2005-2009)?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html#probit",
    "href": "05-PO33Q.html#probit",
    "title": "Week 5",
    "section": "Probit",
    "text": "Probit\n\nFor which waves do traditional-rational values in the population explain a country’s probability to be a democracy?\n\nFor which waves do survival-self-expression values in the population explain a country’s probability to be a democracy?\n\nIdentify reasons why earlier waves fail to explain regime type.\n\nCalculate a model assessing the impact of GDP growth on the probability of democracy for countries in wave 5.\n\nHow much less likely is a country to be a democracy moving from minimum GDP growth to maximum GDP growth in wave 5?\n\nCalculate a model assessing the impact of traditional-rational values, and GDP growth on the probability of democracy for countries in wave 5. Interpret the results.\n\nConsidering the main propositions of cultural modernisation, what are the implications of these results? Assess the results of each of the previous tasks in turn.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO33Q.html#solutions",
    "href": "05-PO33Q.html#solutions",
    "title": "Week 5",
    "section": "Solutions",
    "text": "Solutions\nYou can find the solutions to these exercises in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html",
    "href": "07-PO33Q.html",
    "title": "Week 7",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#learning-outcomes",
    "href": "07-PO33Q.html#learning-outcomes",
    "title": "Week 7",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nUnderstand and apply binary response models in the context of time-series, cross-sectional data (Markov-Transition Models)\nModel the emergence and survival of democracy\nEvaluate model fit for binary response models\nDesign custom lines in modelsummary tables",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#time-series-cross-sectional-data",
    "href": "07-PO33Q.html#time-series-cross-sectional-data",
    "title": "Week 7",
    "section": "Time Series, Cross-Sectional Data",
    "text": "Time Series, Cross-Sectional Data\nWe are now entering the real world, as we will start to look at how characteristics vary, not only across different countries, but also across time. I have already presented the structure of time-series, cross-sectional (TSCS) data in Week 1, but to jog your memory, here it is again:\n\n\n\n\n\nSchematic: Time-Series, Cross-Sectional Data\n\n\ncountry\nyear\nliteracy\n\n\n\n\nA\n2000\n80\n\n\nA\n2001\n81\n\n\nA\n2002\n85\n\n\nA\n2003\n90\n\n\nB\n2000\n92\n\n\nB\n2001\n93\n\n\nB\n2002\n95\n\n\nB\n2003\n99\n\n\n\n\n\n\n\nAs you can see, each country has multiple observations, one for each year in which data have been observed. We will be using this information this week to calculate probabilities of regime transitions. For example, what is the probability of a country to transition from autocracy to democracy? As we only observe the regime type once for each year, this question alludes to the difference in regime type between two years in the same country. We will do this with a so-called Markov Transition Model.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#what-are-markov-transition-models",
    "href": "07-PO33Q.html#what-are-markov-transition-models",
    "title": "Week 7",
    "section": "What are Markov Transition Models?",
    "text": "What are Markov Transition Models?\nA Markov Transition Model (MTM) models the probability of being a democracy in a particular year, given its regime type in the previous year. So, if we model the probability of a country to be a democracy this year, given that it was an autocracy in the previous year, we are modelling the probability of a transition from autocracy to democracy. Similarly, if we model the probability of a country to be a democracy this year, given that it was also a democracy in the previous year, we are modelling the probability of democratic survival. We can express these probabilities as so-called conditional probabilities.\n\nConditional Probabilities\nConditional probabilities express what I have described before in a formal way. The conditional probability to model for democratic emergence is written as follows:\n\\[\\begin{equation}\nP(y_{i,t} = 1 | y_{i, t-1} = 0)\n\\end{equation}\\]\nThis reads: The probability of a country i to be a democracy (y=1) in year t, given (this is what the vertical line | says) that country i was an autocracy (y=0) in the previous year (t-1). Analogously, the conditional probability for democratic survival is:\n\\[\\begin{equation}\nP(y_{i,t} = 1 | y_{i, t-1} = 1)\n\\end{equation}\\]\nThis reads: The probability of a country i to be a democracy (y=1) in year t, given that country i was also a democracy (y=1) in the previous year (t-1).\nWe will now apply this knowledge to create a model in R which calculates these conditional probabilities. Let’s start with democratic emergence.\n\n\nDemocratic Emergence\nDemocratic emergence is expressed as the probability of a country to be a democracy in year \\(t\\), given that it was a dictatorship in the previous year, \\(t-1\\)\n\\[\\begin{equation*}\nP(y_{i,t} = 1 | y_{i, t-1} = 0)\n\\end{equation*}\\]\nAs a first step, we therefore need a variable that gives us the information which regime type each of our countries had in the previous year. We will use the world data set for this illustration.\n\nworld &lt;- read.csv(\"world.csv\")\n\nlibrary(tidyverse)\n\nWe can now create the lagged democracy value. In order for R to know when a new country “starts”, we need to group observations by country first, and then lag the variable democracy. We do this with the intuitively called function lag(). We then ungroup the data again, as we have no need of the groups any more.\n\nworld &lt;- world %&gt;%\n  group_by(countrycode) %&gt;%\n  mutate(l.democracy = lag(democracy)) %&gt;%\n  ungroup()\n\nThis creates a new variable l.democracy which gives us the information we were after: the regime type of the country in the previous year. Note that as the first observation for each country cannot be lagged, it creates as many missing values as we have countries in the data set. Perhaps this makes more sense with a visualisation:\n \n\n\n\nSchematic of lagged variable\n\n\n \n\n\nLagging the Independent Variables\nSo far, we have lagged the dependent variable, to run an MTM. This was a methodological necessity. But from a substantive point of view it makes sense to also lag our independent variables. It is reasonable to assume, that the regime type in year \\(t\\) depends on the state of socio-economic development in the previous year, \\(t-1\\). It is rare that for example a recession hits, and the country immediately changes regime type. These things need time. And this is why we will now also lag the independent variables.\nThis is the same procedure as before. For “per capita GDP” we call:\n\nworld &lt;- world %&gt;%\n  group_by(countrycode) %&gt;%\n  mutate(l.gdppc = lag(gdppc)) %&gt;%\n  ungroup()\n\nLet’s do the same for life expectancy:\n\nworld &lt;- world %&gt;%\n  group_by(countrycode) %&gt;%\n  mutate(l.life = lag(life)) %&gt;%\n  ungroup()\n\nand Primary gross enrolment rate:\n\nworld &lt;- world %&gt;%\n  group_by(countrycode) %&gt;%\n  mutate(l.enrol_gross = lag(enrol_gross)) %&gt;%\n  ungroup()\n\n\nRemember when writing the assessment in general, and the methods section in particular, that lagging the independent variables is not a methodological consideration, but a substantive one. It has nothing to do with the Markov Transition Model itself.\n\n\n\nSubsetting the Data for Conditional Probabilities\nRecall that for democratic emergence we need all of those observations in which a country has been an autocracy in the previous year. We can select these in R by filtering the data:\n\nworld_democ0 &lt;- filter(world, l.democracy == 0)\n\nWhilst we are at it, we might as well also do this for democratic survival. Here we need all observations in which l.democracy == 1\n\nworld_democ1 &lt;- filter(world, l.democracy == 1)\n\nAnd with that we are ready!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#democratic-emergence-1",
    "href": "07-PO33Q.html#democratic-emergence-1",
    "title": "Week 7",
    "section": "Democratic Emergence",
    "text": "Democratic Emergence\nEven though “Markov Transition Model” sounds very complicated, the code in R is actually no different from a “regular” probit model. The only difference is that we have selected a certain type of observations. For emergence we use the data frame world_democ0:\n\nemergence &lt;- glm(democracy ~ l.gdppc, \n                 data = world_democ0, \n                 na.action = na.exclude,\n                 family = binomial(link = \"probit\"))\n\nsummary(emergence)\n\n\nCall:\nglm(formula = democracy ~ l.gdppc, family = binomial(link = \"probit\"), \n    data = world_democ0, na.action = na.exclude)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.977e+00  4.668e-02 -42.360   &lt;2e-16 ***\nl.gdppc     -2.413e-05  1.158e-05  -2.084   0.0372 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 950.50  on 4651  degrees of freedom\nResidual deviance: 943.72  on 4650  degrees of freedom\n  (725 observations deleted due to missingness)\nAIC: 947.72\n\nNumber of Fisher Scoring iterations: 8\n\n\nOnce again, we can include multiple independent variables by connecting these with + in the glm() function:\n\nemergence_full &lt;- glm(democracy ~ l.gdppc + l.life + \n                          l.enrol_gross, \n                      data = world_democ0, \n                      na.action = na.exclude,\n                      family = binomial(link = \"probit\"))\n\nsummary(emergence_full)\n\n\nCall:\nglm(formula = democracy ~ l.gdppc + l.life + l.enrol_gross, family = binomial(link = \"probit\"), \n    data = world_democ0, na.action = na.exclude)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -2.717e+00  3.383e-01  -8.030 9.74e-16 ***\nl.gdppc       -5.247e-05  1.991e-05  -2.635  0.00841 ** \nl.life         1.529e-02  6.975e-03   2.192  0.02840 *  \nl.enrol_gross -5.761e-04  2.309e-03  -0.249  0.80300    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 792.47  on 3350  degrees of freedom\nResidual deviance: 777.66  on 3347  degrees of freedom\n  (2026 observations deleted due to missingness)\nAIC: 785.66\n\nNumber of Fisher Scoring iterations: 8",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#democratic-survival",
    "href": "07-PO33Q.html#democratic-survival",
    "title": "Week 7",
    "section": "Democratic Survival",
    "text": "Democratic Survival\nModelling democratic survival is easy now, as we have already completed all necessary preparations. To model\n\\[\\begin{equation*}\nP(y_{i,t} = 1 | y_{i, t-1} = 1)\n\\end{equation*}\\]\nThis time we use world_democ1:\n\nsurvival &lt;- glm(democracy ~ l.gdppc,\n                data = world_democ1, \n                na.action = na.exclude,\n                family = binomial(link = \"probit\"))\n\nsummary(survival)\n\n\nCall:\nglm(formula = democracy ~ l.gdppc, family = binomial(link = \"probit\"), \n    data = world_democ1, na.action = na.exclude)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 1.629e+00  7.591e-02  21.456  &lt; 2e-16 ***\nl.gdppc     1.893e-04  3.608e-05   5.245 1.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.54  on 4588  degrees of freedom\nResidual deviance: 626.75  on 4587  degrees of freedom\n  (162 observations deleted due to missingness)\nAIC: 630.75\n\nNumber of Fisher Scoring iterations: 11\n\n\nAnd again with all independent variables:\n\nsurvival_full &lt;- glm(democracy ~ l.gdppc + l.life + \n                         l.enrol_gross,\n                     data = world_democ1, \n                     na.action = na.exclude,\n                     family = binomial(link = \"probit\"))\n\nsummary(survival_full)\n\n\nCall:\nglm(formula = democracy ~ l.gdppc + l.life + l.enrol_gross, family = binomial(link = \"probit\"), \n    data = world_democ1, na.action = na.exclude)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   8.272e-01  5.861e-01   1.411  0.15814   \nl.gdppc       1.178e-04  3.745e-05   3.144  0.00166 **\nl.life        7.210e-03  1.122e-02   0.642  0.52056   \nl.enrol_gross 4.698e-03  3.437e-03   1.367  0.17167   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 500.81  on 3584  degrees of freedom\nResidual deviance: 432.34  on 3581  degrees of freedom\n  (1166 observations deleted due to missingness)\nAIC: 440.34\n\nNumber of Fisher Scoring iterations: 11",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#interpretation",
    "href": "07-PO33Q.html#interpretation",
    "title": "Week 7",
    "section": "Interpretation",
    "text": "Interpretation\nThe interpretation is analogous to a cross-sectional probit model we explored in Week 3. Just bear in mind that the predicted probabilities are all indicating the probability for democracy to emerge, or for democracy to survive.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#how-to-measure-model-fit",
    "href": "07-PO33Q.html#how-to-measure-model-fit",
    "title": "Week 7",
    "section": "How to Measure Model Fit",
    "text": "How to Measure Model Fit\nThere are quite a few model fit measures around for binary response models, such as pseudo R-Squared and the Akaike Information Criterion (AIC). What I am going to present here, however, is the so-called ROC curve, as I personally really don’t like the aforementioned measures.\n\nWhat is it?\n\nThe ROC Curve\nYouTube link to follow\n\nThe principle idea of ROC is to determine how well our model is able to separate cases into the two categories of our dependent variable. It approaches this question by comparing the actual observed values of the dependent variable with the values the model would predict, given the values of the independent variables.\nConsider Figure 1 a). On the x-axis you see eight observations, marked as coloured circles. Red circles represent countries which are dictatorships, and orange ones democracies. They are sorted by their respective level of per capita GDP. Now suppose that the displayed CDF is the results of a model we have estimated. With a cut off point \\(\\tau=0.5\\) we would correctly predict the group of four observations on the left to be dictatorships. They are True Negatives (TN). The group on the right would be correctly predicted as democracies, these are True Positives (TP). We have no incorrectly classified cases; our model has been able to separate cases perfectly. You can see this represented in the form of distributions in panel b).\n   \n\n\n\n\n\n\nFigure 1: Perfect Separation\n\n\n\n   \nAs you well know by now, the real world is oddly deficient in achieving perfection such as this. We will observe both poor democracies, and rich dictatorships. This scenario is shown in Figure 2 a).\n   \n\n\n\n\n\n\nFigure 2: Overlap\n\n\n\nAccording to the CDF we would predict the poor democracy as a dictatorship. It would be a False Negative (FN). Conversely, we would predict the rich dictatorship as a democracy and would obtain a False Positive (FP). The distribution of cases in Figure 2 b) is not as clearly separated any more as in Figure 1 b). Now they overlap, leading to incorrect classifications. These are marked accordingly in Figure 3. As we are no longer operating in a world in which we only have TNs and TPs, I think we can all agree that our model fit is no longer as good as in Figure 1.\n\n\n\n\n\n\nFigure 3: False Negatives and False Positives\n\n\n\nBut there is another issue: whilst setting \\(\\tau\\) at 0.5 makes intuitive sense, there is nothing preventing us from shifting \\(\\tau\\) around. Indeed, the number of FPs and FNs very much depends on where we place our cut-off point. For example, if we don’t want any FNs, then we just have to shift \\(\\tau\\) sufficiently downwards. Or if we want to avoid FPs we only need to move it far enough upwards. I have illustrated this in Figure 4 a) and b), respectively.\n\n\n\n\n\n\nFigure 4: Shifting \\(\\tau\\)\n\n\n\n \n\n\n\n\n\n\n\n\n\nActual\n\n\n\n\nPredicted\n\n\nDemocracy\n\n\nTruePositive\n\n\nFalsePositive\n\n\n\n\n\n\nDictatorship\n\n\nFalseNegative\n\n\nTrueNegative\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{equation}\n\\text{False Positive Rate}=\\text{Sensitivity}=\\frac{\\text{False Positives}}{\\text{False Positives}+\\text{True Negatives}}\n\\end{equation}\\]\n\\[\\begin{align}\n\\text{True Positive Rate}&=1-\\text{Specificity}=\\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}\\\\[10pt]\n\\text{Specificity} &=1-\\text{True Positive Rate}=1-\\frac{\\text{TP}}{\\text{TP}+\\text{FN}} \\nonumber\n\\end{align}\\]\n \nFor our example the True Positive Rate (TPR) represents the proportion of correctly specified democracies. If we calculate these rates for each of our n confusion matrices, we are already reducing four quantities into two. Let’s do it. To provide a visual aid in this rather laborious process, I have created Figure 5 which depicts all eight shifts.\n   \n\n\n\n\n\n\nFigure 5: Towards the ROC Curve\n\n\n\n   \nTable 1 displays the confusion matrix for each of the panels in Figure 5, as well as the respective TPR and FPR (you are welcome).\n \n\n\n\n\n\n\n\n\n\n\na)\n\n\nActual\n\n\nb)\n\n\nActual\n\n\n\n\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nDem\n\n\nDic\n\n\n\n\n\n\nPred.\n\n\nDem\n\n\nTP=4\n\n\nFP=4\n\n\nDem\n\n\nTP=4\n\n\nFP=3\n\n\n\n\nDic\n\n\nFN=0\n\n\nTN=0\n\n\nDic\n\n\nFN=0\n\n\nTN=1\n\n\n\n\n\n\n\n\nTPR=1\n\n\nFPR=1\n\n\n\n\nTPR=1\n\n\nFPR=0.75\n\n\n\n\n\n\n\n\n\n\nc)\n\n\nActual\n\n\nd)\n\n\nActual\n\n\n\n\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nPred.\n\n\nDem\n\n\nTP=4\n\n\nFP=2\n\n\nDem\n\n\nTP=3\n\n\nFP=2\n\n\n\n\nDic\n\n\nFN=0\n\n\nTN=2\n\n\nDic\n\n\nFN=1\n\n\nTN=2\n\n\n\n\n\n\n\n\nTPR=1\n\n\nFPR=0.5\n\n\n\n\nTPR=0.75\n\n\nFPR=0.5\n\n\n\n\n\n\n\n\n\n\ne)\n\n\nActual\n\n\nf)\n\n\nActual\n\n\n\n\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nPred.\n\n\nDem\n\n\nTP=3\n\n\nFP=1\n\n\nDem\n\n\nTP=3\n\n\nFP=0\n\n\n\n\nDic\n\n\nFN=1\n\n\nTN=3\n\n\nDic\n\n\nFN=1\n\n\nTN=4\n\n\n\n\n\n\n\n\nTPR=0.75\n\n\nFPR=0.25\n\n\n\n\nTPR=0.75\n\n\nFPR=0\n\n\n\n\n\n\n\n\n\n\ng)\n\n\nActual\n\n\nh)\n\n\nActual\n\n\n\n\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nDem\n\n\nDic\n\n\n\n\nPred.\n\n\nDem\n\n\nTP=2\n\n\nFP=0\n\n\nDem\n\n\nTP=1\n\n\nFP=0\n\n\n\n\nDic\n\n\nFN=2\n\n\nTN=4\n\n\nDic\n\n\nFN=3\n\n\nTN=4\n\n\n\n\n\n\n\n\nTPR=0.5\n\n\nFPR=0\n\n\n\n\nTPR=0.25\n\n\nFPR=0\n\n\n\n\n\n\n\n\n\n\ni)\n\n\nActual\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDem\n\n\nDic\n\n\n\n\n\n\n\n\n\n\nPred.\n\n\nDem\n\n\nTP=0\n\n\nFP=0\n\n\n\n\n\n\n\n\n\n\nDic\n\n\nFN=4\n\n\nTN=4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTPR=0\n\n\nFPR=0\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Confusion Matrices for ROC CUrve\n\n\n\n \nAs we only need the TPRs and FPRs going forward, it makes sense to collect these in their own little table:\n \n\n\n\n\n\n\n\n\npanel\n\n\nTPR\n\n\nFPR\n\n\n\n\n\n\na\n\n\n1.00\n\n\n1.00\n\n\n\n\nb\n\n\n1.00\n\n\n0.75\n\n\n\n\nc\n\n\n1.00\n\n\n0.50\n\n\n\n\nd\n\n\n0.75\n\n\n0.50\n\n\n\n\ne\n\n\n0.75\n\n\n0.25\n\n\n\n\nf\n\n\n0.75\n\n\n0.00\n\n\n\n\ng\n\n\n0.50\n\n\n0.00\n\n\n\n\nh\n\n\n0.25\n\n\n0.00\n\n\n\n\ni\n\n\n0.00\n\n\n0.00\n\n\n\n\n\n\n\nTable 2: TPR and FPR Rates\n\n\n\n \nWe are nearly there! The last step is to display all these values in the form of a curve with the TPR on the y-axis, and the FPR on the x-axis. You can see the result – the ROC curve – in Figure 6 a).\n \n\n\n\n\n\n\nFigure 6: ROC Curve\n\n\n\nNote that I have added a diagonal where TPR = FPR. This is sometimes described as a model without independent variables. I like to think of the line as the graphical point where our model would not be able to separate between the two categories, at all. The further the ROC curve is away from the diagonal, the better our model is at separating the two categories. But there are two sides to the diagonal. We want it to be above the diagonal, as here the model is predicting 0s as 0s and 1s as 1s. Underneath, the prediction is inverse and 0s are predicted as 1s, and 1s are predicted as 0s.\nTo summarize the position of the curve into a numerical expression, the Area Under Curve (AUC) is used, as shown in Figure 6 b). If the area is 100% we are correctly predicting everything. At 50% the model is incapable of separation, and at 0% the model gets everything wrong. This is very useful to compare different models.\n\n\nR\nHow do you do all of this in R? Let’s start with the simple emergence model. In order to calculate a ROC curve, we need a new package, called pROC. Install it and load it.\n\nlibrary(pROC)\n\nTo calculate the ROC curve, we need a few steps:\n\nprob_em &lt;- predict(emergence, type = \"response\")\n\nworld_democ0$prob_em &lt;- unlist(prob_em)\n\nroc1 &lt;- roc(world_democ0$democracy, world_democ0$prob_em)\n\nauc(roc1)\n\nArea under the curve: 0.5227\n\n\nWhat does each line do?\n\npredict the probability of democratic emergence for each observation (country year) and store them in a new vector called prob_em.\nadd the vector prob_em to the data frame world_democ0. In order to do this, we need to unlist the values in the prob_em vector.\nwe then call the roc function in which we compare the predicted probabilities (world_democ0$prob_em) with the actual, observed regime type (world_democ0$democracy) and store the result in an object called roc\nas a last step we calculate the area under the curve with the auc() function\n\nBut we can also plot the ROC curve:\n\nplot(roc1, print.auc = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nNote that print.auc = TRUE prints the numerical value into this plot. You can suppress it by setting it to FALSE.\n\nLet’s follow this procedure for the other models.\n\n\nEmergence: Full Model\n\nprob_em_full &lt;- predict(emergence_full, type = \"response\")\nworld_democ0$prob_em_full &lt;- unlist(prob_em_full)\n\nroc2 &lt;- roc(world_democ0$democracy, world_democ0$prob_em_full)\n\nauc(roc2)\n\nArea under the curve: 0.5982\n\n\n\nplot(roc2, print.auc = TRUE) \n\n   \n\n\n\n\n\n\n\n\n\n   \n\n\nSurvival\n\nprob_sur &lt;- predict(survival, type = \"response\")\nworld_democ1$prob_sur &lt;- unlist(prob_sur)\n\nroc3 &lt;- roc(world_democ1$democracy, world_democ1$prob_sur)\n\nauc(roc3)\n\nArea under the curve: 0.8341\n\n\n   \n\n\n\n\n\n\n\n\n\n   \n\n\nSurvival: Full Model\n\nprob_sur_full &lt;- predict(survival_full, type = \"response\")\nworld_democ1$prob_sur_full &lt;- unlist(prob_sur_full)\n\nroc4 &lt;- roc(world_democ1$democracy, world_democ1$prob_sur_full)\n\nauc(roc4)\n\nArea under the curve: 0.8215",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#including-roc-in-modelsummary",
    "href": "07-PO33Q.html#including-roc-in-modelsummary",
    "title": "Week 7",
    "section": "Including ROC in modelsummary",
    "text": "Including ROC in modelsummary\nWhen I introduced you to modelsummary in Week 3, I mentioned that we will be adding a model fit measure to the results table in Week 7. This model fit measure is the AUC. A properly formatted table summarising the results of all four models we have calculated would look like this:\n \n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:Democracy\n\n\n \nEmergence\nSurvival\n\n        \n              \n                 \n                Classical\n                New\n                Classical \n                New \n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  per capita GDP (lagged)\n                  -0.000*\n                  -0.000**\n                  0.000***\n                  0.000**\n                \n                \n                  \n                  (0.000)\n                  (0.000)\n                  (0.000)\n                  (0.000)\n                \n                \n                  Life Expectancy (lagged)\n                  \n                  0.015*\n                  \n                  0.007\n                \n                \n                  \n                  \n                  (0.007)\n                  \n                  (0.011)\n                \n                \n                  Gross Primary Enrollment (lagged)\n                  \n                  -0.001\n                  \n                  0.005\n                \n                \n                  \n                  \n                  (0.002)\n                  \n                  (0.003)\n                \n                \n                  Intercept\n                  -1.977***\n                  -2.717***\n                  1.629***\n                  0.827\n                \n                \n                  \n                  (0.047)\n                  (0.338)\n                  (0.076)\n                  (0.586)\n                \n                \n                  ROC Curve\n                  0.523\n                  0.598\n                  0.834\n                  0.821\n                \n                \n                  Num.Obs.\n                  4652\n                  3351\n                  4589\n                  3585\n                \n        \n      \n    \n\n\n\n\nTable 3: Democratic Emergence and Survival\n\n\n\n\n \nHow did I get this table? We start again by loading the packages modelsummary and tinytable.\n\nlibrary(modelsummary)\nlibrary(tinytable)\n\nThen we store our models in a list\n\nmodels &lt;- list(\n  \"Classical\"    = emergence,\n  \"New\"          = emergence_full,\n  \"Classical\"    = survival,\n  \"New\"          = survival_full\n)\n\nand create the coefficient map.\n\ncm &lt;- c('l.gdppc'       = 'per capita GDP (lagged)',\n        'l.life'        = 'Life Expectancy (lagged)',\n        'l.enrol_gross' = 'Gross Primary Enrollment (lagged)',\n        '(Intercept)'   = 'Intercept')\n\n\nNote that the variable names need to match exactly those in the model.\n\nTo get the ROC information into the table, we need to create a custom line. For this, we create a mini data frame whose structure corresponds to that of the table. Our table has five columns, and so we need to specify information for all five columns of our custom line:\n\nrows &lt;- tibble(\n  '~term' = 'ROC Curve',\n  `~(1)`  = auc(roc1),\n  `~(2)`  = auc(roc2),\n  `~(3)`  = auc(roc3),\n  `~(4)`  = auc(roc4)\n)\n\nmodelsummary is very flexible when it comes to designing the layout of the final table, and we can place the custom row into the exact location we want. Here, it is the ninth row:\n\nattr(rows, 'position') &lt;- c(9)\n\n\nIf you get a row on NAs at the bottom of the finished table, then you have counted too many rows.\n\nIn a last step, we include the custom row in the actual modelsummary code. Note that I have added a header at the top which groups the emergence and survival models.\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'AIC|BIC|Log.Lik|F|RMSE',\n             add_rows = rows) |&gt;                                 # this is the custom row\n  group_tt(j = list(\"Emergence\" = 2:3, \"Survival\" = 4:5)) |&gt;     # this creates the header\n  group_tt(j = list(\"Dependent Variable: Democracy\" = 2:5))      # this creates the top line\n\n\n\n\nFull code you can adapt for the assessment\n\n\n# load packages\nlibrary(modelsummary)\nlibrary(tinytable)\n\n\n# store models in a list\nmodels &lt;- list(\n  \"Classical\"    = emergence,\n  \"New\"    = emergence_full,\n  \"Classical\"    = survival,\n  \"New\"    = survival_full\n)\n\n# write the coefficient map\ncm &lt;- c('l.gdppc'    = 'per capita GDP (lagged)',\n        'l.life'    = 'Life Expectancy (lagged)',\n        'l.enrol_gross'    = 'Gross Primary Enrollment (lagged)',\n        '(Intercept)' = 'Intercept')\n\n# create a mini dataset with the information on the ROC curves\nrows &lt;- tibble(\n  '~term' = 'ROC Curve',\n  `~(1)` = auc(roc1),\n  `~(2)` = auc(roc2),\n  `~(3)` = auc(roc3),\n  `~(4)` = auc(roc4)\n)\n\n# place the custom row into position in the final table\nattr(rows, 'position') &lt;- c(9)\n\n# modelsummary code\nmodelsummary(models,\n             title = \"Democratic Emergence and Survival\",\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'AIC|BIC|Log.Lik|F|RMSE',\n             escape = FALSE,                                    # needed for spacer\n             notes = \"\\\\vspace{0.2\\\\baselineskip}\",             # add space before caption\n             notes_append = TRUE)|&gt;                             # this is the custom row\n  group_tt(j = list(\"Emergence\" = 2:3, \"Survival\" = 4:5))|&gt;     # this creates the groups\n  group_tt(j = list(\"Dependent Variable: Democracy\" = 2:5))|&gt;   # this creates the header\n  theme_latex(outer = \"label={tblr:regmodels}\")|&gt;               # add label for crossref\n  theme_latex(placement= \"H\")|&gt;                                 # place the table [H]ere\n  theme_latex(resize_width= 0.5, resize_direction=\"both\")   # adjust table size (% of textwidth)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#rationale",
    "href": "07-PO33Q.html#rationale",
    "title": "Week 7",
    "section": "Rationale",
    "text": "Rationale\nIt is perfectly adequate to estimate the processes of democratic emergence and survival separately. There is a more elegant approach, however, in which emergence and survival are estimated together. This is often employed in the literature, for example in the article by Boix & Stokes (2003). The rationale here is that the model will, overall, use more observations and therefore this might influence standard errors and thus statistical significance.\n\nAs you will see, the joint estimation requires a lot of manual calculation and general faff. I am mainly showing you this here, so that you can understand the output in aforementioned articles. If you want to employ this yourself, then you could:\n\nestimate the joint model as outlined here to check statistical significance of variables\nthen estimate emergence and survival separately and work with those separately estimated models to calculate predicted probabilities and ROC curves\n\n\nSo, what’s the setup? Again, we are incorporating the lagged value \\(y_{t-1}\\), to encapsulate all the history prior to period \\(t\\). There is just one little trick I need to introduce before we can start on the model: I will replace \\(y_{t-1}\\) by an indicator variable \\(I_{D}\\) in Equation 1 which assumes the value 1 if a country was a democracy in the previous year, and zero otherwise (notation adapted from Epstein et al., 2006, p. 553). Following on from this, we can write the model as follows2:\n\\[\nP(D_{i,t}) = \\Phi(\\beta_{0}+\\beta_{1} X_{i,t} + \\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t} + \\epsilon_{i,t})\n\\qquad(1)\\]\nwhere \\(P(D_{it})\\) is the probability that a country \\(i\\) was a democracy in year \\(t\\), \\(\\Phi\\) is the cumulative normal distribution (the s-shaped distribution which you know from the introduction of probit in week 3), \\(I_{D}\\) the aforementioned indicator variable, \\(X_{i,t}\\) is an independent variable for country \\(i\\) in year \\(t\\), and \\(\\epsilon_{i,t}\\) is a zero mean stochastic disturbance.\n\nPlease note that I am deliberately not lagging the independent variables (IVs) in the following discussion to keep notation as simple as possible. If you decide to run the interaction model, make sure you lag the IVs as explained above. I will also drop \\(\\epsilon_{i,t}\\) in what is to follow, as this is irrelevant for us here and now and I don’t want to confuse you unncessarily.\n\nThis model is a multiplicative interaction model which allow us to model the probability of a country to be a democracy, conditional on its regime type in the previous year. The indicator variable \\(I_{D}\\), equal to \\(y_{t-1}\\) captures this information. How does this work?\nLet us start by assuming that in year \\(t-1\\) country \\(i\\) was an autocracy. In this case, the indicator variable \\(I_{D}\\) would be equal to zero, and therefore Equation 1 can be re-written as\n\\[\\begin{equation}\nP(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t})\n\\end{equation}\\]\nIn this case, the coefficient \\(\\beta_{1}\\) would only represent the impact of variable \\(X_{i,t}\\) on the probability of an autocracy transitioning to democracy. Expressed more formally, we are dealing with conditional probabilities here, in the case of \\(\\beta_{1}\\), we would obtain the impact of variable \\(X_{i,t}\\) on a country to be a democracy in year \\(t\\), under the condition that it was an autocracy in year \\(t-1\\). It is conditional on this, because we set the indicator variable to zero before.\nWe can construct a similar scenario for the condition, that a country was a democracy in the previous year. In this case, the indicator variable \\(I_{D}\\) would be equal to \\(1\\), and we would obtain Equation 1 again:\n\\[\\begin{equation*}\nP(D_{i,t})=\\Phi(\\beta_{0}+\\beta_{1} X_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} X_{i,t})\n\\end{equation*}\\]\nWith \\(I_{D}=1\\), this can be simplified to:\n\\[\\begin{equation}\nP(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) X_{i,t})\n\\end{equation}\\]\nThis equation illustrates very well, that the impact of variable \\(X_{i,t}\\) on the probability of democracy to remain a democracy is now made up by the sum of the two coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\), whereas the constant is the sum of coefficients \\(\\beta_{0}\\) and \\(\\beta_{2}\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#example",
    "href": "07-PO33Q.html#example",
    "title": "Week 7",
    "section": "Example",
    "text": "Example\nTo make this more tangible, let’s look at an example. Assume we want to look at the effect of per capita GDP on democratic emergence (i.e. the transition of an autocracy to democracy), and democratic survival (i.e. the transition from democracy to democracy). In this case we would specify the model as follows:\n\\[\\begin{equation}\nP(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t})\n(\\#eq:interaction)\n\\end{equation}\\]\nThe coefficient indicating the impact of per capita GDP on democratic emergence is \\(\\beta_{1}\\). As illustrated above, the indicator variable \\(I_{D}\\) is zero in this case, and the equation would be reduced to\n\\[\\begin{equation}\nP(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t})\n\\end{equation}\\]\nFor democratic survival, the coefficient indicating the impact of per capita GDP would be the sum of coefficients \\(\\beta_{1}\\) and \\(\\beta_{3}\\)\n\\[\\begin{equation}\nP(D_{it})=\\Phi((\\beta_{0} +\\beta_{2}) + (\\beta_{1} + \\beta_{3}) \\text{per capita GDP}_{i,t})\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#application-in-r",
    "href": "07-PO33Q.html#application-in-r",
    "title": "Week 7",
    "section": "Application in R",
    "text": "Application in R\nHow does all of this look in R, and how do you apply this to a real-world scenario? Let’s do this using the above example of per capita GDP in the global data set. You have already created the indicator variable \\(I_{D}\\) in the form of the variable l.democracy. What we need next is a variable that captures \\(I_{D} \\text{per capita GDP}_{i,t}\\), so that we can calculate \\(\\beta_{3}\\). To create this variable, type\n\nworld$gdppc_l.democracy &lt;- world$l.democracy * world$gdppc\n\nNow, we are ready to estimate the model. Remember, formally, this is written as:\n\\[P(D_{it})=\\Phi(\\beta_{0}+\\beta_{1} \\text{per capita GDP}_{i,t} +\\beta_{2} I_{D} + \\beta_{3} I_{D} \\text{per capita GDP}_{i,t})\\]\nWe replace this in the R command with the equivalent variables:\n\njoint &lt;- glm(democracy ~ gdppc + l.democracy + gdppc_l.democracy, \n             data = world,\n             na.action = na.exclude,\n             family = binomial(link = \"probit\"))\n\n\nIf you have done regression analysis before, and you worry about (multi-)collinearity in such a model, then please note that:\n\nAnalysts should include all constitutive terms when specifying multiplicative interaction models except in very rare circumstances. By constitutive terms, we mean each of the elements that constitute the interaction term. Thus, X and Z are the constitutive terms in [this model: \\(y=\\beta_0 + \\beta_1 X + \\beta_2 Z + \\beta_4 XZ + \\epsilon\\)]. (Brambor et al., 2006, p. 66)\n\n\nYou should obtain the following output:\n\nsummary(joint)\n\n\nCall:\nglm(formula = democracy ~ gdppc + l.democracy + gdppc_l.democracy, \n    family = binomial(link = \"probit\"), data = world, na.action = na.exclude)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.981e+00  4.666e-02 -42.454  &lt; 2e-16 ***\ngdppc             -2.268e-05  1.103e-05  -2.056   0.0398 *  \nl.democracy        3.594e+00  8.959e-02  40.117  &lt; 2e-16 ***\ngdppc_l.democracy  2.123e-04  3.727e-05   5.697 1.22e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 12862  on 9277  degrees of freedom\nResidual deviance:  1569  on 9274  degrees of freedom\n  (1150 observations deleted due to missingness)\nAIC: 1577\n\nNumber of Fisher Scoring iterations: 11\n\n\nFor democratic emergence, we can report the Intercept (Intercept) and the slope coefficient gdppc straight away as \\(-1.981\\) and \\(-0.0000227\\), respectively. The slope coefficient is significant, as the p-value is \\(&lt;0.05\\). This is in line with the findings from estimating emergence separately. For democratic survival, we need to take the sum of (Intercept) and l.democracy to obtain the intercept, and gdppc and gdppc_l.democracy to obtain the slope coefficient. This calculation will yield the same coefficients as the two-stepped analysis.\nThe last step is the assessment of statistical significance. For the emergence model, we can once again interpret the output straight away. For the survival scenario, however, we need to test the hypothesis that for example \\(\\beta_{0}\\) and \\(\\beta_{3}\\) are jointly different from zero. As the survival effect is a joint-venture between these two coefficients, we also need to assess their significance jointly, and not just concentrate on \\(\\beta_{3}\\). This is the mistake Przeworski et al. (2000) have made in their seminal book, and this is what is discussed on the first few pages of the article by Epstein et al. (2006). To do this, we are using a post-estimation command which is testing the aforementioned hypothesis that \\(\\beta_{0}\\) and \\(\\beta_{3}\\) are jointly different from zero, called a Wald-Test.\nFor this we need to install and load a new package, called survey. This is a regression term test, where you first need to state the object within which the results are stored (here: joint), and the two terms you want to test, preceded by a tilde and connected by a plus. Lastly we specify that we want the Wald Test.\n\nlibrary(survey)\nregTermTest(joint, ~gdppc + gdppc_l.democracy, method = \"Wald\")\n\nWald test for gdppc gdppc_l.democracy\n in glm(formula = democracy ~ gdppc + l.democracy + gdppc_l.democracy, \n    family = binomial(link = \"probit\"), data = world, na.action = na.exclude)\nF =  16.30128  on  2  and  9274  df: p= 8.5676e-08 \n\n\nWith \\(8.5676e-08\\) we can reject the null hypothesis, and conclude that jointly, the slope coefficients are different from zero, and as such that per capita GDP explains democratic survival.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#literature-recommendations",
    "href": "07-PO33Q.html#literature-recommendations",
    "title": "Week 7",
    "section": "Literature Recommendations",
    "text": "Literature Recommendations\n\nBeck et al. (1998) “Time-Series-Cross-Section Analysis with a Binary Dependent Variable” – the title says it all\nBeck et al. (2002) This is more specifically for state failure\nBrambor et al. (2006): This is for the Joint Estimation of Emergence and Survival",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#the-data-set",
    "href": "07-PO33Q.html#the-data-set",
    "title": "Week 7",
    "section": "The Data Set",
    "text": "The Data Set\n Use the data set called prz.dta which is available in the Downloads Section. This is the data set used in the book “Democracy and Development” (Przeworski et al., 2000). Deposit this in an appropriate working directory and import the data set into a data frame called prz. You will need the haven package for this, as it is a Stata data set. We will only be looking at a few variables3:\n \n\n\n\n\n\n\n\n\nVariable\n\n\nLabel\n\n\n\n\n\n\ndemoc\n\n\n1 if democracy, 0 otherwise\n\n\n\n\ngdpw\n\n\nGDP per worker\n\n\n\n\ng\n\n\nGrowth rate\n\n\n\n\noil\n\n\n1 if oil producer, 0 otherwise\n\n\n\n\n\n\n\nTable 4: Codebook for Przeworski et al. (2000) Data Set",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#basics",
    "href": "07-PO33Q.html#basics",
    "title": "Week 7",
    "section": "Basics",
    "text": "Basics\n\nIn these exercises, I am taking you step-by-step to the final Markov Transition Model. Only the code for Exercises 3 and 4 correspond to what you will be doing in the final assessment.\n\n\nRun a probit model where democ is the dependent variable and g, gdpw and oil are the independent variables. Put the results in column 1 of Table 5. What is (possibly) wrong with this approach? Interpret the coefficients on one or two of the variables.\nRun the same probit model as before but now include a lagged dependent variable. To create the lagged dependent variable, call:\n\n\nprz &lt;- prz %&gt;%\ngroup_by(country) %&gt;%\nmutate(l.democ = lag(democ)) %&gt;%\nungroup()\n\nPut the results in column two of Table 5. What are we assuming by including a lagged dependent variable? Do you think that this is appropriate here?\n\nNow estimate a probit “transition to democracy” model i.e. how do growth, wealth and oil affect the probability that a country is a democracy this year given that it was a dictatorship last year. We are also lagging the independent variables by one year. Put the results in column 3 of Table 5. Interpret the sign of the coefficients on each independent variable.\nNow estimate a probit “survival of democracy” model i.e. how do (lagged) growth, wealth and oil affect the probability that a country is a democracy this year given that it was a democracy last year. Put the results in column 4 of Table 5. Interpret the sign of the coefficients on each independent variable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#advanced",
    "href": "07-PO33Q.html#advanced",
    "title": "Week 7",
    "section": "Advanced",
    "text": "Advanced\n This section draws on the instructions for joint estimation.\n\nNow interact all the lagged independent variables with the lagged dependent variable. Estimate a fully interactive model and include all the constitutive terms. Put the results in column 5 of Table 5. What is the relationship between these coefficients and those in the previous two columns? Is there any extra information provided by this full interaction model that was not available from the previous two models?\nNow consider the straight probit model, the probit model with the lagged dependent variable, and the full interaction model. Produce the ROC curve for each of these models. Interpret a point on one of these curves. What do the ROC curves tell you about the fit of these three models?\n\nYou can find the populated version of Table 5 and the corresponding RScript in the Downloads Section.\n \n\n\n\n\n\n\n\n\n\n\nProbit (static)\n\n\nProbit (lagged)\n\n\nEmergence\n\n\nSurvival\n\n\nFull Interaction\n\n\n\n\n\n\nGrowth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWealth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOil\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nL.Democracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGrowth*L.Democracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWealth*L.Democracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOil*L.Democracy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5: Regression Results for Lagged, Emergence, Survival, and Interaction Models\n\n\n\n \n\n\n\nmodelsummary code for the Results Table\n\nIf you haven’t had a serious stab at the exercises yourself, please close this window now.\n\n# Set Working Directory\n########################\n\nsetwd()\n\n\n# PACKAGES\n############\n\nlibrary(haven)\nlibrary(tidyverse)\nlibrary(modelsummary)\n\n\n# Import Data\n#############\n\nprz &lt;- read_dta(\"prz.dta\")\n\n\n\n# Exercise 1\n#####################\n\n\ndemocracy &lt;- glm(democ ~ g + gdpw + oil, \n                 data = prz,\n                 na.action = na.exclude,\n                 family = binomial(link = \"probit\"))\n\n\n# Exercise 2\n#####################\n\n\nprz &lt;- prz %&gt;%\n  group_by(country) %&gt;%\n  mutate(l.democ = lag(democ)) %&gt;%\n  ungroup()\n\n\n\ndynamic &lt;- glm(democ ~ g + gdpw + oil + l.democ, \n               data = prz,\n               na.action = na.exclude,\n               family = binomial(link = \"probit\")) \n\n\n# Exercise 3\n#####################\n\nprz &lt;- prz %&gt;%\n  group_by(country) %&gt;%\n  mutate(l.g = lag(g)) %&gt;%\n  ungroup()\n\nprz &lt;- prz %&gt;%\n  group_by(country) %&gt;%\n  mutate(l.gdpw = lag(gdpw)) %&gt;%\n  ungroup()\n\nprz &lt;- prz %&gt;%\n  group_by(country) %&gt;%\n  mutate(l.oil = lag(oil)) %&gt;%\n  ungroup()\n\nprz_democ0 &lt;- filter(prz, l.democ==0) \n\nemergence &lt;- glm(democ ~ l.g + l.gdpw + l.oil, \n                 data = prz_democ0,\n                 na.action = na.exclude,\n                 family = binomial(link = \"probit\"))\n\n\n# Exercise 4\n#####################\n\nprz_democ1 &lt;- filter(prz, l.democ==1)\n\nsurvive &lt;- glm(democ ~ l.g + l.gdpw + l.oil, \n               data = prz_democ1,\n               na.action = na.exclude,\n               family = binomial(link = \"probit\"))\n\n\n# Exercise 5\n#####################\n\nprz$l.democgdpw &lt;- prz$l.democ * prz$l.gdpw\nprz$l.democg &lt;- prz$l.democ * prz$l.g\nprz$l.democoil &lt;- prz$l.democ * prz$l.oil\n\n\njoint &lt;- glm(democ ~ l.g + l.gdpw + l.oil + l.democ + l.democg + l.democgdpw + l.democoil, \n             data = prz,\n             na.action = na.exclude,\n             family = binomial(link = \"probit\"))\n\n# Exercise 6\n#####################\n\nlibrary(pROC)\n\n# Democracy model: \n\nprob_democracy &lt;- predict(democracy, type=\"response\")\nprz$prob_democracy &lt;- unlist(prob_democracy)\n\nroc_democracy &lt;- roc(prz$democ, prz$prob_democracy)\n\n\n# Dynamic Model\n\nprob_dynamic &lt;- predict(dynamic, type=\"response\")\nprz$prob_dynamic &lt;- unlist(prob_dynamic)\n\nroc_dynamic &lt;- roc(prz$democ, prz$prob_dynamic)\n\n# emergence model\nprob_emergence &lt;- predict(emergence, type=\"response\")\nprz_democ0$prob_emergence &lt;- unlist(prob_emergence)\n\nroc_emergence &lt;- roc(prz_democ0$democ, prz_democ0$prob_emergence)\n\n\n# survive model\n\nprob_survive &lt;- predict(survive, type=\"response\")\nprz_democ1$prob_survive &lt;- unlist(prob_survive)\n\nroc_survive &lt;- roc(prz_democ1$democ, prz_democ1$prob_survive)\n\n\n\n# Joint Model\n\n\nprob_joint &lt;- predict(joint, type=\"response\")\nprz$prob_joint &lt;- unlist(prob_joint)\n\nroc_joint &lt;- roc(prz$democ, prz$prob_joint)\n\n\n\n#####################\n# Modelsummary\n#####################\n\n# store models in a list\nmodels &lt;- list(\n  \"Probit (static)\"    = static,\n  \"Probit (lagged)\"    = dynamic,\n  \"Emergence\"    = emergence,\n  \"Survival\"    = survive,\n  \"Full Interaction\"    = joint\n)\n\n\n# write the coefficient map\ncm &lt;- c('g'    = 'Growth',\n        'gdpw'    = 'per capita GDP',\n        'oil'    = 'Oil Exporter (Yes)',\n        'l.g'    = 'Growth (lagged)',\n        'l.gdpw'    = 'per capita GDP (lagged)',\n        'l.oil'    = 'Oil Exports (lagged)',\n        'l.democ'    = 'Democracy (lagged)',       \n        'l.democg'    = 'Democracy x Growth (lagged)',  \n        'l.democgdpw'    = 'Democracy x per capita GDP (lagged)',  \n        'l.democoil'    = 'Democracy x Oil Exporter (lagged)',  \n        '(Intercept)' = 'Intercept')\n\n# create a mini dataset with the information on the ROC curves\nrows &lt;- tibble(\n  '~term' = 'ROC Curve',\n  `~(1)` = auc(roc_static),\n  `~(2)` = auc(roc_dynamic),\n  `~(3)` = auc(roc_emergence),\n  `~(4)` = auc(roc_survive),\n  `~(5)` = auc(roc_joint)\n)\n\n# place the custom row into position in the final table\nattr(rows, 'position') &lt;- c(23)\n\n# modelsummary code\nmodelsummary(models,\n             title = 'Regression Models, data are taken from \\\\citet{prz:2000}',\n             escape = FALSE,\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'AIC|BIC|Log.Lik|F|RMSE',\n             notes = \"\\\\vspace{0.3\\\\baselineskip}\",  # &lt;- spacer before caption\n             notes_append = TRUE,                    # keep default notes + add spacer\n             add_rows = rows)|&gt;                      # this is the custom row\n  group_tt(j = list(\"Dependent Variable: Democracy\" = 2:6))|&gt;\n  theme_latex(resize_width= 1.0, resize_direction=\"both\")|&gt; \n  theme_latex(outer = \"label={tblr:test}\")|&gt; \n  theme_latex(placement= \"H\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO33Q.html#footnotes",
    "href": "07-PO33Q.html#footnotes",
    "title": "Week 7",
    "section": "",
    "text": "This is a verbatim reproduction from Reiche (forthcoming). The text is based on an explanatory video by StatQuest with Josh Starmer. All figures are copyrighted.↩︎\nThis discussion draws on Brambor et al. (2006).↩︎\nI have taken these exercises from some material written by Matt Golder from whom I learned all this many moons ago.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "08-PO33Q.html",
    "href": "08-PO33Q.html",
    "title": "Week 8",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO33Q.html#learning-outcomes",
    "href": "08-PO33Q.html#learning-outcomes",
    "title": "Week 8",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nExplain the difference between exogenous and endogenous democratisation\nEvaluate empirical evidence for exogenous and endogenous democratisation\nApply the stages of a research project to the summative assessment of the module",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "09-PO33Q.html",
    "href": "09-PO33Q.html",
    "title": "Week 9",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO33Q.html#learning-outcomes",
    "href": "09-PO33Q.html#learning-outcomes",
    "title": "Week 9",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nDifferentiate between types of autocratic regimes\nDistinguish strategies for autocratic survival\nEvaluate the role of political regimes in people’s well-being\nEstimate transitional probabilities\nPresent statistical results in a professional manner\nUnderstand the assessment guidelines for the module",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "10-PO33Q.html",
    "href": "10-PO33Q.html",
    "title": "Week 10",
    "section": "",
    "text": "Road Map for Today",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO33Q.html#learning-outcomes",
    "href": "10-PO33Q.html#learning-outcomes",
    "title": "Week 10",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter this week, students should be able to:\n\nUnderstand and evaluate commonly discussed factors which inhibit the development-democracy link proposed by modernisation theory in different regions of the world",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "11-Quarto.html",
    "href": "11-Quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "Professional Writing in Political Science\nBefore I even start this section, please read Professional Writing in Political Science: A Highly opinionated Essay by James A. Stimson. It captures a lot of the things I talk about in the seminars in a much better and more eloquent way. Admittedly, the article is aimed at postgraduates and faculty members, but the principles outlined do apply generally to writing a good research project. And this is, after all, what I am asking you to do on this module.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-Quarto.html#rendering",
    "href": "11-Quarto.html#rendering",
    "title": "Quarto",
    "section": "Rendering",
    "text": "Rendering\nTo convert the qmd file to a pdf, you will have to “render” the qmd file. An icon with an arrow is located in the task bar at the top whenever you have an qmd file open. Click it to start the conversion.\n\nWhen there is a mistake in your qmd file, such as an unbalanced bracket in a code chunk, for example, R will not render the pdf. Therefore, knit your document regularly, so that you can trace the error more easily.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-Quarto.html#the-template-explained",
    "href": "11-Quarto.html#the-template-explained",
    "title": "Quarto",
    "section": "The Template Explained",
    "text": "The Template Explained\nThere is quite a bit of formatting going on in the qmd file, and this is what all of this code achieves:\n\nLight grey background to improve readability (a little present to myself for marking)\nFont size 12, one-half spacing\nMain font: Latin Modern Sans Serif for legibility\nMaths font: Fira Maths, ditto\nTitle page with module name, your registration number, submission date, and word count. No page number.\nPage numbering in Roman numerals for table of contents, list of figures, and list of tables\nTable of contents, list of figures, and list of tables are automatic\nPage numbering in Arabic numerals from the start of the text\nAll captions are placed beneath figures and tables\nFully adjustable size of modelsummary table (controlled via percentage of text width), so you can control the number of words the table “costs”\nAutomatic page break if there are fewer than seven lines after a (sub-)section heading\nAutomatic List of References, formatted in APA (American Psychological Association) style\nAppendix after the List of References\nCode chunks are set by default not to show the code in the rendered file. Messages, warnings, and errors are suppressed",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-Quarto.html#assessment-submission",
    "href": "11-Quarto.html#assessment-submission",
    "title": "Quarto",
    "section": "Assessment Submission",
    "text": "Assessment Submission\nWhen it comes to submitting your assessment, please only submit the rendered pdf, and NOT the qmd file. This will not be accepted by Tabula. If you wish to use Quarto on modules other than those taught by me, then please consult the module director whether they are happy to accept submissions in pdf format.\nPlease ensure that you have copied and pasted your complete RScript into the Appendix section of the template. By complete I mean a script that takes you (or anybody else) from loading the original data sets I provide to the output you include in the report you submit on Tabula. The script should be properly annotated to make your work reproducible and easy to follow.\n\n\n\n\nPrzeworski, A., Alvarez, M. E., Cheibub, J. A., & Limongi, F. (2000). Democracy and Development - Political Institutions and Well-Being in the World, 1950-1990. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511804946",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "11-Quarto.html#footnotes",
    "href": "11-Quarto.html#footnotes",
    "title": "Quarto",
    "section": "",
    "text": "https://quarto.org/↩︎\nYou can create absolute and relative file paths to other locations, but let’s not over-complicate things here.↩︎\nOtherwise, this will be placed into the document as the very last thing, but because you have an appendix, I needed to overrule this behaviour in the template.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "12-Downloads.html",
    "href": "12-Downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Documents",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "12-Downloads.html#documents",
    "href": "12-Downloads.html#documents",
    "title": "Downloads",
    "section": "",
    "text": "PO33Q Bibliography\nCodebook for all Data Sets\nStatistical Tables\nWeek 7 Results Table",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "12-Downloads.html#data-sets-in-alphabetical-order",
    "href": "12-Downloads.html#data-sets-in-alphabetical-order",
    "title": "Downloads",
    "section": "Data Sets (in alphabetical order)",
    "text": "Data Sets (in alphabetical order)\n\nAfrica\nEurope\nEastern Europe\nExample\nLatin America\nMiddle East\nPolity V\nPrzeworski et al. (2000)\nSouth East Asia\nSub-Saharan Africa\nWorld\nWorld Value Survey (WVS)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "12-Downloads.html#r-scripts",
    "href": "12-Downloads.html#r-scripts",
    "title": "Downloads",
    "section": "R Scripts",
    "text": "R Scripts\n\nWeek 1\nWeek 3\nWeek 5 Solutions\nWeek 7 Solutions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "13-Glossary.html",
    "href": "13-Glossary.html",
    "title": "Full Glossary",
    "section": "",
    "text": "Full Glossary\n\n\n\nUnless otherwise noted, the definitions are taken from Reiche (forthcoming).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\nadjusted rsquared\nThe coefficient of determination for multiple regression.\n\n\nanalysis\nA detailed evaluation of data to discover their structure and relevant information to answer a research question\n\n\nattribute\nA component or characteristic of a concept\n\n\nautocorrelation\nThe value of one error term does not allow us to predict the value of another error term. As such their covariances must be zero\n\n\nbackground concept\nThe broad constellation of meanings and understandings associated with the concept (Adcock & Collier, 2001, p. 531)\n\n\ncategorical\nDescribing the qualitative categories of a characteristic, for example different religions\n\n\ncausal\nIn order to establish a causal relationship, the following criteria must be met concurrently: 1. Relevance of the variables within the broader theoretical and empirical context of the research (a) Clear Theoretical Framework, (b) Clear conceptualization, (c) Exclusion of alternative explanations; 2. Asymmetry; 3. Significant (and sufficiently strong) statistical association\n\n\nceteris paribus\nAll other things being equal\n\n\ncoefficient\nA coefficient is a numerical expression which is multiplied with the value of a variable\n\n\ncoefficient of determination\nIndicates the proportion of the variation in the dependent variable which is explained through the independent variable. It is defined as \\(\\frac{\\text{Explained Sum of Squares}}{\\text{Total Sum of Squares}}\\)\n\n\nconcept\nAbstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150)\n\n\nconceptualization\nFormulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock & Collier, 2001, p. 531)\n\n\nconditional expectation function\nsee Population Regression Function (PRF)\n\n\nconfidence interval\nA confidence interval is an estimated range, based on a sample, that is likely to contain the true population parameter (such as the mean). If the sampling process were repeated many times, approximately \\((1 - \\alpha) \\cdot 100\\) % of the resulting intervals would contain the true parameter. The value of \\(\\alpha\\) determines the confidence level – for example, \\(\\alpha = 0.05\\) corresponds to a 95% confidence level, while \\(\\alpha = 0.01\\) corresponds to 99%\n\n\nconfidence level\nThe confidence level is the proportion of confidence intervals, constructed from repeated random samples of the same population using the same method, that are expected to contain the true population parameter. It is denoted by \\(1 - \\alpha\\), where \\(\\alpha\\) is the significance level. For example, a 95% confidence level implies that 95% of such intervals would contain the true parameter in the long run.\n\n\ncontinuous\nCan assume any value within defined measurement boundaries\n\n\ncontrol variables\nA variable that is held constant or included in a statistical model to account for its potential influence, allowing clearer estimation of the relationship between the main independent and dependent variables\n\n\ncross-sectional data\nLook at different units (or cross-sections) \\(i\\) at a single point in time\n\n\ncumulative density function\nNA\n\n\ndata\nDerives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis\n\n\ndata set\nA collection of numerical values for individual observations, separated into distinctive variables\n\n\ndegrees of freedom\nDegrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary\n\n\ndependent variable\nIs dependent through some statistical or stochastic process on the value of an independent variable\n\n\ndescriptive statistics\nSummarise information about the centre and variability of a variable\n\n\ndeviation\nThe deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\)\n\n\ndichotomous\nCan only assume two mutually exclusive, but internally homogeneous qualitative categories\n\n\ndichotomous\nCan only assume two mutually exclusive, but internally homogeneous qualitative categories\n\n\ndiscrete\nThe result of a counting process\n\n\ndiscussion\nThe purpose of the discussion section in a research project is to interpret the results, explain their implications, compare them with existing literature, and highlight their significance, limitations, and potential for future research\n\n\nerror term\nThe error term quantifies the distance between each observation and the corresponding point on the regression line. The terms are denoted as \\(\\epsilon_{i}\\)\n\n\ngeneralized linear model\na family of models which allows us to estimate a linear relationship between dependent and independent variables even though the underlying relationship is not linear by means of a link function. It also allows us to vary the assumed distribution of our error terms\n\n\nhypothesis\nIn statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference.\n\n\nindependent variable\nInfluences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable”\n\n\nintercept\nThe intercept is the point at which the regression line intersects the y-axis. In this book we denote it as \\(\\beta_{0}\\)\n\n\nlatent variable\nVariables that can only be inferred indirectly through a mathematical model from other observable variables that can be directly observed or measured\n\n\nlistwise deletion\nWhen a value within an observation is missing, the entire observation is dropped\n\n\nliterature review\nAn analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question\n\n\nmean\nIs equal to the sum of the observations divided by the number of observations\n\n\nmeasurement\nRefers to the selection of a measure or variable\n\n\nmedian\nSeparates the lower half from the upper half of observations\n\n\nmethod\nA tool for systematic investigation\n\n\nmode\nIs the most frequently occurring value\n\n\nmodel building\nRunning a number of regression models, each testing a different combination of variables.\n\n\nnormal distribution\nThe normal distribution is a bell-shaped probability distribution that is symmetrical around the mean. Approximately 68% of values fall within 1 standard deviation of the mean, 96% within 2 standard deviations, and 99.7% within 3 standard deviations. This is known as the empirical rule.\n\n\nOrdinary Least Squares\nThe method of fitting a regression line by means of minimizing the sum of the squared distances between the observations and the estimated values\n\n\np-value\nThe p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\).\n\n\nparameter\nA parameter is the value a statistic would assume in the long run. It is also called the Expected Value\n\n\npartial slope coefficient\nA partial slope coefficient measures the influence of a variable in multiple regression, holding all other independent variables in the model constant\n\n\npopulation\nCollection of all cases which possess certain pre-defined characteristics\n\n\nPopulation Regression Function\nThe Population Regression Function (PRF) describes the expected distribution of \\(y\\), given the values of the independent variable(s) \\(x\\). It is also called the conditional expectation function (CEF) and can be denoted as \\(E(y_{i} \\mid x_{i})\\)\n\n\npredicted probabilities\nThe value of a predicted probability is equal to the cumulative density function, \\(F(X^\\prime\\beta)\\), evaluated at a chosen value of the independent variable\n\n\nprobability\nRefers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring\n\n\nprobability distribution\nSpecifies the likelihood of all possible outcomes for a particular variable\n\n\nQM\nThe process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question\n\n\nquartile\nDivides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below\n\n\nregression\nRegression analysis determines the direction and magnitude of influence of one or more independent variables on a dependent variable\n\n\nregression line\nThe regression line describes how the dependent variable is functionally related to the values of the independent variable. It it defined by the intercept \\(\\beta_{0}\\) and the slope \\(\\beta_{1}\\)\n\n\nresearch question\nA specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle\n\n\nresidual\nAn estimation of the error term. The difference between an observation \\(y_{i}\\) and the estimated value \\(\\hat{y}_{i}\\). Denoted as \\(\\hat{\\epsilon}_{i}\\)\n\n\nsample\nA sub-group of the population\n\n\nSample Regression Function\nA regression line based on a randomly drawn sample\n\n\nsecondary data\nSecondary data are data which have been collected by somebody else\n\n\nsignificance level\nThe significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9.\n\n\nsignificance test\nA significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true\n\n\nslope\nA slope is defined as rise over run, and so it tells us how many units of y we need to climb (or descend if the slope is negative) for every additional unit of the independent variable \\(x\\)\n\n\nSocial Sciences\nAre concerned with the study of society and seek to scientifically describe and explain the behaviour of actors\n\n\nstandard deviation\nThe standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\]\n\n\nstandard error\nThe standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\]\n\n\nsystematized concept\nA specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock & Collier, 2001, p. 531)\n\n\nt-distribution\nThe t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process.\n\n\ntest statistic\nA test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors.\n\n\ntheory\nA formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.)\n\n\ntime-series data\nTime-series data review a certain characteristic over time \\(t\\), where \\(t\\) runs from 1 to \\(T\\)\n\n\nvalidity\nThe extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong”\n\n\nvariable\nAn element of a conceptual component which varies. We also call these “measures”\n\n\nvariance\nIs equal to the squared standard deviation\n\n\nz-score\nThe z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean.\n\n\n\n\n\n\nTable 1: Glossary for PO33Q\n\n\n\n\n\n\n\n\n\nAdcock, R., & Collier, D. (2001). Measurement Validity: A Shared Standard for Qualitative and Quantitative Research. American Political Science Review, 95(3), 529–546. https://doi.org/10.1017/S0003055401003100\n\n\nClark, T., Foster, L., & Bryman, A. (2021). Bryman’s Social Research Methods (Sixth Edition). Oxford: Oxford University Press.\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/\n\n\nReiche, F. (forthcoming). Introduction to Quantitative Methods in the Social Sciences. Oxford: Oxford University Press.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full Glossary</span>"
    ]
  },
  {
    "objectID": "14-bib.html",
    "href": "14-bib.html",
    "title": "List of References",
    "section": "",
    "text": "List of References\n\n\nAdcock, R., & Collier, D. (2001). Measurement\nValidity: A Shared Standard for Qualitative and Quantitative\nResearch. American Political Science\nReview, 95(3), 529–546. https://doi.org/10.1017/S0003055401003100\n\n\nArel-Bundock, V. (2022). modelsummary: Data and\nModel Summaries in R. Journal of Statistical Software,\n103(1), 1–23. https://doi.org/10.18637/jss.v103.i01\n\n\nBeck, N., Epstein, D., Jackman, S., & O’Halloran, S. (2002).\nAlternative Models of Dynamics in binary\nTime-Series-Cross Section Models: The Example of State\nFailure. https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=acdce03f6a768a63b4afd3b013d0050439a5355f\n\n\nBeck, N., Katz, J. N., & Tucker, R. (1998). Taking Time Seriously: Time‑Series-Cross‑Section Analysis\nwith a Binary Dependent Variable. American Journal of\nPolitical Science, 42(4), 1260‑1288. https://doi.org/10.2307/2991857\n\n\nBoix, C., & Stokes, S. (2003). Endogenous\nDemocratization. World Politics,\n55, 517–549. https://doi.org/https://doi.org/10.1353/wp.2003.0019\n\n\nBrambor, T., Clark, W. R., & Golder, M. (2006). Understanding\nInteraction Models: Improving Empirical Analysis.\nPolitical Analysis, 14, 62–83. https://doi.org/10.1093/pan/mpi014\n\n\nClark, T., Foster, L., & Bryman, A. (2021). Bryman’s Social Research Methods (Sixth\nEdition). Oxford: Oxford University Press.\n\n\nDiamond, L. (1992). Economic Development and\nDemocracy Reconsidered. American Behavioral\nScientist, 35(4/5), 450–499. https://doi.org/10.1177/000276429203500407\n\n\nEpstein, D. L., Bates, R., Goldstone, J., Kristensen, I., &\nO’Halloran, S. (2006). Democratic Transitions.\nAmerican Journal of Political Science,\n50(3), 551–569. https://doi.org/10.1111/j.1540-5907.2006.00201.x\n\n\nFogarty, B. J. (2023). Quantitative Social Science Data With\nR (Second Edition). Thousand Oaks, CA: Sage.\n\n\nKing, G. (1995). Replication, replication.\nPS: Political Science and Politics, 28(3), 541–559. https://doi.org/10.2307/420301\n\n\nLipset, S. M. (1959). Some Social Requisites of\nDemocracy: Economic Development and Political Legitimacy. The\nAmerican Political Science Review, 53(1), 69–105. https://doi.org/10.2307/1951731\n\n\nLong, J. D., & Teetor, P. (2019). R\ncookbook: proven recipes for data analysis, statistics, and\ngraphics. O’Reilly Media.\n\n\nLong, J. S. (1997). Regression Models for\nCategorial and Limited Dependent Variables. Thousand Oaks:\nSage. https://doi.org/10.2307/3006005\n\n\nMarshall, M. G., & Gurr, T. R. (2020). Polity V Project: Political Regime Characteristics and\nTransitions, 1800-2018.\n\n\nMiller, M., Boix, C., & Rosato, S. (2022). Boix-Miller-Rosato Dichotomous Coding of Democracy,\n1800-2020 (Version V1) [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/FENWWR\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/\n\n\nPrzeworski, A., Alvarez, M. E., Cheibub, J. A., & Limongi, F.\n(2000). Democracy and Development - Political\nInstitutions and Well-Being in the World, 1950-1990.\nCambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511804946\n\n\nReiche, F. (forthcoming). Introduction to\nQuantitative Methods in the Social Sciences. Oxford: Oxford\nUniversity Press.\n\n\nStimson, J. A. (n.d.). Professional Writing in\nPolitical Science: A Highly opinionated Essay. available\nonline at http://stimson.web.unc.edu/files/2018/02/Writing.pdf.\n\n\nStinerock, R. (2022). Statistics with R – A\nBeginner’s Guide (Second Edition). Thousand Oaks,\nCA: Sage.\n\n\nWorld Bank. (n.d.). World Development Indicators.\nhttps://datacatalog.worldbank.org/dataset/world-development-indicators",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>List of References</span>"
    ]
  }
]